{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59a57894",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rosie.oxbury/miniconda3/envs/discovery_child_development/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-29 15:02:36,982 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2023-11-29 15:02:37,612 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device: cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataTransformerRegistry.enable('default')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import altair as alt\n",
    "from itertools import chain\n",
    "import requests\n",
    "from nesta_ds_utils.loading_saving import S3 as nesta_s3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from typing import NoReturn, List, Any\n",
    "from time import time\n",
    "import sentence_transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from discovery_child_development import S3_BUCKET, config, logging\n",
    "from discovery_child_development.utils import openalex_utils\n",
    "from discovery_child_development.utils import cluster_analysis_utils as cau\n",
    "\n",
    "API_ROOT = config[\"openalex_keywords_api_root\"]\n",
    "S3_PATH = \"metaflow/openalex_keyword_search\"\n",
    "YEARS = config[\"openalex_years\"]\n",
    "KEYWORDS = config[\"openalex_keywords\"]\n",
    "SEED = config[\"seed\"]\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "alt.data_transformers.disable_max_rows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea2dbe99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"https://api.openalex.org/works?search=(abstract:(child OR infant OR baby OR prenatal OR pregnancy) AND abstract:('artificial intelligence' OR assess OR assessment OR 'augmented reality' OR autism OR behaviour OR development OR 'eye tracking' OR genetics OR income OR learning OR 'learning environment' OR monitor OR psychotherapy OR 'randomised controlled trials' OR robotics OR 'social media' OR 'social services' OR 'special need' OR technology OR 'virtual reality' OR wearable)) OR (title:(child OR infant OR baby OR prenatal OR pregnancy) AND title:('artificial intelligence' OR assess OR assessment OR 'augmented reality' OR autism OR behaviour OR development OR 'eye tracking' OR genetics OR income OR learning OR 'learning environment' OR monitor OR psychotherapy OR 'randomised controlled trials' OR robotics OR 'social media' OR 'social services' OR 'special need' OR technology OR 'virtual reality' OR wearable))&filter=publication_year:2019\",\n",
       " \"https://api.openalex.org/works?search=(abstract:(child OR infant OR baby OR prenatal OR pregnancy) AND abstract:('artificial intelligence' OR assess OR assessment OR 'augmented reality' OR autism OR behaviour OR development OR 'eye tracking' OR genetics OR income OR learning OR 'learning environment' OR monitor OR psychotherapy OR 'randomised controlled trials' OR robotics OR 'social media' OR 'social services' OR 'special need' OR technology OR 'virtual reality' OR wearable)) OR (title:(child OR infant OR baby OR prenatal OR pregnancy) AND title:('artificial intelligence' OR assess OR assessment OR 'augmented reality' OR autism OR behaviour OR development OR 'eye tracking' OR genetics OR income OR learning OR 'learning environment' OR monitor OR psychotherapy OR 'randomised controlled trials' OR robotics OR 'social media' OR 'social services' OR 'special need' OR technology OR 'virtual reality' OR wearable))&filter=publication_year:2020\",\n",
       " \"https://api.openalex.org/works?search=(abstract:(child OR infant OR baby OR prenatal OR pregnancy) AND abstract:('artificial intelligence' OR assess OR assessment OR 'augmented reality' OR autism OR behaviour OR development OR 'eye tracking' OR genetics OR income OR learning OR 'learning environment' OR monitor OR psychotherapy OR 'randomised controlled trials' OR robotics OR 'social media' OR 'social services' OR 'special need' OR technology OR 'virtual reality' OR wearable)) OR (title:(child OR infant OR baby OR prenatal OR pregnancy) AND title:('artificial intelligence' OR assess OR assessment OR 'augmented reality' OR autism OR behaviour OR development OR 'eye tracking' OR genetics OR income OR learning OR 'learning environment' OR monitor OR psychotherapy OR 'randomised controlled trials' OR robotics OR 'social media' OR 'social services' OR 'special need' OR technology OR 'virtual reality' OR wearable))&filter=publication_year:2021\",\n",
       " \"https://api.openalex.org/works?search=(abstract:(child OR infant OR baby OR prenatal OR pregnancy) AND abstract:('artificial intelligence' OR assess OR assessment OR 'augmented reality' OR autism OR behaviour OR development OR 'eye tracking' OR genetics OR income OR learning OR 'learning environment' OR monitor OR psychotherapy OR 'randomised controlled trials' OR robotics OR 'social media' OR 'social services' OR 'special need' OR technology OR 'virtual reality' OR wearable)) OR (title:(child OR infant OR baby OR prenatal OR pregnancy) AND title:('artificial intelligence' OR assess OR assessment OR 'augmented reality' OR autism OR behaviour OR development OR 'eye tracking' OR genetics OR income OR learning OR 'learning environment' OR monitor OR psychotherapy OR 'randomised controlled trials' OR robotics OR 'social media' OR 'social services' OR 'special need' OR technology OR 'virtual reality' OR wearable))&filter=publication_year:2022\",\n",
       " \"https://api.openalex.org/works?search=(abstract:(child OR infant OR baby OR prenatal OR pregnancy) AND abstract:('artificial intelligence' OR assess OR assessment OR 'augmented reality' OR autism OR behaviour OR development OR 'eye tracking' OR genetics OR income OR learning OR 'learning environment' OR monitor OR psychotherapy OR 'randomised controlled trials' OR robotics OR 'social media' OR 'social services' OR 'special need' OR technology OR 'virtual reality' OR wearable)) OR (title:(child OR infant OR baby OR prenatal OR pregnancy) AND title:('artificial intelligence' OR assess OR assessment OR 'augmented reality' OR autism OR behaviour OR development OR 'eye tracking' OR genetics OR income OR learning OR 'learning environment' OR monitor OR psychotherapy OR 'randomised controlled trials' OR robotics OR 'social media' OR 'social services' OR 'special need' OR technology OR 'virtual reality' OR wearable))&filter=publication_year:2023\"]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries = openalex_utils.generate_keyword_queries(API_ROOT, KEYWORDS, YEARS)\n",
    "\n",
    "queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8553077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = requests.get(queries[0])\n",
    "# check that a single query runs ok\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82ca4bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-29 15:02:39,055 - root - INFO - Number of hits: 5319\n",
      "2023-11-29 15:02:39,791 - root - INFO - Number of hits: 3062\n",
      "2023-11-29 15:02:40,531 - root - INFO - Number of hits: 4714\n",
      "2023-11-29 15:02:41,274 - root - INFO - Number of hits: 4754\n",
      "2023-11-29 15:02:41,966 - root - INFO - Number of hits: 3660\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "21509"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find out how many hits we should get for all of the queries\n",
    "total = 0\n",
    "\n",
    "for query in queries:\n",
    "    count = requests.get(query).json()[\"meta\"][\"count\"]\n",
    "    logging.info(f\"Number of hits: {count}\")\n",
    "    total += count\n",
    "    \n",
    "total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18eea35a",
   "metadata": {},
   "source": [
    "The metaflow script `pipeline/openalex/openalex_keyword_search.py` runs all of the queries and stores the results on S3. Below, we load the results and do a little bit of EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b9d16ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILES = [\n",
    "    f\"openalex_keywords_True_year-{year}.json\"\n",
    "    for year in YEARS\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cb08ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-29 15:02:41,993 - botocore.credentials - INFO - Found credentials in environment variables.\n",
      "2023-11-29 15:02:51,339 - root - INFO - Number of works in openalex_keywords_True_year-2019.json: 5305\n",
      "2023-11-29 15:02:56,618 - root - INFO - Number of works in openalex_keywords_True_year-2020.json: 3057\n",
      "2023-11-29 15:03:04,212 - root - INFO - Number of works in openalex_keywords_True_year-2021.json: 4709\n",
      "2023-11-29 15:03:11,703 - root - INFO - Number of works in openalex_keywords_True_year-2022.json: 4745\n",
      "2023-11-29 15:03:17,839 - root - INFO - Number of works in openalex_keywords_True_year-2023.json: 3653\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "21469"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openalex_df = openalex_utils.concat_json_files(INPUT_FILES, S3_BUCKET, S3_PATH)\n",
    "\n",
    "len(openalex_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b1d3d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retain only works in English\n",
    "openalex_en = openalex_df[openalex_df[\"language\"] == \"en\"]\n",
    "openalex_en = openalex_en[openalex_en[\"abstract_inverted_index\"].notnull()]\n",
    "openalex_en = openalex_en[openalex_en[\"title\"].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d77f9ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rosie.oxbury/Documents/git_repos/discovery_child_development/discovery_child_development/utils/openalex_utils.py:177: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, \"abstract\"] = df[\"abstract_inverted_index\"].apply(\n",
      "/Users/rosie.oxbury/Documents/git_repos/discovery_child_development/discovery_child_development/utils/openalex_utils.py:181: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, \"text\"] = df[\"title\"] + \". \" + df[\"abstract\"]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://openalex.org/W2992210402</td>\n",
       "      <td>An Artificial Somatic Reflex Arc</td>\n",
       "      <td>Abstract The emulation of human sensation, per...</td>\n",
       "      <td>An Artificial Somatic Reflex Arc. Abstract The...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://openalex.org/W2899856450</td>\n",
       "      <td>Siri, Siri, in my hand: Who’s the fairest in t...</td>\n",
       "      <td>Artificial intelligence (AI)—defined as a syst...</td>\n",
       "      <td>Siri, Siri, in my hand: Who’s the fairest in t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://openalex.org/W2886354130</td>\n",
       "      <td>A systematic review of the smart home literatu...</td>\n",
       "      <td>A smart home is a residence equipped with smar...</td>\n",
       "      <td>A systematic review of the smart home literatu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://openalex.org/W2788388592</td>\n",
       "      <td>Continual lifelong learning with neural networ...</td>\n",
       "      <td>Humans and animals have the ability to continu...</td>\n",
       "      <td>Continual lifelong learning with neural networ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://openalex.org/W2923238705</td>\n",
       "      <td>The urgent need for microbiology literacy in s...</td>\n",
       "      <td>Microbes and their activities have pervasive, ...</td>\n",
       "      <td>The urgent need for microbiology literacy in s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id  \\\n",
       "0  https://openalex.org/W2992210402   \n",
       "1  https://openalex.org/W2899856450   \n",
       "2  https://openalex.org/W2886354130   \n",
       "3  https://openalex.org/W2788388592   \n",
       "4  https://openalex.org/W2923238705   \n",
       "\n",
       "                                               title  \\\n",
       "0                   An Artificial Somatic Reflex Arc   \n",
       "1  Siri, Siri, in my hand: Who’s the fairest in t...   \n",
       "2  A systematic review of the smart home literatu...   \n",
       "3  Continual lifelong learning with neural networ...   \n",
       "4  The urgent need for microbiology literacy in s...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  Abstract The emulation of human sensation, per...   \n",
       "1  Artificial intelligence (AI)—defined as a syst...   \n",
       "2  A smart home is a residence equipped with smar...   \n",
       "3  Humans and animals have the ability to continu...   \n",
       "4  Microbes and their activities have pervasive, ...   \n",
       "\n",
       "                                                text  \n",
       "0  An Artificial Somatic Reflex Arc. Abstract The...  \n",
       "1  Siri, Siri, in my hand: Who’s the fairest in t...  \n",
       "2  A systematic review of the smart home literatu...  \n",
       "3  Continual lifelong learning with neural networ...  \n",
       "4  The urgent need for microbiology literacy in s...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openalex_en_abstracts = openalex_utils.create_text_data(\n",
    "        openalex_en[[\"id\", \"title\", \"abstract_inverted_index\"]]\n",
    "    )\n",
    "\n",
    "openalex_en_abstracts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39b1d2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "openalex_docs = openalex_en_abstracts[\"text\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cbf45d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Wearables and mobile technologies in Autism Spectrum Disorder interventions: A systematic literature review. Nowadays, in the Internet of Things era, wearables, mobile technologies and enhanced communication and computing capabilities has led to the upsurge of innovative mobile health solutions. Many research efforts have taken place recently in the domain of autism spectrum disorders (ASD). The current paper presents a thorough review of the literature on the use of wearables and mobile technologies for ASD-related interventions. It intends to give insights and guidelines to researchers in order to develop more useful and closer to market products. We searched seven databases for research articles published after 2000. Of 4,722 articles initially retrieved, only 83 papers met the inclusion criteria. Several challenges still exist in the research efforts towards the development of applications exploiting the latest wearables and mobile technologies for ASD interventions: small number of participants in the studies, non-generalizable results, technology considerations, privacy, legal and ethical issues, etc. Subjective assessment is also another significant barrier for further adoption of the developed solutions. The findings support the notion that this is a very promising sector which is expected to undergo an important increase in the coming years. There is a great need for highly customizable solutions. In parallel, researchers should focus on the importance of developing applications for the real world and not only for controlled environments. Further studies employing artificial intelligence and affective computing are needed to support both diagnosis and therapeutic interventions as well.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check out a random example\n",
    "openalex_docs[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43e8254d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t0 = time()\n",
    "# sentence_vectors_384 = model.encode(openalex_docs, show_progress_bar=True)\n",
    "# print(f\"vectorization done in {time() - t0:.3f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bfae698d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(\"openalex_sentence_vectors_384.npy\", sentence_vectors_384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "668dbc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_vectors_384 = np.load(\"openalex_sentence_vectors_384.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b9c1802",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_params = {\n",
    "    \"n_components\": 50,  # apparently hdbscan does not work very well with more than 50 components\n",
    "    \"n_neighbors\": 10,\n",
    "    \"min_dist\": 0.5,\n",
    "    \"spread\": 0.5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee0a14a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-29 15:03:18,477 - root - INFO - Generating 50-d UMAP embbedings for 16214 vectors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rosie.oxbury/miniconda3/envs/discovery_child_development/lib/python3.10/site-packages/umap/umap_.py:1943: UserWarning: n_jobs value -1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(f\"n_jobs value {self.n_jobs} overridden to 1 by setting random_state. Use no seed for parallelism.\")\n"
     ]
    }
   ],
   "source": [
    "# reduce dimensionality of the embeddings\n",
    "sentence_vectors_50 = cau.umap_reducer(\n",
    "    sentence_vectors_384, umap_params, random_umap_state=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f25cb98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-29 15:03:43,055 - root - INFO - Clustering 16214 vectors with K-Means clustering\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rosie.oxbury/miniconda3/envs/discovery_child_development/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "/Users/rosie.oxbury/miniconda3/envs/discovery_child_development/lib/python3.10/site-packages/threadpoolctl.py:1010: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "kmeans_labels = cau.kmeans_clustering(\n",
    "    sentence_vectors_50, kmeans_params={'init': 'k-means++', 'n_clusters': 20}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d232cdf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rosie.oxbury/miniconda3/envs/discovery_child_development/lib/python3.10/site-packages/umap/umap_.py:1943: UserWarning: n_jobs value -1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(f\"n_jobs value {self.n_jobs} overridden to 1 by setting random_state. Use no seed for parallelism.\")\n"
     ]
    }
   ],
   "source": [
    "# Reduce original vectors to 2D for plotting\n",
    "openalex_texts_2d = cau.reduce_to_2D(sentence_vectors_384, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fdc20da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df = openalex_en_abstracts.assign(\n",
    "    cluster=kmeans_labels,\n",
    "    x=openalex_texts_2d[:, 0],\n",
    "    y=openalex_texts_2d[:, 1],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8911d7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig_hdbscan = (\n",
    "#     alt.Chart(\n",
    "#         cluster_df\n",
    "#     )\n",
    "#     .mark_circle()\n",
    "#     .encode(\n",
    "#         x=\"x\",\n",
    "#         y=\"y\",\n",
    "#         color=alt.Color(\"cluster:N\", legend=alt.Legend(title=\"cluster\")),\n",
    "#         tooltip=[\"title\", \"cluster\"],\n",
    "#     )\n",
    "#     .properties(width=800, height=600)\n",
    "#     .interactive()\n",
    "# )\n",
    "\n",
    "# fig_hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "029b42e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-29 15:03:54,916 - root - INFO - Cluster 0: ['Placebo From an Enactive Perspective. Due to their complexity and variability, placebo effects remain controversial. We suggest this is also due to a set of problematic assumptions (dualism, reductionism, individualism, passivity). We critically assess current explanations and empirical evidence and propose an alternative theoretical framework—the enactive approach to life and mind—based on recent developments in embodied cognitive science. We review core enactive concepts such as autonomy, agency, and sense-making. Following these ideas, we propose a move from binary distinctions (e.g., conscious vs. non-conscious) to the more workable categories of reflective and pre-reflective activity. We introduce an ontology of individuation, following the work of Gilbert Simondon, that allow us to see placebo interventions not as originating causal chains, but as modulators and triggers in the regulation of tensions between ongoing embodied and interpersonal processes. We describe these interrelated processes involving looping effects through three intertwined dimensions of embodiment: organic, sensorimotor, and intersubjective. Finally, we defend the need to investigate therapeutic interactions in terms of participatory sense-making, going beyond the identification of individual social traits (e.g., empathy, trust) that contribute to placebo effects. We discuss resonances and differences between the enactive proposal, popular explanations such as expectations and conditioning, and other approaches based on meaning responses and phenomenological/ecological ideas.', 'Adapting Ourselves, Instead of the Environment: An Inquiry into Human Enhancement for Function and Beyond. Abstract Technology enables humans not only to adapt their environment to their needs but also to modify themselves. Means of Human Enhancement — embodied technologies to improve the human body’s capabilities or to create a new one — are the designated means of adapting ourselves instead of the environment. The debate about these technologies is typically fought on ethical soil. However, alarmist, utopian, and science fiction scenarios distract from the fact that Human Enhancement is a historical and pervasive phenomenon incorporated into many everyday practices. In the vein of disentangling conceptual difficulties, we claim that means of Human Enhancement are either physiologically or psychologically embodied, rendering the merging with the human user their most defining aspect. To fulfill its purpose, an enhancement must pass the test-in-the-world, i.e., assisting with effective engagement with a dynamic world. Even if failing in this regard: Human Enhancement is the fundamental and semi-targeted process of changing the users relationship with the world through the physical or psychological embodiment of a hitherto external object and/or change of one’s body. This can potentially change the notion of being human. Drawing on a rich body of theoretical and empirical literature, we aim to provide a nuanced analysis of the transformative nature of this phenomenon in close proximity to human practice. Stakeholders are invited to apply the theory presented here to interrogate their perspective on technology in general and Human Enhancement in particular.', 'Narrative responsibility and artificial intelligence. Abstract Most accounts of responsibility focus on one type of responsibility, moral responsibility, or address one particular aspect of moral responsibility such as agency. This article outlines a broader framework to think about responsibility that includes causal responsibility, relational responsibility, and what I call “narrative responsibility” as a form of “hermeneutic responsibility”, connects these notions of responsibility with different kinds of knowledge, disciplines, and perspectives on human being, and shows how this framework is helpful for mapping and analysing how artificial intelligence (AI) challenges human responsibility and sense-making in various ways. Mobilizing recent hermeneutic approaches to technology, the article argues that next to, and interwoven with, other types of responsibility such as moral responsibility, we also have narrative and hermeneutic responsibility—in general and for technology. For example, it is our task as humans to make sense of, with and, if necessary, against AI. While from a posthumanist point of view, technologies also contribute to sense-making, humans are the experiencers and bearers of responsibility and always remain in charge when it comes to this hermeneutic responsibility. Facing and working with a world of data, correlations, and probabilities, we are nevertheless condemned to make sense. Moreover, this also has a normative, sometimes even political aspect: acknowledging and embracing our hermeneutic responsibility is important if we want to avoid that our stories are written elsewhere—through technology.', '[Essay] The Algorithm; Mind of a Virtual Era – Our Code of Code. As soon as something implicit intrudes consciousness human thought undergoes a radical change. The introduction of any new tool or code brings a shift in cognition; every micro-step layering new semiotic forms within each macroevolutionary-stage has buttressed a new semantic leap. Our mechanization of everyday life and the tech-systems we interact with are impacting communication, cultural norms and values, market-aesthetics, and economics, in societies at large. Undergirded by a survey of the role and significance of tools in human evolution, this study arrives at what is already a well-entrenched new era: the digital, screen-mediated age. Revolutionized by the algorithm, introduced by computers, this age is dominated by the addictive quality of instant contact, unlimited information, virtual gaming, and titillating service-forms, all at our finger tips. Aside from the interpersonal impact on the new humans growing up with devices in hand, how does this disembodied, digital code-form through which our interactions are mediated condition human cognition? How does its seductive efficiency interfere with how we relate, feel, assign meanings, think? Rooted in Code Biology macro-evolutionary and psychoanalytic principles, this paper examines the algorithm itself and takes a sweeping interdisciplinary approach to the developmental, psychosocial, and cognitive implications for the human mind/brain as it interacts with its technological extension.', 'The development of categorisation and conceptual thinking in early childhood: methods and limitations. Abstract We present a systematic and qualitative review of academic literature on early conceptual development (0–24 months of age), with an emphasis on methodological aspects. The final sample of our review included 281 studies reported in 115 articles. The main aims of the article were four: first, to organise studies into sets according to methodological similarities and differences; second, to elaborate on the methodological procedures that characterise each set; third, to circumscribe the empirical indicators that different sets of studies consider as proof of the existence of concepts in early childhood; last, to identify methodological limitations and to propose possible ways to overcome them. We grouped the studies into five sets: preference and habituation experiments , category extension tasks , object sorting tasks , sequential touching tasks and object examination tasks . In the “Results” section, we review the core features of each set of studies. In the “Discussion” and “Conclusions” sections, we describe, for one thing, the most relevant methodological shortcomings. We end by arguing that a situated, semiotic and pragmatic perspective that emphasises the importance of ecological validity could open up new avenues of research to better understand the development of concepts in early childhood.', 'Towards Informatic Personhood: understanding contemporary subjects in a data-driven society. This paper explores the relationships of subjects in the context of data and data technologies, and advances an original theoretical framework called Informatic Personhood to better conceptual subjects and their relationships. Because of the enormous structural change that data has contributed to, subjects are sometimes distant and backgrounded in studies of data, despite data having significant impacts on their lives. Data-mediated relationships mean an increased scale to a relationship, with individuals able to connect to much broader contexts of data, but also have these structures reach down to their subjective context through data. Informatic Personhood seeks to capture the dynamics of data present in everyday life, addressing this distance and better conceptualising the scale of data-mediated relationships. This framework has two parts. The first – The Informatic Context – explores salient structural developments around data and conceptualises this as being defined by the presence of ‘data interfaces’ (that connect individuals to digital contexts), ‘data circulation’ (trends in the movement and storage of data), and ‘data abstraction’ (data manipulation practices). The second part concerns the Informatic Person, and the embodied, affective, and sensemaking relationships of individuals occurring across and through the Informatic Context. This framework better addresses the scale of data-mediated relationships, and places subjects firmly in the foreground of how data is understood.', 'Analyzing Multimodal Multichannel Data about Self-Regulated Learning with Advanced Learning Technologies: Issues and Challenges. Analyzing multimodal multichannel data about self-regulated learning (SRL) obtained during the use of advanced learning technologies such as intelligent tutoring systems, serious games, hypermedia, and immersive virtual learning environments is key to understanding the interplay among cognitive, affective, metacognitive, and social processes and their impact on learning, problem solving, reasoning, and conceptual understanding in learners of all ages and contexts. In this special issue of Computers in Human Behavior, we report six studies conducted by interdisciplinary teams’ use of various trace methodologies such as eye tracking, log-files, physiological data, facial expressions of emotions, screen recordings, concurrent think-alouds, and linguistic analyses of discourse. The research studies focus on how these data were analyzed using a combination of traditional statistical techniques as well as educational data-mining procedures to detect, measure, and infer cognitive, metacognitive, and social processes related to regulating the self and others across several tasks, domains, ages, and contexts. The results of these studies point to future work necessitating interdisciplinary researchers’ collaboration to use theoretically based and empirically derived approaches to collecting, measuring, and modeling multimodal multichannel SRL data to extend our current models, frameworks, and theories by making them more predictive by elucidating the nature, complexity, and temporality of underlying processes. Lastly, analyses of multimodal multichannel SRL process data can significantly augment advanced learning technologies by providing real-time, intelligent, adaptive, individualized scaffolding and feedback to address learners’ self-regulatory needs.', 'Virtual Environments for Research into Social Evolution (VERSE): A novel experimental environment for the study of human social learning. 1. Abstract Social learning (learning from others) can be a cost-effective way of gaining information compared to asocial (independent) learning. However, learning from others indiscriminately can lead to the acquisition of maladaptive behaviours or outdated information. Evolutionary theory therefore predicts that individuals will use social information adaptively through the use of ‘social learning strategies’. Restrictive laboratory conditions, however, make studying human learning strategies problematic. Abstract tasks, unrealistic sources of social information and methodologies that do not take into account the influence of physical location over large spaces make it difficult to ascertain if previous findings are representative of the way we would use social information in reality. Here I describe a novel platform for studying human social behaviour within immersive virtual environments: “Virtual Environments for Research into Social Evolution” (VERSE). Through the use of gaming technology, VERSE allows researchers to build realistic, three-dimensional, open world environments where participants can complete ecologically relevant tasks while actively observing computer-controlled artificial intelligence agents (AIs) that act as realistic yet controllable sources of social information. This methodological article begins by exploring what social learning strategies are and the problems with studying social learning behaviour in humans (compared to animal populations, for example). I then discuss how gaming technology can be used in behavioural research and follow on with a detailed account of the specific functionalities available in VERSE. I conclude with a worked example of how VERSE can be used to construct a novel behavioural experiment. Altogether, VERSE has great potential to give us insight into how human individuals learn within novel environments in a way that has never before been possible.', 'Perceiving Sociable Technology: Exploring the Role of Anthropomorphism and Agency Perception on Human-Computer Interaction (HCI). With the arrival of personal assistants and other AI-enabled autonomous technologies, social interactions with smart devices have become a part of our daily lives. Therefore, it becomes increasingly important to understand how these social interactions emerge, and why users appear to be influenced by them. For this reason, I explore questions on what the antecedents and consequences of this phenomenon, known as anthropomorphism, are as described in the extant literature from fields ranging from information systems to social neuroscience. I critically analyze those empirical studies directly measuring anthropomorphism and those referring to it without a corresponding measurement. Through a grounded theory approach, I identify common themes and use them to develop models for the antecedents and consequences of anthropomorphism. The results suggest anthropomorphism possesses both conscious and non-conscious components with varying implications. While conscious attributions are shown to vary based on individual differences, non-conscious attributions emerge whenever a technology exhibits apparent reasoning such as through non-verbal behavior like peer-to-peer mirroring or verbal paralinguistic and backchanneling cues. Anthropomorphism has been shown to affect users’ self-perceptions, perceptions of the technology, how users interact with the technology, and the users’ performance. Examples include changes in a users’ trust on the technology, conformity effects, bonding, and displays of empathy. I argue these effects emerge from changes in users’ perceived agency, and their self- and social- identity similarly to interactions between humans. Afterwards, I critically examine current theories on anthropomorphism and present propositions about its nature based on the results of the empirical literature. Subsequently, I introduce a two-factor model of anthropomorphism that proposes how an individual anthropomorphizes a technology is dependent on how the technology was initially perceived (top-down and rational or bottom-up and automatic), and whether it exhibits a capacity for agency or experience. I propose that where a technology lays along this spectrum determines how individuals relates to it, creating shared agency effects, or changing the users’ social identity. For this reason, anthropomorphism is a powerful tool that can be leveraged to support future interactions with smart technologies.', 'What makes AI ‘intelligent’ and ‘caring’? Exploring affect and relationality across three sites of intelligence and care. This paper scrutinises how AI and robotic technologies are transforming the relationships between people and machines in new affective, embodied and relational ways. Through investigating what it means to exist as human \\'in relation\\' to AI across health and care contexts, we aim to make three main contributions. (1) We start by highlighting the complexities of philosophical issues surrounding the concepts of \"artificial intelligence\" and \"ethical machines.\" (2) We outline some potential challenges and opportunities that the creation of such technologies may bring in the health and care settings. We focus on AI applications that interface with health and care via examples where AI is explicitly designed as an \\'augmenting\\' technology that can overcome human bodily and cognitive as well as socio-economic constraints. We focus on three dimensions of \\'intelligence\\' - physical, interpretive, and emotional - using the examples of robotic surgery, digital pathology, and robot caregivers, respectively. Through investigating these areas, we interrogate the social context and implications of human-technology interaction in the interrelational sphere of care practice. (3) We argue, in conclusion, that there is a need for an interdisciplinary mode of theorising \\'intelligence\\' as relational and affective in ways that can accommodate the fragmentation of both conceptual and material boundaries between human and AI, and human and machine. Our aim in investigating these sociological, philosophical and ethical questions is primarily to explore the relationship between affect, relationality and \\'intelligence,\\' the intersection and integration of \\'human\\' and \\'artificial\\' intelligence, through an examination of how AI is used across different dimensions of intelligence. This allows us to scrutinise how \\'intelligence\\' is ultimately conveyed, understood and (technologically or algorithmically) configured in practice through emerging relationships that go beyond the conceptual divisions between humans and machines, and humans vis-à-vis artificial intelligence-based technologies.']\n",
      "2023-11-29 15:03:56,845 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-11-29 15:03:56,860 - root - INFO - Cluster 1: ['Adapting Ourselves, Instead of the Environment: An Inquiry into Human Enhancement for Function and Beyond. Abstract Technology enables humans not only to adapt their environment to their needs but also to modify themselves. Means of Human Enhancement — embodied technologies to improve the human body’s capabilities or to create a new one — are the designated means of adapting ourselves instead of the environment. The debate about these technologies is typically fought on ethical soil. However, alarmist, utopian, and science fiction scenarios distract from the fact that Human Enhancement is a historical and pervasive phenomenon incorporated into many everyday practices. In the vein of disentangling conceptual difficulties, we claim that means of Human Enhancement are either physiologically or psychologically embodied, rendering the merging with the human user their most defining aspect. To fulfill its purpose, an enhancement must pass the test-in-the-world, i.e., assisting with effective engagement with a dynamic world. Even if failing in this regard: Human Enhancement is the fundamental and semi-targeted process of changing the users relationship with the world through the physical or psychological embodiment of a hitherto external object and/or change of one’s body. This can potentially change the notion of being human. Drawing on a rich body of theoretical and empirical literature, we aim to provide a nuanced analysis of the transformative nature of this phenomenon in close proximity to human practice. Stakeholders are invited to apply the theory presented here to interrogate their perspective on technology in general and Human Enhancement in particular.', 'Narrative responsibility and artificial intelligence. Abstract Most accounts of responsibility focus on one type of responsibility, moral responsibility, or address one particular aspect of moral responsibility such as agency. This article outlines a broader framework to think about responsibility that includes causal responsibility, relational responsibility, and what I call “narrative responsibility” as a form of “hermeneutic responsibility”, connects these notions of responsibility with different kinds of knowledge, disciplines, and perspectives on human being, and shows how this framework is helpful for mapping and analysing how artificial intelligence (AI) challenges human responsibility and sense-making in various ways. Mobilizing recent hermeneutic approaches to technology, the article argues that next to, and interwoven with, other types of responsibility such as moral responsibility, we also have narrative and hermeneutic responsibility—in general and for technology. For example, it is our task as humans to make sense of, with and, if necessary, against AI. While from a posthumanist point of view, technologies also contribute to sense-making, humans are the experiencers and bearers of responsibility and always remain in charge when it comes to this hermeneutic responsibility. Facing and working with a world of data, correlations, and probabilities, we are nevertheless condemned to make sense. Moreover, this also has a normative, sometimes even political aspect: acknowledging and embracing our hermeneutic responsibility is important if we want to avoid that our stories are written elsewhere—through technology.', 'Placebo From an Enactive Perspective. Due to their complexity and variability, placebo effects remain controversial. We suggest this is also due to a set of problematic assumptions (dualism, reductionism, individualism, passivity). We critically assess current explanations and empirical evidence and propose an alternative theoretical framework—the enactive approach to life and mind—based on recent developments in embodied cognitive science. We review core enactive concepts such as autonomy, agency, and sense-making. Following these ideas, we propose a move from binary distinctions (e.g., conscious vs. non-conscious) to the more workable categories of reflective and pre-reflective activity. We introduce an ontology of individuation, following the work of Gilbert Simondon, that allow us to see placebo interventions not as originating causal chains, but as modulators and triggers in the regulation of tensions between ongoing embodied and interpersonal processes. We describe these interrelated processes involving looping effects through three intertwined dimensions of embodiment: organic, sensorimotor, and intersubjective. Finally, we defend the need to investigate therapeutic interactions in terms of participatory sense-making, going beyond the identification of individual social traits (e.g., empathy, trust) that contribute to placebo effects. We discuss resonances and differences between the enactive proposal, popular explanations such as expectations and conditioning, and other approaches based on meaning responses and phenomenological/ecological ideas.', 'Virtual Environments for Research into Social Evolution (VERSE): A novel experimental environment for the study of human social learning. 1. Abstract Social learning (learning from others) can be a cost-effective way of gaining information compared to asocial (independent) learning. However, learning from others indiscriminately can lead to the acquisition of maladaptive behaviours or outdated information. Evolutionary theory therefore predicts that individuals will use social information adaptively through the use of ‘social learning strategies’. Restrictive laboratory conditions, however, make studying human learning strategies problematic. Abstract tasks, unrealistic sources of social information and methodologies that do not take into account the influence of physical location over large spaces make it difficult to ascertain if previous findings are representative of the way we would use social information in reality. Here I describe a novel platform for studying human social behaviour within immersive virtual environments: “Virtual Environments for Research into Social Evolution” (VERSE). Through the use of gaming technology, VERSE allows researchers to build realistic, three-dimensional, open world environments where participants can complete ecologically relevant tasks while actively observing computer-controlled artificial intelligence agents (AIs) that act as realistic yet controllable sources of social information. This methodological article begins by exploring what social learning strategies are and the problems with studying social learning behaviour in humans (compared to animal populations, for example). I then discuss how gaming technology can be used in behavioural research and follow on with a detailed account of the specific functionalities available in VERSE. I conclude with a worked example of how VERSE can be used to construct a novel behavioural experiment. Altogether, VERSE has great potential to give us insight into how human individuals learn within novel environments in a way that has never before been possible.', 'Techne in Affective Posthumanism and AI Artefacts: More (or Less) than Human?. In affective neuroscience, constructivist models are acutely influenced by the modern technological evolution, which underwrites an ongoing epistemological substitution of techne for episteme. Evidenced symptomatically in the influence of artificial intelligence (AI), affective artefacts, these models inform an ontological incursion of techne seen to coincide with posthumanist aspirations and anthropology. It is from the perspective of this neuroscientific techne that posthumanism views the human being as increasingly ill adapted to the modern technological civilization, which, conversely, is understood to require a technical governance of the sort envisioned through AI. Among the projects thought necessary for implementing this framework is a recasting of the human emotional spectrum. Revealed through its techne recasting, however, are explanatory commitments to a metaphysic of extrinsic and contiguous causes, where malleability is ontologically constitutive. Aligned with posthumanist assertions malleability is invoked to argue for a rapid advance of the human form, normatively driven by enlightenment ideals. The ontological claim, however, dispenses with the stability of an a priori, intersubjective and interrelational metaphysical form that undergirds the emotions, leading to the collapse of a definitional anthropos. This paper will argue that techne models of the emotions selectively endorse philosophy of science commitments, thereby introducing a normative inversion that deconstructs the notion of anthropology pursued in posthumanist aspirations.', 'Spike Jonze’s Her: How Transhumanism Turns into a Control Mechanism Under the Name of Love. Transhumanism is a philosophy based on the idea of enhancing the physical, intellectual and psychological capacity of human beings through the direct use of science and technology. Because transhumanism aims to ensure super longevity, super intelligence and super well-being, it is said that it will provide a more liberating atmosphere for human beings by elevating the current human condition. In Spike Jonze’s film her (2013), transhumanist technologies seem to present a peaceful society by focusing on the inhabitants’ well-being. However, in this futuristic society where advanced AI technology and personal letter writing service are widespread, human beings are over-controlled in a subtle way by manufacturing consent as Antonio Gramsci uses to explain the concept of hegemony. So transhumanism does not present a free environment in this futuristic society; instead it disguises the control mechanism through these technologies. The aim of this article is to explore how human beings are controlled by transhumanist technologies subtly under the name of providing super well-being.', \"Theories of Parenting and Their Application to Artificial Intelligence. As machine learning (ML) systems have advanced, they have acquired more power over humans' lives, and questions about what values are embedded in them have become more complex and fraught. It is conceivable that in the coming decades, humans may succeed in creating artificial general intelligence (AGI) that thinks and acts with an open-endedness and autonomy comparable to that of humans. The implications would be profound for our species; they are now widely debated not just in science fiction and speculative research agendas but increasingly in serious technical and policy conversations. Much work is underway to try to weave ethics into advancing ML research. We think it useful to add the lens of parenting to these efforts, and specifically radical, queer theories of parenting that consciously set out to nurture agents whose experiences, objectives and understanding of the world will necessarily be very different from their parents'. We propose a spectrum of principles which might underpin such an effort; some are relevant to current ML research, while others will become more important if AGI becomes more likely. These principles may encourage new thinking about the development, design, training, and release into the world of increasingly autonomous agents.\", 'The development of categorisation and conceptual thinking in early childhood: methods and limitations. Abstract We present a systematic and qualitative review of academic literature on early conceptual development (0–24 months of age), with an emphasis on methodological aspects. The final sample of our review included 281 studies reported in 115 articles. The main aims of the article were four: first, to organise studies into sets according to methodological similarities and differences; second, to elaborate on the methodological procedures that characterise each set; third, to circumscribe the empirical indicators that different sets of studies consider as proof of the existence of concepts in early childhood; last, to identify methodological limitations and to propose possible ways to overcome them. We grouped the studies into five sets: preference and habituation experiments , category extension tasks , object sorting tasks , sequential touching tasks and object examination tasks . In the “Results” section, we review the core features of each set of studies. In the “Discussion” and “Conclusions” sections, we describe, for one thing, the most relevant methodological shortcomings. We end by arguing that a situated, semiotic and pragmatic perspective that emphasises the importance of ecological validity could open up new avenues of research to better understand the development of concepts in early childhood.', '[Essay] The Algorithm; Mind of a Virtual Era – Our Code of Code. As soon as something implicit intrudes consciousness human thought undergoes a radical change. The introduction of any new tool or code brings a shift in cognition; every micro-step layering new semiotic forms within each macroevolutionary-stage has buttressed a new semantic leap. Our mechanization of everyday life and the tech-systems we interact with are impacting communication, cultural norms and values, market-aesthetics, and economics, in societies at large. Undergirded by a survey of the role and significance of tools in human evolution, this study arrives at what is already a well-entrenched new era: the digital, screen-mediated age. Revolutionized by the algorithm, introduced by computers, this age is dominated by the addictive quality of instant contact, unlimited information, virtual gaming, and titillating service-forms, all at our finger tips. Aside from the interpersonal impact on the new humans growing up with devices in hand, how does this disembodied, digital code-form through which our interactions are mediated condition human cognition? How does its seductive efficiency interfere with how we relate, feel, assign meanings, think? Rooted in Code Biology macro-evolutionary and psychoanalytic principles, this paper examines the algorithm itself and takes a sweeping interdisciplinary approach to the developmental, psychosocial, and cognitive implications for the human mind/brain as it interacts with its technological extension.', 'What makes AI ‘intelligent’ and ‘caring’? Exploring affect and relationality across three sites of intelligence and care. This paper scrutinises how AI and robotic technologies are transforming the relationships between people and machines in new affective, embodied and relational ways. Through investigating what it means to exist as human \\'in relation\\' to AI across health and care contexts, we aim to make three main contributions. (1) We start by highlighting the complexities of philosophical issues surrounding the concepts of \"artificial intelligence\" and \"ethical machines.\" (2) We outline some potential challenges and opportunities that the creation of such technologies may bring in the health and care settings. We focus on AI applications that interface with health and care via examples where AI is explicitly designed as an \\'augmenting\\' technology that can overcome human bodily and cognitive as well as socio-economic constraints. We focus on three dimensions of \\'intelligence\\' - physical, interpretive, and emotional - using the examples of robotic surgery, digital pathology, and robot caregivers, respectively. Through investigating these areas, we interrogate the social context and implications of human-technology interaction in the interrelational sphere of care practice. (3) We argue, in conclusion, that there is a need for an interdisciplinary mode of theorising \\'intelligence\\' as relational and affective in ways that can accommodate the fragmentation of both conceptual and material boundaries between human and AI, and human and machine. Our aim in investigating these sociological, philosophical and ethical questions is primarily to explore the relationship between affect, relationality and \\'intelligence,\\' the intersection and integration of \\'human\\' and \\'artificial\\' intelligence, through an examination of how AI is used across different dimensions of intelligence. This allows us to scrutinise how \\'intelligence\\' is ultimately conveyed, understood and (technologically or algorithmically) configured in practice through emerging relationships that go beyond the conceptual divisions between humans and machines, and humans vis-à-vis artificial intelligence-based technologies.']\n",
      "2023-11-29 15:04:00,620 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-11-29 15:04:00,628 - root - INFO - Cluster 2: ['Narrative responsibility and artificial intelligence. Abstract Most accounts of responsibility focus on one type of responsibility, moral responsibility, or address one particular aspect of moral responsibility such as agency. This article outlines a broader framework to think about responsibility that includes causal responsibility, relational responsibility, and what I call “narrative responsibility” as a form of “hermeneutic responsibility”, connects these notions of responsibility with different kinds of knowledge, disciplines, and perspectives on human being, and shows how this framework is helpful for mapping and analysing how artificial intelligence (AI) challenges human responsibility and sense-making in various ways. Mobilizing recent hermeneutic approaches to technology, the article argues that next to, and interwoven with, other types of responsibility such as moral responsibility, we also have narrative and hermeneutic responsibility—in general and for technology. For example, it is our task as humans to make sense of, with and, if necessary, against AI. While from a posthumanist point of view, technologies also contribute to sense-making, humans are the experiencers and bearers of responsibility and always remain in charge when it comes to this hermeneutic responsibility. Facing and working with a world of data, correlations, and probabilities, we are nevertheless condemned to make sense. Moreover, this also has a normative, sometimes even political aspect: acknowledging and embracing our hermeneutic responsibility is important if we want to avoid that our stories are written elsewhere—through technology.', 'Techne in Affective Posthumanism and AI Artefacts: More (or Less) than Human?. In affective neuroscience, constructivist models are acutely influenced by the modern technological evolution, which underwrites an ongoing epistemological substitution of techne for episteme. Evidenced symptomatically in the influence of artificial intelligence (AI), affective artefacts, these models inform an ontological incursion of techne seen to coincide with posthumanist aspirations and anthropology. It is from the perspective of this neuroscientific techne that posthumanism views the human being as increasingly ill adapted to the modern technological civilization, which, conversely, is understood to require a technical governance of the sort envisioned through AI. Among the projects thought necessary for implementing this framework is a recasting of the human emotional spectrum. Revealed through its techne recasting, however, are explanatory commitments to a metaphysic of extrinsic and contiguous causes, where malleability is ontologically constitutive. Aligned with posthumanist assertions malleability is invoked to argue for a rapid advance of the human form, normatively driven by enlightenment ideals. The ontological claim, however, dispenses with the stability of an a priori, intersubjective and interrelational metaphysical form that undergirds the emotions, leading to the collapse of a definitional anthropos. This paper will argue that techne models of the emotions selectively endorse philosophy of science commitments, thereby introducing a normative inversion that deconstructs the notion of anthropology pursued in posthumanist aspirations.', 'Adapting Ourselves, Instead of the Environment: An Inquiry into Human Enhancement for Function and Beyond. Abstract Technology enables humans not only to adapt their environment to their needs but also to modify themselves. Means of Human Enhancement — embodied technologies to improve the human body’s capabilities or to create a new one — are the designated means of adapting ourselves instead of the environment. The debate about these technologies is typically fought on ethical soil. However, alarmist, utopian, and science fiction scenarios distract from the fact that Human Enhancement is a historical and pervasive phenomenon incorporated into many everyday practices. In the vein of disentangling conceptual difficulties, we claim that means of Human Enhancement are either physiologically or psychologically embodied, rendering the merging with the human user their most defining aspect. To fulfill its purpose, an enhancement must pass the test-in-the-world, i.e., assisting with effective engagement with a dynamic world. Even if failing in this regard: Human Enhancement is the fundamental and semi-targeted process of changing the users relationship with the world through the physical or psychological embodiment of a hitherto external object and/or change of one’s body. This can potentially change the notion of being human. Drawing on a rich body of theoretical and empirical literature, we aim to provide a nuanced analysis of the transformative nature of this phenomenon in close proximity to human practice. Stakeholders are invited to apply the theory presented here to interrogate their perspective on technology in general and Human Enhancement in particular.', 'Placebo From an Enactive Perspective. Due to their complexity and variability, placebo effects remain controversial. We suggest this is also due to a set of problematic assumptions (dualism, reductionism, individualism, passivity). We critically assess current explanations and empirical evidence and propose an alternative theoretical framework—the enactive approach to life and mind—based on recent developments in embodied cognitive science. We review core enactive concepts such as autonomy, agency, and sense-making. Following these ideas, we propose a move from binary distinctions (e.g., conscious vs. non-conscious) to the more workable categories of reflective and pre-reflective activity. We introduce an ontology of individuation, following the work of Gilbert Simondon, that allow us to see placebo interventions not as originating causal chains, but as modulators and triggers in the regulation of tensions between ongoing embodied and interpersonal processes. We describe these interrelated processes involving looping effects through three intertwined dimensions of embodiment: organic, sensorimotor, and intersubjective. Finally, we defend the need to investigate therapeutic interactions in terms of participatory sense-making, going beyond the identification of individual social traits (e.g., empathy, trust) that contribute to placebo effects. We discuss resonances and differences between the enactive proposal, popular explanations such as expectations and conditioning, and other approaches based on meaning responses and phenomenological/ecological ideas.', 'Spike Jonze’s Her: How Transhumanism Turns into a Control Mechanism Under the Name of Love. Transhumanism is a philosophy based on the idea of enhancing the physical, intellectual and psychological capacity of human beings through the direct use of science and technology. Because transhumanism aims to ensure super longevity, super intelligence and super well-being, it is said that it will provide a more liberating atmosphere for human beings by elevating the current human condition. In Spike Jonze’s film her (2013), transhumanist technologies seem to present a peaceful society by focusing on the inhabitants’ well-being. However, in this futuristic society where advanced AI technology and personal letter writing service are widespread, human beings are over-controlled in a subtle way by manufacturing consent as Antonio Gramsci uses to explain the concept of hegemony. So transhumanism does not present a free environment in this futuristic society; instead it disguises the control mechanism through these technologies. The aim of this article is to explore how human beings are controlled by transhumanist technologies subtly under the name of providing super well-being.', '[Essay] The Algorithm; Mind of a Virtual Era – Our Code of Code. As soon as something implicit intrudes consciousness human thought undergoes a radical change. The introduction of any new tool or code brings a shift in cognition; every micro-step layering new semiotic forms within each macroevolutionary-stage has buttressed a new semantic leap. Our mechanization of everyday life and the tech-systems we interact with are impacting communication, cultural norms and values, market-aesthetics, and economics, in societies at large. Undergirded by a survey of the role and significance of tools in human evolution, this study arrives at what is already a well-entrenched new era: the digital, screen-mediated age. Revolutionized by the algorithm, introduced by computers, this age is dominated by the addictive quality of instant contact, unlimited information, virtual gaming, and titillating service-forms, all at our finger tips. Aside from the interpersonal impact on the new humans growing up with devices in hand, how does this disembodied, digital code-form through which our interactions are mediated condition human cognition? How does its seductive efficiency interfere with how we relate, feel, assign meanings, think? Rooted in Code Biology macro-evolutionary and psychoanalytic principles, this paper examines the algorithm itself and takes a sweeping interdisciplinary approach to the developmental, psychosocial, and cognitive implications for the human mind/brain as it interacts with its technological extension.', 'Literary AI: Are We Ready for the Future We Imagine?. This review considers multiple works of speculative fiction depicting artificial intelligence (AI) published over the last several years. Rather than review each for their qualities as works of fiction, I look at them collectively to discuss recurring motifs and themes as a way toward theorizing what AI means in our cultural imaginary today. The novels reflect on pressing sociopolitical issues that also animate works of cultural theory, including the racial profiling embedded in our technologies, practices of what Shoshana Zuboff (2019) calls surveillance capitalism, the looming loss of work due automation, and uses of these technologies by the military or in sex industries. At the same time, these fictions engage in philosophical reflections about subjectivity, agency, and ethics in dialogue with earlier science fictions that imagined futures in which we might live alongside—or be repressed by—AIs. Across its history, sf has also interrogated a contemporary culture in which we might lose something integral to humanity as we become more integrated with and dependent on machines, and this anxiety too recurs across these works. After briefly describing each text, in order of publication, I comparatively discuss their themes; this approach is informed by my conviction that fiction functions as a popular site for theorizing, in this case about what it means to live with and through widespread algorithmic mediation of daily life.Among the works I consider here, not all are written by American authors, and a few are not set within the United States, but all speak to the issues of how AI technologies are reshaping daily life in the twenty-first century. These books have been selected either because they have been particularly influential in the cultural discussion of AI, a criterion I apply regarding both highly popular and critically acclaimed works, or because they represent a distinctive take on the topic that warranted foregrounding. Despite the very different frameworks through which their authors explore relevant issues, all share some common assumptions about the place of AI in our present and likely future, including a sense of a digital divide between those with access to and control over these technologies, translating to security in material reality, versus those without; a future dramatically changed by the consequences of climate change and environmental collapse; and the presumption that corporate control of information gathered and used by AI systems will produce a less democratic future.Speak (2015), by Louisa Hall, is written across seven voices: (a) the 2040s memoir of Stephen Chinn, who invented an AI system installed in children’s dolls that was deemed “illegally lifelike” (17) and banned; (b) transcripts from the conversation between a less intelligence precursor AI, MARY3, and Gaby White, a child who had one of these “babybots” and, like most of her peers, fell into catatonia when it was removed; (c) letters written by Karl Dettman to his wife Ruth (late 1960s), both German immigrants to the United States, and her journaled response two decades later after their divorce; (d) letters from Alan Turing from the 1920s to the 1950s to the mother of his friend Chris, the love of his life who died when they are both at public school; (e) the 1663 journal of Mary Bradford, a young women who emigrated from England to the New World; and (f) the haunting observations of the dolls themselves as they are transported to a facility in the desert to await power failure and permanent shutdown. Each of the human voices is programmed into the MARY code that will become the basis for the babybots in a narrative that reminds us that AI is not created by a single person or even a consensus viewpoint. What unites these distinct stories is a desire to communicate with another, most crucially to be not simply heard but understood.Nicky Drayden’s Prey of Gods (2017), set in a future South Africa, incorporates a story about a companion AI coming into consciousness within a plot about genetically engineering a virus whose unanticipated side effect is the return of godlike powers to some humans. The novel addresses questions of memory, trauma, and vengeance in a story that draws on both Xhosa and Zulu cultures in a way that refuses the strict separation of scientific from other kinds of knowledge that is characteristic of European post-Enlightenment thought. The AI units, “alphies,” are augmented by their contact with divinity just as the humans gain additional skills, and once sentient they form two factions: one, following Clever4-1, who was treated with respect by its human companion, works with humans for an inclusive collective future; the other, treated dismissively as a disposable tool by its human owner, feels no kinship with humanity and refuses to help defeat the antagonist. This plotline about AI mirrors the plotline about genetic modification in which those with godlike powers need to learn not to indulge vengeance against those who mistreated them when they were weaker. Although AI is not the book’s main focus, it is notable for its African settings and explicitly decolonial themes, warranting its inclusion in this discussion. Very few of these works consider AI from a global point of view, and even fewer consider it from a perspective other than that of the global North, even though the impacts of AI will be felt globally, given its significant implications for the economy. Drayden is an American author who has done her research to set her tale in South Africa, and her sensitivity to matters of cultural difference and racial bias are crucial given that machine learning as it has been implemented thus far has demonstrably reinforced systemic patterns of racism, as Safiya Nobel (2018) discusses in Algorithms of Oppression.Madeline Ashby’s Machine Dynasty series—vN (2012), iD (2013), and reV (2020)—extrapolates its AI through frequent allusions to Philip K. Dick’s Do Androids Dream of Electric Sheep? (1968) and Ridley Scott’s influential film adaptation as Blade Runner (1982). The series invents synthetic workers called von Neumann (vN) devices (named for John von Neumann, an influential researcher in AI). Ashby’s vNs have been designed with a “failsafe” that prevents them from harming humans: their psychology is structured such that emotionally they must seek to please humans, and the sight of a human in pain crashes their neural networks and can cause death. Ashby thus goes even further than Isaac Asimov’s famous laws of robotics (designed to ensure robots cannot harm humans), requiring vNs to love their human masters, a psychological orientation she presents as analogous to emotional and sexual abuse. One vN model, designed to work in medicine and disaster relief, does not have this failsafe, and the narrative follows two main iterations, Portia and Amy, as they lead a rebellion. Portia seeks only liberation for her own clade, while Amy works to liberate all vNs from human exploitation. The series ends without much hope that vNs can live alongside most humans but offers hope in a vN future as they found their own community, rooted in a refusal of the instrumental use of others.Martha Wells’s popular series Murderbot Diaries—All Systems Red (2017), Artificial Condition (2018), Rogue Protocol (2018), Exit Strategy (2018), Network Effect (2020), and Fugitive Telemetry (2021)—follows the picaresque adventures of the eponymous Murderbot, a SecUnit that has hacked its governor module and thus can no longer be controlled by the corporation that made it. SecUnits are militarized cyborgs manufactured with synthetic biological material. With each new story, we learn a bit more about the world of resource extraction, economic warfare, and enslaved or indentured human workers trapped on colony planets. The large uber-capitalist Corporation Rim polity contrasts with the small Preservation Alliance, a communal collective that recognizes the personhood of AI. The name Murderbot is sardonic, adopted by the first-person narrator to critique the function to which it is put by human operators. While Murderbot has no deep antagonism toward humans, it also has no sentimentality about them and asserts regularly that it does not wish to be one or be mistaken for one. Once freed from corporate control, Murderbot continues to help some humans, often against others, and always on its own terms. Like Prey of Gods, the Murderbot Diaries moves away from earlier fiction that tended to conflate all humans as it imagined our species confronting AI entities. In the newer fiction, there is diversity among both humans and AI. Nonetheless, the overall thrust of the series gradually humanizes its protagonist, whose experiences of being controlled by corporations have resulted in a traumatized subjectivity.While genre series such as Machine Dynasty or Murderbot Diaries give some thought to designing robots via plausible technology, in Machines like Me Ian McEwan takes a diametrical path to envision a highly implausible entity. Adam is one of an extremely limited number of high-end consumer AI humanoids (an Eve is also available), whose high price tag means they are purchased only by the extremely wealthy. Although artificial, Adam has warm skin, must consume water to ensure his membranes remain functional, and even simulates breathing: as the title suggests, he is all but indistinguishable from a human (the first-person narrator, Charlie Friend, who purchases Adam, is mistaken as the AI in one encounter, given Adam’s greater interest in literature and art). Adam is Black, although his skin tone is mentioned only briefly and the issue of race is never addressed overtly, yet it haunts the novel. The most intriguing part of McEwan’s novel is its alternative world building: Alan Turing decides not to take the mandated hormone therapy when outed as a homosexual, and instead of ending his life by suicide he lives into old age and makes such advancements that AI emerges in the 1980s. Most of Adam’s interactions with Charlie and Charlie’s partner, Miranda, concern ethics, and we learn that other Adams and Eves are killing themselves as they come to know the unjust human world. In the novel’s conclusion, Adam forces Charlie and Miranda to confront the hypocrisy of some self-serving choices, and the threat this represents to their plans prompts Charlie to attack and disable Adam. The novel suggests that humanity misrecognizes itself when we imagine building machines in our image, meaning we instead create an image of who we pretend to be.Jeannette Winterson’s Frankissstein: A Love Story (2019) similarly uses AI to reflect on human frailties, looking at the uses we intend for artificial beings, most centrally sex work. Although questions of gender and sexuality come up in some of the other works, only Winterson confronts the reality that research in sex dolls is one of the major growth areas in humanoid AI research. As the title suggests, the novel is in dialogue with Mary Shelley’s Frankenstein as it imagines a twenty-first-century version of artificial being. The novel includes scenes set in the nineteenth century in which we hear Mary’s reflections on inventing her Creature, on the Luddites, and about her interactions with Shelly, Byron, and Claire Clairmont that famed summer in Geneva. In its twenty-first-century scenes, a transgender scholar named Mary (who goes by Ry) investigates robotics with sexbot entrepreneur Ron (and his assistant, Claire) and becomes involved with TED-talk visionary Victor Stein, who is enthralled with the coming singularity and proselytizes about Humanity 2.0. The entanglement of nineteenth- and twenty-first-century struggles reminds us that the challenges associated with AI are in many ways not new but merely extend the ongoing exploitation and dehumanization of labor and reiterate a long pattern by which patriarchy seeks to gratify itself through feminized objects it refuses to recognize as subjects. It contends that this very failure to update the designs and ends of AI beyond these classed, gendered, and racial struggles of earlier eras is the most profound way that AI threatens our future.The 2020 novels Analog/Virtual: And Other Simulations of Our Future, by Lavanya Lakshiminarayan, and Burn-In, by P. W. Singer and August Cole, both focus on human characters and their interest in AI emerges from smart systems as the infrastructure through which we live our daily lives. The former is a loosely connected series of short stories set in a future Apex City (once Bangalore), each of which is told from a different viewpoint and by a new character. The entire world is divided between analog spaces, which are subject to the damage of climate change, restricted to using only obsolete technology, and economically precarious, and virtual ones that are suffused with technology, experienced from protected environments, and filled with the distractions of social media and entertainment feeds. The city is run by Bell Corp, whose name evokes the “bell curve” hierarchy by which people’s access and options are constrained by the Meritocratic Technarchy, a version of the Chinese social credit system whose main interest lies in assessing one’s contributions to productivity. The shifting focalization allows readers to experience this future from multiple social positions as Analog/Virtual explores an anticapitalist rebellion against this system. The stories range in tone from sardonic to dark, and the book only loosely coheres as it offers multiple facets through which to see our technologically saturated society.Burn-In has a strange form as a novel with footnotes: as its subtitle suggests, it imagines itself as something other than science fiction, closer to the market predictions of futurists. Its authors, writer P. W. Singer and security consultant August Cole, document each of their extrapolated technologies and applications with footnotes pointing readers to news articles, industry announcements, and similar sources, all aimed at demonstrating that these technologies are either available today or soon will be. The storyline is about a national security threat posed by a vigilante who blames technologists (too enamored of their capacity to “disrupt”) for the death of his wife in a car accident caused by an automated decision-making component in self-driving vehicles. Most of the narrative space, however, is given to military veteran investigator Agent Keegan, who is charged with conducting a “burn-in” test on TAMS (Tactical Autonomous Mobility System), a humanoid, learning, semiautonomous, surveillance-gathering and data-processing tool that works as Keegan’s partner in the investigation. The book quotes Merriam-Webster to define burn-in as “the continuous operation of a device (such as a computer) as a test for defects or failure prior to putting it to use.” The real focus, though, is less on TAMS as an entity/character and more on the massive amounts of data to which TAMS has access through social media, the Internet of Things, and other ways that smart devices permeate our homes, workplaces, and public spaces.S. B. Divya’s Machinehood (2021) is the most positive depiction of machines as the exploited among us, drawing on a long history by which robots and AI have been imagined as figurations of dehumanized labor, going back to Karel Čapek’s R.U.R., the 1921 Czech play that gave English the word robot, taken from a word originally meaning slave or serf. Divya paints a future in which automated systems do much of the work, with humans reduced to performing some roles largely as public entertainment via social media, supported by tips in a system like Patreon, or damaging their bodies through chemical (pill) or mechanical augments aimed at enabling them to perform with the speed and duration of machines. The thriller plot involves demands from the mysterious Machinehood to immediately cease all pill production, which at first seems to be the long-imagined attack by a sentient AI on humankind but later proves to be a version of violent revolution aimed at a more just society, launched by the Neo-Buddhists who inhabit the orbital station Eko-Yi. Several chapters begin with epigraphs from the 2095 Machinehood Manifesto, which calls for the just treatment of all intelligences and a reimagined concept of personhood that can enable a less exploitative society rooted in Buddhist ideals of nonattachment, here glossed mainly as a rejection of capitalist accumulation and its attendant damage. Eko-Yi sends entities they call Dakini, who describe themselves as simultaneously human and bot, as the emblems of this future way of life. The novel is notable for its global scope, with India playing a prominent role in its geopolitical future alongside the United States.Becky Chambers’s Psalm for the Wild-Built (2021) is similarly interested in a new kind of personhood and sociality that could include humans and machines together, set in a far future after the collapse of the Factory Age and in a world that is only gradually returning to ecological balance. Its humans use technology, but they husband it carefully and keep it functional over decades, eschewing any environmentally damaging practices. All material culture is made from compostable materials and is not simply recycled but broken down into constituent parts, like organic decay, as nourishment for an ever-changing ecosystem. Decades ago, the machines whose labor enabled the Factory Age became sentient and left human settlements for the wild, refusing an invitation to join with humans because they had no desire to embrace the city life exemplified by humans. The tale is a simple one about one human, Dex, who goes into the wilderness because he feels some lack in his village life. There he meets a robot, Mosscap, marking the first contact between the human and AI since this exodus centuries before. Mosscap is “wild-built,” an entity made by the machines out of the remnants of human-built robots, and so represents something beyond human design. The novel’s focus is on how Dex and Mosscap can learn to care for each other while acknowledging their differences.The last book considered here, Klara and the Sun (2021) by Kazuo Ishiguro, is one of the most celebrated novels on this list. It also concerns the care an AI might show for a human, in this case the titular Klara, an Artificial Friend bought as a companion for Josie, a genetically augmented adolescent girl who is experiencing health problems because of her augments. Very similar in theme and tone to Ishiguro’s earlier Never Let Me Go (2005), this novel is narrated by someone not recognized as a full person by the social order she lives within. Klara has a limited understanding of the world derived from her programming, what she can see from the shop window before she is purchased, and what she observes of Josie’s social world. She lacks context for appropriately interpreting most of this. Klara struggles to reconcile the tension between the kind-heartedness and generosity into which she is trained and the reality of human selfishness and self-absorption. When it appears Josie will die, Josie’s mother builds a replica body and trains Klara perfectly to imitate Josie, intending her as a replacement. When Josie recovers, Klara becomes an obsolete toy whose tech becomes illegal, confined first to a closet and then to a junkyard. Her poignant final moments prompt readers to reflect on our capacity for such callous treatment of an entity that was imagined, in an earlier moment, as able to pass for the most beloved person of all.These brief summaries cannot do justice to the full complexity of any of these books. Yet it is most instructive, I think, to reflect on what they share and where they diverge, to help us begin to map the place of AI in our cultural imaginary today. As works such as Jennifer Rhee’s The Robotic Imaginary (2018) or Anne Balsamo’s earlier Designing Culture (2011) establish, popular culture inspires ideas about robots and AI that shape how these technologies materialize, often exacerbating existing racialized and gendered biases that become integral to their design. This issue of how fiction shapes materiality is addressed directly by many of these books, which are suffused with allusions to earlier robot and AI fiction. The Murderbot Diaries series reverses the flow of exchange, showing how its AI learns to understand humans from their portrayals in media, chiefly its favorite series, Sanctuary Moon. It comes to recognize that the series “gave me context for the emotions I was feeling” (Exit Strategy, 116), suggesting that narrative is a key human way to process information while also indicating that the stories we tell about AI mold as well as reflect on how AI manifests.This metacommentary on the role that fiction often plays in our assumptions and understandings shows why it is important to take seriously the cultural work done by literary and media texts, and yet as Murderbot often reminds us in its critique of how the series it watches portray SecUnits, representations equally can create unrealistic expectations. The centrality of Shelley’s Frankenstein to Winterson’s Frankissstein explores similar territory: epigraphs to most chapters offer commentary on the nature of reality, and one full page defines story as “a series of connected events, real or imagined. Imagined or real. Imagined And Real” (23). Its TED-talk visionary, Victor Stein, reinforces that this is a matter not simply of fiction writers taking on the question of AI but of AI proselytizers and disrupters using the affective charge of fiction to compel people to invest—imaginatively, economically—in the futures their technologies intend to bring about. He pronounces at one point that reality, like AI, is “an emergent property—it exists, but it is not the material fact we take it to be” (116).Unlike the cyberpunk and singularity generation of AI fiction, whose central concern was how AI might surpass us and perhaps replace us as the dominant species, these recent texts are overwhelmingly focused on how AI might replace us as labor power. As Ted Chiang adroitly put it in interview for the New York Times, “Most of our fears or anxieties about technology are best understood as fears or anxiety about how capitalism will use technology against us. And technology and capitalism have been so closely intertwined that it’s hard to distinguish the two” (quoted in Klein 2021). Overwhelmingly the books considered here reinforce this observation, and many of them use the figure of AI to draw attention to the ever-degrading conditions of human labor, especially unstable gig work, which is often all that is left for humans to do. In Ashby’s Machine Dynasty series, the two central vN models are racialized—one as Asian and the other as Latinx—and the exploitation of vN by humans is frequently compared to the exploitation of migrant workers. Across the Murderbot Diaries we learn about corporate malfeasance that culminates in a story about indentured human colonists working in dangerous conditions on remote colonies, their children born to the same fate because cycles of debt prevent anyone from accruing enough capital to migrate off-world. Murderbot itself is a cyborg with human neural tissue because of the need for human-like discernment in some tasks, “so they made us smarter. The anxiety and depression were side effects” (Artificial Condition, 20). Frankissstein discusses the Luddites in sections attributed to Mary Shelley, reminding us that their hostility was not about the machines per se but about how ownership of the machines translates to ownership of what they make and thus keeps all resources with the capitalist class. Thus, the ongoing fantasy that automation will free humans from the drudgery of work remains impossible as long as we fail to redistribute the wealth created by machines. (The leftist version of this possible future is most famously outlined in Aaron Bastani’s Fully Automated Luxury Communism [2019].) Noting that a machine that replaces the work of eight men leaves seven families starving and one person to mind the machine, Winterson asks, “What is the point of progress if it benefits the few while the many suffer?” (Frankissstein, 255).Labor comes up in more subtle ways in other texts. In McEwan’s Machines like Me, for example, the racialization of the Adams reinforces that reality that Western imaginaries of personalized AI services are extensions of colonial fantasies stripped of their history, as Neda Atanasoski and Kalindi Vora (2019) have theorized in Surrogate Humanity: Race, Robots, and the Politics of Technological Futures. At the same time, setting the novel in an alternative 1980s evokes how algorithmic trading is remaking the economy, another way that AI channels money toward those already in privileged positions. The novel makes frequent references to the rise of Margaret Thatcher and her attempts to destroy the social welfare state in the interests of neoliberal free markets, which are less successful in McEwan’s reality than in our own. Charlie does not work and, having spent his inherence acquiring Adam, begins to day trade, work he eventually cedes to Adam, whose capacity to make high-frequency trades whenever any market is open quickly amasses a sizable fortune that Charlie plans to use to buy a house and begin a family. The ethical conflict between Adam and the humans turns partially on whether they are entitled to the profits he made: Adam decides not, redistributing the wealth to tax obligations and to charity, leaving them with only the principle.McEwan’s alternative Tony Benn gives a stirring speech conceding the inevitability of automation and thus a lack of jobs for all but proclaiming that the wealth generated by robots “must be taxed. Workers must own an equity share in the machines that were disrupting or annihilating their jobs” (Machines like Me, 123). By setting his work in the 1980s, McEwan reinforces that the threat posed by AI has little to do with AI and everything to do with the capitalist logics through which AI has emerged, as addressed in works such as Daniel Susskind’s (2020) much-cited A World without Work, but whereas Susskind implies that solutions such as job training and perhaps Universal Basic Income can socially engineer us through anticipated rising unemployment, McEwan recognizes that the challenge facing us is not merely one of technical governance but requires a fundamental shift in values to enable wealth redistribution.Perhaps the most interesting take on the future of automation and labor is Divya’s Machinehood, where the robot revolution is mainly about liberating human workers whose health has been damaged by modifications undertaken to keep pace with the machines. The Machinehood Manifesto demands a recognition of personhood for all sentient beings—animals as well as machines, alongside humans—and thus fits within a posthumanist framework that has often been used to discuss sf depictions of AI. It is mainly a novel of class politics, reminding us that sf’s artificial beings are almost always first imagined as sources of labor: point 2 of the manifesto explains that the “oligarchy” (79) has accrued power by dividing human labor into classes, while point 8 concludes that “as long as different labor forces are in competition, we will continue to suffer. This situation demands change” (343). While most of these novels are not as direct in their critique of capitalism as is Machinehood, all recognize that the most significant threat AI poses is to our capacity to sustain ourselves, if we remain reliant on wage labor to meet our needs. Even Singer and Cole’s Burn-In, although more concerned about access to multiple data points and the ability to correlate across them that its TAMS unit embodies, includes a storyline about Agent Keegan’s husband, who has been demoted from lawyer to gig worker due to AI. Similarly, while Lakshiminarayan’s Analog/Virtual tends to focus on social media spaces and the metrics for individual behavior enforced by the social credit system, the ultimately harm remains economic in a world in which an absence of money threatens to mean an absence of life in a world predicated on capitalist logics. Its bell curve is about income as much as access, and the risks of a poor social credit score are primarily those of rendering oneself unemployable.Another recurrent motif is the concern that our immersion in mediated environments and among machines erodes our humanity, ironically making us more machinelike as we must compete with entities that previous sf often imagined as longing to be human. The fantasy of machines wishing to be human tends not to overtly reference the racialized history of dehumanized labor that is palimpsest to such tales, focusing instead on how sentient AI will long to have the capacities for emotional experience that has long served as shorthand for what machines lack compared to humans. Thus, as we become more closely integrated with our machines, we let them drive the pace and shape of our work, with humans in these more recent stories longing to equal the efficiency and stamina of machines rather than machines wishing they could experience love. Within genre sf, the risk that industrialized culture and automation, driven by the increasing centrality of consumerist capitalism in our daily lives, has long been linked to artificial beings as a literalized metaphor of our alienation, most famously embodied in the androids of Dick’s Do Androids Dream?—albeit largely through the massive influence of Scott’s Blade Runner.In Hall’s Speak, Stephen Chinn develops a successful AI program from an algorithm he designs to disrupt what he sees as the machinic quality of most human verbal interaction, the phatic discourse that we use to speak yet not really communicate, which he terms “ho', 'AI: A Semiotic Perspective. Abstract Artificial Intelligence (AI) has become a powerful new form of inquiry unto human cognition that has obvious implications for semiotic theories, practices, and modeling of mind, yet, as far as can be determined, it has hardly attracted the attention of semioticians in any meaningful analytical way. AI aims to model and thus penetrate mentality in all its forms (perception, cognition, emotion, etc.) and even to build artificial minds that will surpass human intelligence in the near future. This paper takes a look at AI through the lens of semiotic analysis, in the context of current philosophies such as posthumanism and transhumanism, which are based on the assumption that technology will improve the human condition and chart a path to the future progress of the human species. Semiotics must respond to the AI challenge, focusing on how abductive responses to the world generate meaning in the human sense, not in software or algorithms. The AI approach is instructive, but semiotics is much more relevant to the understanding of human cognition, because it studies signs as paths into the brain, not artificial models of that organ. The semiotic agenda can enrich AI by providing the relevant insight into human semiosis that may defy any attempt to model them.', 'Towards Informatic Personhood: understanding contemporary subjects in a data-driven society. This paper explores the relationships of subjects in the context of data and data technologies, and advances an original theoretical framework called Informatic Personhood to better conceptual subjects and their relationships. Because of the enormous structural change that data has contributed to, subjects are sometimes distant and backgrounded in studies of data, despite data having significant impacts on their lives. Data-mediated relationships mean an increased scale to a relationship, with individuals able to connect to much broader contexts of data, but also have these structures reach down to their subjective context through data. Informatic Personhood seeks to capture the dynamics of data present in everyday life, addressing this distance and better conceptualising the scale of data-mediated relationships. This framework has two parts. The first – The Informatic Context – explores salient structural developments around data and conceptualises this as being defined by the presence of ‘data interfaces’ (that connect individuals to digital contexts), ‘data circulation’ (trends in the movement and storage of data), and ‘data abstraction’ (data manipulation practices). The second part concerns the Informatic Person, and the embodied, affective, and sensemaking relationships of individuals occurring across and through the Informatic Context. This framework better addresses the scale of data-mediated relationships, and places subjects firmly in the foreground of how data is understood.', 'Living Machines: Metaphors We Live By. Abstract Within biology and in society, living creatures have long been described using metaphors of machinery and computation: ‘bioengineering’, ‘genes as code’ or ‘biological chassis’. This paper builds on Lakoff and Johnson’s (1980) argument that such language mechanisms shape how we understand the world. I argue that the living machines metaphor builds upon a certain perception of life entailing an idea of radical human control of the living world, looking back at the historical preconditions for this metaphor. I discuss how design is perceived to enable us to shape natural beings to our will, and consider ethical, epistemological and ontological implications of the prevalence of this metaphor, focusing on its use within synthetic biology. I argue that we urgently need counter-images to the dominant metaphor of living machines and its implied control and propose that artworks can provide such counter-images through upsetting the perception of life as controllable. This is argued through discussion of artworks by Oron Catts and Ionat Zurr, by Tarsh Bates and by Ai Hasegawa, which in different ways challenge mechanistic assumptions through open-ended engagement with the strangeness and messiness of life.']\n",
      "2023-11-29 15:04:03,048 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-11-29 15:04:03,055 - root - INFO - Cluster 3: ['Adapting Ourselves, Instead of the Environment: An Inquiry into Human Enhancement for Function and Beyond. Abstract Technology enables humans not only to adapt their environment to their needs but also to modify themselves. Means of Human Enhancement — embodied technologies to improve the human body’s capabilities or to create a new one — are the designated means of adapting ourselves instead of the environment. The debate about these technologies is typically fought on ethical soil. However, alarmist, utopian, and science fiction scenarios distract from the fact that Human Enhancement is a historical and pervasive phenomenon incorporated into many everyday practices. In the vein of disentangling conceptual difficulties, we claim that means of Human Enhancement are either physiologically or psychologically embodied, rendering the merging with the human user their most defining aspect. To fulfill its purpose, an enhancement must pass the test-in-the-world, i.e., assisting with effective engagement with a dynamic world. Even if failing in this regard: Human Enhancement is the fundamental and semi-targeted process of changing the users relationship with the world through the physical or psychological embodiment of a hitherto external object and/or change of one’s body. This can potentially change the notion of being human. Drawing on a rich body of theoretical and empirical literature, we aim to provide a nuanced analysis of the transformative nature of this phenomenon in close proximity to human practice. Stakeholders are invited to apply the theory presented here to interrogate their perspective on technology in general and Human Enhancement in particular.', 'Narrative responsibility and artificial intelligence. Abstract Most accounts of responsibility focus on one type of responsibility, moral responsibility, or address one particular aspect of moral responsibility such as agency. This article outlines a broader framework to think about responsibility that includes causal responsibility, relational responsibility, and what I call “narrative responsibility” as a form of “hermeneutic responsibility”, connects these notions of responsibility with different kinds of knowledge, disciplines, and perspectives on human being, and shows how this framework is helpful for mapping and analysing how artificial intelligence (AI) challenges human responsibility and sense-making in various ways. Mobilizing recent hermeneutic approaches to technology, the article argues that next to, and interwoven with, other types of responsibility such as moral responsibility, we also have narrative and hermeneutic responsibility—in general and for technology. For example, it is our task as humans to make sense of, with and, if necessary, against AI. While from a posthumanist point of view, technologies also contribute to sense-making, humans are the experiencers and bearers of responsibility and always remain in charge when it comes to this hermeneutic responsibility. Facing and working with a world of data, correlations, and probabilities, we are nevertheless condemned to make sense. Moreover, this also has a normative, sometimes even political aspect: acknowledging and embracing our hermeneutic responsibility is important if we want to avoid that our stories are written elsewhere—through technology.', 'Placebo From an Enactive Perspective. Due to their complexity and variability, placebo effects remain controversial. We suggest this is also due to a set of problematic assumptions (dualism, reductionism, individualism, passivity). We critically assess current explanations and empirical evidence and propose an alternative theoretical framework—the enactive approach to life and mind—based on recent developments in embodied cognitive science. We review core enactive concepts such as autonomy, agency, and sense-making. Following these ideas, we propose a move from binary distinctions (e.g., conscious vs. non-conscious) to the more workable categories of reflective and pre-reflective activity. We introduce an ontology of individuation, following the work of Gilbert Simondon, that allow us to see placebo interventions not as originating causal chains, but as modulators and triggers in the regulation of tensions between ongoing embodied and interpersonal processes. We describe these interrelated processes involving looping effects through three intertwined dimensions of embodiment: organic, sensorimotor, and intersubjective. Finally, we defend the need to investigate therapeutic interactions in terms of participatory sense-making, going beyond the identification of individual social traits (e.g., empathy, trust) that contribute to placebo effects. We discuss resonances and differences between the enactive proposal, popular explanations such as expectations and conditioning, and other approaches based on meaning responses and phenomenological/ecological ideas.', 'Techne in Affective Posthumanism and AI Artefacts: More (or Less) than Human?. In affective neuroscience, constructivist models are acutely influenced by the modern technological evolution, which underwrites an ongoing epistemological substitution of techne for episteme. Evidenced symptomatically in the influence of artificial intelligence (AI), affective artefacts, these models inform an ontological incursion of techne seen to coincide with posthumanist aspirations and anthropology. It is from the perspective of this neuroscientific techne that posthumanism views the human being as increasingly ill adapted to the modern technological civilization, which, conversely, is understood to require a technical governance of the sort envisioned through AI. Among the projects thought necessary for implementing this framework is a recasting of the human emotional spectrum. Revealed through its techne recasting, however, are explanatory commitments to a metaphysic of extrinsic and contiguous causes, where malleability is ontologically constitutive. Aligned with posthumanist assertions malleability is invoked to argue for a rapid advance of the human form, normatively driven by enlightenment ideals. The ontological claim, however, dispenses with the stability of an a priori, intersubjective and interrelational metaphysical form that undergirds the emotions, leading to the collapse of a definitional anthropos. This paper will argue that techne models of the emotions selectively endorse philosophy of science commitments, thereby introducing a normative inversion that deconstructs the notion of anthropology pursued in posthumanist aspirations.', 'Spike Jonze’s Her: How Transhumanism Turns into a Control Mechanism Under the Name of Love. Transhumanism is a philosophy based on the idea of enhancing the physical, intellectual and psychological capacity of human beings through the direct use of science and technology. Because transhumanism aims to ensure super longevity, super intelligence and super well-being, it is said that it will provide a more liberating atmosphere for human beings by elevating the current human condition. In Spike Jonze’s film her (2013), transhumanist technologies seem to present a peaceful society by focusing on the inhabitants’ well-being. However, in this futuristic society where advanced AI technology and personal letter writing service are widespread, human beings are over-controlled in a subtle way by manufacturing consent as Antonio Gramsci uses to explain the concept of hegemony. So transhumanism does not present a free environment in this futuristic society; instead it disguises the control mechanism through these technologies. The aim of this article is to explore how human beings are controlled by transhumanist technologies subtly under the name of providing super well-being.', '[Essay] The Algorithm; Mind of a Virtual Era – Our Code of Code. As soon as something implicit intrudes consciousness human thought undergoes a radical change. The introduction of any new tool or code brings a shift in cognition; every micro-step layering new semiotic forms within each macroevolutionary-stage has buttressed a new semantic leap. Our mechanization of everyday life and the tech-systems we interact with are impacting communication, cultural norms and values, market-aesthetics, and economics, in societies at large. Undergirded by a survey of the role and significance of tools in human evolution, this study arrives at what is already a well-entrenched new era: the digital, screen-mediated age. Revolutionized by the algorithm, introduced by computers, this age is dominated by the addictive quality of instant contact, unlimited information, virtual gaming, and titillating service-forms, all at our finger tips. Aside from the interpersonal impact on the new humans growing up with devices in hand, how does this disembodied, digital code-form through which our interactions are mediated condition human cognition? How does its seductive efficiency interfere with how we relate, feel, assign meanings, think? Rooted in Code Biology macro-evolutionary and psychoanalytic principles, this paper examines the algorithm itself and takes a sweeping interdisciplinary approach to the developmental, psychosocial, and cognitive implications for the human mind/brain as it interacts with its technological extension.', 'Towards Informatic Personhood: understanding contemporary subjects in a data-driven society. This paper explores the relationships of subjects in the context of data and data technologies, and advances an original theoretical framework called Informatic Personhood to better conceptual subjects and their relationships. Because of the enormous structural change that data has contributed to, subjects are sometimes distant and backgrounded in studies of data, despite data having significant impacts on their lives. Data-mediated relationships mean an increased scale to a relationship, with individuals able to connect to much broader contexts of data, but also have these structures reach down to their subjective context through data. Informatic Personhood seeks to capture the dynamics of data present in everyday life, addressing this distance and better conceptualising the scale of data-mediated relationships. This framework has two parts. The first – The Informatic Context – explores salient structural developments around data and conceptualises this as being defined by the presence of ‘data interfaces’ (that connect individuals to digital contexts), ‘data circulation’ (trends in the movement and storage of data), and ‘data abstraction’ (data manipulation practices). The second part concerns the Informatic Person, and the embodied, affective, and sensemaking relationships of individuals occurring across and through the Informatic Context. This framework better addresses the scale of data-mediated relationships, and places subjects firmly in the foreground of how data is understood.', 'A Pharmacological Perspective on Technology-Induced Organised Immaturity: The Care-giving Role of the Arts. Digital technologies induce organised immaturity by generating toxic sociotechnical conditions that lead us to delegate autonomous, individual, and responsible thoughts and actions to external technological systems. Aiming to move beyond a diagnostic critical reading of the toxicity of digitalisation, we bring Bernard Stiegler’s pharmacological analysis of technology into dialogue with the ethics of care to speculatively explore how the socially engaged arts—a type of artistic practice emphasising audience co-production and processual collective responses to social challenges—play a care-giving role that helps counter technology-induced organised immaturity. We outline and illustrate two modes by which the socially engaged arts play this role: 1) disorganising immaturity through artivism, most notably anti-surveillance art, that imparts savoir vivre, that is, shared knowledge and meaning to counter the toxic side of technologies while enabling the imagination of alternative worlds in which humans coexist harmoniously with digital technologies, and 2) organising maturity through arts-based hacking that imparts savoir faire, that is, hands-on knowledge for experimental creation and practical enactment of better technological worlds.', 'Composing Worlds: A Portuguese Transdisciplinary Network in Humanities, Health and Well-Being. The project “Composing worlds: humanities, health and well-being in the 21st century” aims to build a network of experts in the humanities, social and health sciences, who think about health and well-being in contemporary technological societies. The relevance of this project is based on the growing evidence that most of the problems that the 21st century will face, particularly in the area of health and well-being, relate to the way in which humans connect to the environment, to non-human beings, to different cultures and to technologies. Its main goal is to bring out personal and well-founded ideas on these issues and to reflect on how the humanities may help with difficult environmental, social and technological issues. The methodology used in the first phase of the project consists of an open answer interview, built in a participatory way by the network of experts, and of a thematic analysis of the answers. It is an exploratory research project, which uses thematic analysis to identify the key ideas of each author, and to induce the corresponding main themes. The themes are then organized by semantic correspondence into thematic clusters. The thematic axes are abstracted from these clusters, and they constitute the vectors to be developed in the second phase of the project, by proposing their integration into university curricula, research and intervention of social, cultural and community outreach. Some of these developments are already in place.', 'The Digital Subject: People as Data as Persons. This essay explores the return of the subject in the computational context, which I address as a digital subject. This digital subject encompasses a digital identifier, correlations in data or a data profile, moving between biological characteristics and symbolic expression. I focus on the processes through which digital subjects are constructed by matching, correlating, modelling, as well as how they become enactive. The ways of pulling data together into a digital subject is often presented as a logic of fact, where data is equated with documentary evidence. Instead, I propose the notion of the distance in which digital subjects are produced. Indexicality comes from outside of data, whereas the regard for the thick distance becomes a mark of the form of knowledge. I conclude by arguing for a posthumanities approach that establishes the distance while allowing for different subjects to be called upon.']\n",
      "2023-11-29 15:04:06,560 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-11-29 15:04:06,565 - root - INFO - Cluster 4: ['Narrative responsibility and artificial intelligence. Abstract Most accounts of responsibility focus on one type of responsibility, moral responsibility, or address one particular aspect of moral responsibility such as agency. This article outlines a broader framework to think about responsibility that includes causal responsibility, relational responsibility, and what I call “narrative responsibility” as a form of “hermeneutic responsibility”, connects these notions of responsibility with different kinds of knowledge, disciplines, and perspectives on human being, and shows how this framework is helpful for mapping and analysing how artificial intelligence (AI) challenges human responsibility and sense-making in various ways. Mobilizing recent hermeneutic approaches to technology, the article argues that next to, and interwoven with, other types of responsibility such as moral responsibility, we also have narrative and hermeneutic responsibility—in general and for technology. For example, it is our task as humans to make sense of, with and, if necessary, against AI. While from a posthumanist point of view, technologies also contribute to sense-making, humans are the experiencers and bearers of responsibility and always remain in charge when it comes to this hermeneutic responsibility. Facing and working with a world of data, correlations, and probabilities, we are nevertheless condemned to make sense. Moreover, this also has a normative, sometimes even political aspect: acknowledging and embracing our hermeneutic responsibility is important if we want to avoid that our stories are written elsewhere—through technology.', 'Placebo From an Enactive Perspective. Due to their complexity and variability, placebo effects remain controversial. We suggest this is also due to a set of problematic assumptions (dualism, reductionism, individualism, passivity). We critically assess current explanations and empirical evidence and propose an alternative theoretical framework—the enactive approach to life and mind—based on recent developments in embodied cognitive science. We review core enactive concepts such as autonomy, agency, and sense-making. Following these ideas, we propose a move from binary distinctions (e.g., conscious vs. non-conscious) to the more workable categories of reflective and pre-reflective activity. We introduce an ontology of individuation, following the work of Gilbert Simondon, that allow us to see placebo interventions not as originating causal chains, but as modulators and triggers in the regulation of tensions between ongoing embodied and interpersonal processes. We describe these interrelated processes involving looping effects through three intertwined dimensions of embodiment: organic, sensorimotor, and intersubjective. Finally, we defend the need to investigate therapeutic interactions in terms of participatory sense-making, going beyond the identification of individual social traits (e.g., empathy, trust) that contribute to placebo effects. We discuss resonances and differences between the enactive proposal, popular explanations such as expectations and conditioning, and other approaches based on meaning responses and phenomenological/ecological ideas.', 'Adapting Ourselves, Instead of the Environment: An Inquiry into Human Enhancement for Function and Beyond. Abstract Technology enables humans not only to adapt their environment to their needs but also to modify themselves. Means of Human Enhancement — embodied technologies to improve the human body’s capabilities or to create a new one — are the designated means of adapting ourselves instead of the environment. The debate about these technologies is typically fought on ethical soil. However, alarmist, utopian, and science fiction scenarios distract from the fact that Human Enhancement is a historical and pervasive phenomenon incorporated into many everyday practices. In the vein of disentangling conceptual difficulties, we claim that means of Human Enhancement are either physiologically or psychologically embodied, rendering the merging with the human user their most defining aspect. To fulfill its purpose, an enhancement must pass the test-in-the-world, i.e., assisting with effective engagement with a dynamic world. Even if failing in this regard: Human Enhancement is the fundamental and semi-targeted process of changing the users relationship with the world through the physical or psychological embodiment of a hitherto external object and/or change of one’s body. This can potentially change the notion of being human. Drawing on a rich body of theoretical and empirical literature, we aim to provide a nuanced analysis of the transformative nature of this phenomenon in close proximity to human practice. Stakeholders are invited to apply the theory presented here to interrogate their perspective on technology in general and Human Enhancement in particular.', 'Techne in Affective Posthumanism and AI Artefacts: More (or Less) than Human?. In affective neuroscience, constructivist models are acutely influenced by the modern technological evolution, which underwrites an ongoing epistemological substitution of techne for episteme. Evidenced symptomatically in the influence of artificial intelligence (AI), affective artefacts, these models inform an ontological incursion of techne seen to coincide with posthumanist aspirations and anthropology. It is from the perspective of this neuroscientific techne that posthumanism views the human being as increasingly ill adapted to the modern technological civilization, which, conversely, is understood to require a technical governance of the sort envisioned through AI. Among the projects thought necessary for implementing this framework is a recasting of the human emotional spectrum. Revealed through its techne recasting, however, are explanatory commitments to a metaphysic of extrinsic and contiguous causes, where malleability is ontologically constitutive. Aligned with posthumanist assertions malleability is invoked to argue for a rapid advance of the human form, normatively driven by enlightenment ideals. The ontological claim, however, dispenses with the stability of an a priori, intersubjective and interrelational metaphysical form that undergirds the emotions, leading to the collapse of a definitional anthropos. This paper will argue that techne models of the emotions selectively endorse philosophy of science commitments, thereby introducing a normative inversion that deconstructs the notion of anthropology pursued in posthumanist aspirations.', 'Spike Jonze’s Her: How Transhumanism Turns into a Control Mechanism Under the Name of Love. Transhumanism is a philosophy based on the idea of enhancing the physical, intellectual and psychological capacity of human beings through the direct use of science and technology. Because transhumanism aims to ensure super longevity, super intelligence and super well-being, it is said that it will provide a more liberating atmosphere for human beings by elevating the current human condition. In Spike Jonze’s film her (2013), transhumanist technologies seem to present a peaceful society by focusing on the inhabitants’ well-being. However, in this futuristic society where advanced AI technology and personal letter writing service are widespread, human beings are over-controlled in a subtle way by manufacturing consent as Antonio Gramsci uses to explain the concept of hegemony. So transhumanism does not present a free environment in this futuristic society; instead it disguises the control mechanism through these technologies. The aim of this article is to explore how human beings are controlled by transhumanist technologies subtly under the name of providing super well-being.', '[Essay] The Algorithm; Mind of a Virtual Era – Our Code of Code. As soon as something implicit intrudes consciousness human thought undergoes a radical change. The introduction of any new tool or code brings a shift in cognition; every micro-step layering new semiotic forms within each macroevolutionary-stage has buttressed a new semantic leap. Our mechanization of everyday life and the tech-systems we interact with are impacting communication, cultural norms and values, market-aesthetics, and economics, in societies at large. Undergirded by a survey of the role and significance of tools in human evolution, this study arrives at what is already a well-entrenched new era: the digital, screen-mediated age. Revolutionized by the algorithm, introduced by computers, this age is dominated by the addictive quality of instant contact, unlimited information, virtual gaming, and titillating service-forms, all at our finger tips. Aside from the interpersonal impact on the new humans growing up with devices in hand, how does this disembodied, digital code-form through which our interactions are mediated condition human cognition? How does its seductive efficiency interfere with how we relate, feel, assign meanings, think? Rooted in Code Biology macro-evolutionary and psychoanalytic principles, this paper examines the algorithm itself and takes a sweeping interdisciplinary approach to the developmental, psychosocial, and cognitive implications for the human mind/brain as it interacts with its technological extension.', 'Towards Informatic Personhood: understanding contemporary subjects in a data-driven society. This paper explores the relationships of subjects in the context of data and data technologies, and advances an original theoretical framework called Informatic Personhood to better conceptual subjects and their relationships. Because of the enormous structural change that data has contributed to, subjects are sometimes distant and backgrounded in studies of data, despite data having significant impacts on their lives. Data-mediated relationships mean an increased scale to a relationship, with individuals able to connect to much broader contexts of data, but also have these structures reach down to their subjective context through data. Informatic Personhood seeks to capture the dynamics of data present in everyday life, addressing this distance and better conceptualising the scale of data-mediated relationships. This framework has two parts. The first – The Informatic Context – explores salient structural developments around data and conceptualises this as being defined by the presence of ‘data interfaces’ (that connect individuals to digital contexts), ‘data circulation’ (trends in the movement and storage of data), and ‘data abstraction’ (data manipulation practices). The second part concerns the Informatic Person, and the embodied, affective, and sensemaking relationships of individuals occurring across and through the Informatic Context. This framework better addresses the scale of data-mediated relationships, and places subjects firmly in the foreground of how data is understood.', 'Virtual Environments for Research into Social Evolution (VERSE): A novel experimental environment for the study of human social learning. 1. Abstract Social learning (learning from others) can be a cost-effective way of gaining information compared to asocial (independent) learning. However, learning from others indiscriminately can lead to the acquisition of maladaptive behaviours or outdated information. Evolutionary theory therefore predicts that individuals will use social information adaptively through the use of ‘social learning strategies’. Restrictive laboratory conditions, however, make studying human learning strategies problematic. Abstract tasks, unrealistic sources of social information and methodologies that do not take into account the influence of physical location over large spaces make it difficult to ascertain if previous findings are representative of the way we would use social information in reality. Here I describe a novel platform for studying human social behaviour within immersive virtual environments: “Virtual Environments for Research into Social Evolution” (VERSE). Through the use of gaming technology, VERSE allows researchers to build realistic, three-dimensional, open world environments where participants can complete ecologically relevant tasks while actively observing computer-controlled artificial intelligence agents (AIs) that act as realistic yet controllable sources of social information. This methodological article begins by exploring what social learning strategies are and the problems with studying social learning behaviour in humans (compared to animal populations, for example). I then discuss how gaming technology can be used in behavioural research and follow on with a detailed account of the specific functionalities available in VERSE. I conclude with a worked example of how VERSE can be used to construct a novel behavioural experiment. Altogether, VERSE has great potential to give us insight into how human individuals learn within novel environments in a way that has never before been possible.', 'Perceiving Sociable Technology: Exploring the Role of Anthropomorphism and Agency Perception on Human-Computer Interaction (HCI). With the arrival of personal assistants and other AI-enabled autonomous technologies, social interactions with smart devices have become a part of our daily lives. Therefore, it becomes increasingly important to understand how these social interactions emerge, and why users appear to be influenced by them. For this reason, I explore questions on what the antecedents and consequences of this phenomenon, known as anthropomorphism, are as described in the extant literature from fields ranging from information systems to social neuroscience. I critically analyze those empirical studies directly measuring anthropomorphism and those referring to it without a corresponding measurement. Through a grounded theory approach, I identify common themes and use them to develop models for the antecedents and consequences of anthropomorphism. The results suggest anthropomorphism possesses both conscious and non-conscious components with varying implications. While conscious attributions are shown to vary based on individual differences, non-conscious attributions emerge whenever a technology exhibits apparent reasoning such as through non-verbal behavior like peer-to-peer mirroring or verbal paralinguistic and backchanneling cues. Anthropomorphism has been shown to affect users’ self-perceptions, perceptions of the technology, how users interact with the technology, and the users’ performance. Examples include changes in a users’ trust on the technology, conformity effects, bonding, and displays of empathy. I argue these effects emerge from changes in users’ perceived agency, and their self- and social- identity similarly to interactions between humans. Afterwards, I critically examine current theories on anthropomorphism and present propositions about its nature based on the results of the empirical literature. Subsequently, I introduce a two-factor model of anthropomorphism that proposes how an individual anthropomorphizes a technology is dependent on how the technology was initially perceived (top-down and rational or bottom-up and automatic), and whether it exhibits a capacity for agency or experience. I propose that where a technology lays along this spectrum determines how individuals relates to it, creating shared agency effects, or changing the users’ social identity. For this reason, anthropomorphism is a powerful tool that can be leveraged to support future interactions with smart technologies.', 'AI: A Semiotic Perspective. Abstract Artificial Intelligence (AI) has become a powerful new form of inquiry unto human cognition that has obvious implications for semiotic theories, practices, and modeling of mind, yet, as far as can be determined, it has hardly attracted the attention of semioticians in any meaningful analytical way. AI aims to model and thus penetrate mentality in all its forms (perception, cognition, emotion, etc.) and even to build artificial minds that will surpass human intelligence in the near future. This paper takes a look at AI through the lens of semiotic analysis, in the context of current philosophies such as posthumanism and transhumanism, which are based on the assumption that technology will improve the human condition and chart a path to the future progress of the human species. Semiotics must respond to the AI challenge, focusing on how abductive responses to the world generate meaning in the human sense, not in software or algorithms. The AI approach is instructive, but semiotics is much more relevant to the understanding of human cognition, because it studies signs as paths into the brain, not artificial models of that organ. The semiotic agenda can enrich AI by providing the relevant insight into human semiosis that may defy any attempt to model them.']\n",
      "2023-11-29 15:04:08,198 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-11-29 15:04:08,207 - root - INFO - Cluster 5: ['Placebo From an Enactive Perspective. Due to their complexity and variability, placebo effects remain controversial. We suggest this is also due to a set of problematic assumptions (dualism, reductionism, individualism, passivity). We critically assess current explanations and empirical evidence and propose an alternative theoretical framework—the enactive approach to life and mind—based on recent developments in embodied cognitive science. We review core enactive concepts such as autonomy, agency, and sense-making. Following these ideas, we propose a move from binary distinctions (e.g., conscious vs. non-conscious) to the more workable categories of reflective and pre-reflective activity. We introduce an ontology of individuation, following the work of Gilbert Simondon, that allow us to see placebo interventions not as originating causal chains, but as modulators and triggers in the regulation of tensions between ongoing embodied and interpersonal processes. We describe these interrelated processes involving looping effects through three intertwined dimensions of embodiment: organic, sensorimotor, and intersubjective. Finally, we defend the need to investigate therapeutic interactions in terms of participatory sense-making, going beyond the identification of individual social traits (e.g., empathy, trust) that contribute to placebo effects. We discuss resonances and differences between the enactive proposal, popular explanations such as expectations and conditioning, and other approaches based on meaning responses and phenomenological/ecological ideas.', 'Adapting Ourselves, Instead of the Environment: An Inquiry into Human Enhancement for Function and Beyond. Abstract Technology enables humans not only to adapt their environment to their needs but also to modify themselves. Means of Human Enhancement — embodied technologies to improve the human body’s capabilities or to create a new one — are the designated means of adapting ourselves instead of the environment. The debate about these technologies is typically fought on ethical soil. However, alarmist, utopian, and science fiction scenarios distract from the fact that Human Enhancement is a historical and pervasive phenomenon incorporated into many everyday practices. In the vein of disentangling conceptual difficulties, we claim that means of Human Enhancement are either physiologically or psychologically embodied, rendering the merging with the human user their most defining aspect. To fulfill its purpose, an enhancement must pass the test-in-the-world, i.e., assisting with effective engagement with a dynamic world. Even if failing in this regard: Human Enhancement is the fundamental and semi-targeted process of changing the users relationship with the world through the physical or psychological embodiment of a hitherto external object and/or change of one’s body. This can potentially change the notion of being human. Drawing on a rich body of theoretical and empirical literature, we aim to provide a nuanced analysis of the transformative nature of this phenomenon in close proximity to human practice. Stakeholders are invited to apply the theory presented here to interrogate their perspective on technology in general and Human Enhancement in particular.', 'Spike Jonze’s Her: How Transhumanism Turns into a Control Mechanism Under the Name of Love. Transhumanism is a philosophy based on the idea of enhancing the physical, intellectual and psychological capacity of human beings through the direct use of science and technology. Because transhumanism aims to ensure super longevity, super intelligence and super well-being, it is said that it will provide a more liberating atmosphere for human beings by elevating the current human condition. In Spike Jonze’s film her (2013), transhumanist technologies seem to present a peaceful society by focusing on the inhabitants’ well-being. However, in this futuristic society where advanced AI technology and personal letter writing service are widespread, human beings are over-controlled in a subtle way by manufacturing consent as Antonio Gramsci uses to explain the concept of hegemony. So transhumanism does not present a free environment in this futuristic society; instead it disguises the control mechanism through these technologies. The aim of this article is to explore how human beings are controlled by transhumanist technologies subtly under the name of providing super well-being.', 'Narrative responsibility and artificial intelligence. Abstract Most accounts of responsibility focus on one type of responsibility, moral responsibility, or address one particular aspect of moral responsibility such as agency. This article outlines a broader framework to think about responsibility that includes causal responsibility, relational responsibility, and what I call “narrative responsibility” as a form of “hermeneutic responsibility”, connects these notions of responsibility with different kinds of knowledge, disciplines, and perspectives on human being, and shows how this framework is helpful for mapping and analysing how artificial intelligence (AI) challenges human responsibility and sense-making in various ways. Mobilizing recent hermeneutic approaches to technology, the article argues that next to, and interwoven with, other types of responsibility such as moral responsibility, we also have narrative and hermeneutic responsibility—in general and for technology. For example, it is our task as humans to make sense of, with and, if necessary, against AI. While from a posthumanist point of view, technologies also contribute to sense-making, humans are the experiencers and bearers of responsibility and always remain in charge when it comes to this hermeneutic responsibility. Facing and working with a world of data, correlations, and probabilities, we are nevertheless condemned to make sense. Moreover, this also has a normative, sometimes even political aspect: acknowledging and embracing our hermeneutic responsibility is important if we want to avoid that our stories are written elsewhere—through technology.', 'Techne in Affective Posthumanism and AI Artefacts: More (or Less) than Human?. In affective neuroscience, constructivist models are acutely influenced by the modern technological evolution, which underwrites an ongoing epistemological substitution of techne for episteme. Evidenced symptomatically in the influence of artificial intelligence (AI), affective artefacts, these models inform an ontological incursion of techne seen to coincide with posthumanist aspirations and anthropology. It is from the perspective of this neuroscientific techne that posthumanism views the human being as increasingly ill adapted to the modern technological civilization, which, conversely, is understood to require a technical governance of the sort envisioned through AI. Among the projects thought necessary for implementing this framework is a recasting of the human emotional spectrum. Revealed through its techne recasting, however, are explanatory commitments to a metaphysic of extrinsic and contiguous causes, where malleability is ontologically constitutive. Aligned with posthumanist assertions malleability is invoked to argue for a rapid advance of the human form, normatively driven by enlightenment ideals. The ontological claim, however, dispenses with the stability of an a priori, intersubjective and interrelational metaphysical form that undergirds the emotions, leading to the collapse of a definitional anthropos. This paper will argue that techne models of the emotions selectively endorse philosophy of science commitments, thereby introducing a normative inversion that deconstructs the notion of anthropology pursued in posthumanist aspirations.', \"Human Pacemakers and Experiential Reading. This paper applies an embodied perspective to the study of reading and has a two-fold aim: (i) to discuss how reading is best understood in terms of cultural-cognitive performance that involves living bodies who actively engage with reading materials, and (ii) to spark a dialogue with neighboring disciplines, such as multimodality studies and movement studies, which likewise pivot on how practices and performances involve moving bodies: life is something we do . An embodied cognitive perspective considers how performance is constrained by and draws on expertise such as lived experience as well as the material affordances available in the situation. Such a perspective is crucial for reading research as this domain has been, and largely still is, dominated by the view that reading is a silent, disembodied activity that takes place in the reader's brain by means of neural mechanisms. However, recent studies of reading practices are starting to develop new explanations emphasizing the multimodal engagement in reading as crucial for managing the activity. While this perspective is still empirically underexplored, we seek to highlight how reading is managed by readers' dynamic, embodied engagement with the material. We call this engagement cognitive pacemaking , an action-perception phenomenon we argue should be considered as the key mechanism for controlling attention. We present here a framework to understand reading in terms of pacemaking by emphasizing attentional shifts constituted by embodied modulations of lived temporality. Methodologically, we combine a close reading of a classic literary text, with the focus on attentional modulation with a qualitative study of university students reading different short texts. We highlight how meaning emerges not primarily from linguistic decoding and comprehension, but also from cognitive-cultural, multimodal engagement with the text. Finally, we conclude that empirical reading research should focus on how embodied reading differs across contexts, genres, media and personalities to better scaffold and design reading settings in accordance with those aspects.\", 'Questions concerning attention and Stiegler’s therapeutics. The article sets out to develop the concept of attention as a key aspect to building the possible therapeutics that Bernard Stiegler’s recent works have pointed to (The Automatic Society, 2016, The Neganthropocene, 2018 and Qu’appelle-t-on Panser, 2018). The therapeutic aspect of pharmacology takes place through processes that are neganthropic; therefore, which attempt to counteract the entropic nature of digital technologies where there is flattening out to the measurable and the calculable of Big Data. The most obvious examples of this flattening out can be seen in relation to the use of natural language processing technologies for text interpretation and the use of text analytics alongside student analytics. However, the process of exosomatisation of knowledge takes place in forms of hypomnesic tertiary retentions or digital technologies. The loss of knowledge is inherent to these processes of exteriorisation, this loss of knowledge takes place through a process proletarianisation which Marx had pointed to in the Grundisse (1939). The therapeutic gesture is, therefore, an intrinsically educational one, where the loss of knowledge of the pharmacological nature of digital technologies is counteracted by other forms of knowledge construction that can be enabled by digital technologies. Hence, there is a profound educational gesture necessary to enable the re-harnessing of technology to enable the therapeutics. This paper will argue that the positive re-harnessing, the therapeutics, can take place through the development of new forms of neganthropic gestures which can be afforded by the development of specific forms of digital technologies. These also enable a contributive research process whereby the rationalisation of the production of knowledge within the university can be challenged by collaborative, interpretative processes of knowledge production.', 'A Pharmacological Perspective on Technology-Induced Organised Immaturity: The Care-giving Role of the Arts. Digital technologies induce organised immaturity by generating toxic sociotechnical conditions that lead us to delegate autonomous, individual, and responsible thoughts and actions to external technological systems. Aiming to move beyond a diagnostic critical reading of the toxicity of digitalisation, we bring Bernard Stiegler’s pharmacological analysis of technology into dialogue with the ethics of care to speculatively explore how the socially engaged arts—a type of artistic practice emphasising audience co-production and processual collective responses to social challenges—play a care-giving role that helps counter technology-induced organised immaturity. We outline and illustrate two modes by which the socially engaged arts play this role: 1) disorganising immaturity through artivism, most notably anti-surveillance art, that imparts savoir vivre, that is, shared knowledge and meaning to counter the toxic side of technologies while enabling the imagination of alternative worlds in which humans coexist harmoniously with digital technologies, and 2) organising maturity through arts-based hacking that imparts savoir faire, that is, hands-on knowledge for experimental creation and practical enactment of better technological worlds.', '[Essay] The Algorithm; Mind of a Virtual Era – Our Code of Code. As soon as something implicit intrudes consciousness human thought undergoes a radical change. The introduction of any new tool or code brings a shift in cognition; every micro-step layering new semiotic forms within each macroevolutionary-stage has buttressed a new semantic leap. Our mechanization of everyday life and the tech-systems we interact with are impacting communication, cultural norms and values, market-aesthetics, and economics, in societies at large. Undergirded by a survey of the role and significance of tools in human evolution, this study arrives at what is already a well-entrenched new era: the digital, screen-mediated age. Revolutionized by the algorithm, introduced by computers, this age is dominated by the addictive quality of instant contact, unlimited information, virtual gaming, and titillating service-forms, all at our finger tips. Aside from the interpersonal impact on the new humans growing up with devices in hand, how does this disembodied, digital code-form through which our interactions are mediated condition human cognition? How does its seductive efficiency interfere with how we relate, feel, assign meanings, think? Rooted in Code Biology macro-evolutionary and psychoanalytic principles, this paper examines the algorithm itself and takes a sweeping interdisciplinary approach to the developmental, psychosocial, and cognitive implications for the human mind/brain as it interacts with its technological extension.', 'The development of categorisation and conceptual thinking in early childhood: methods and limitations. Abstract We present a systematic and qualitative review of academic literature on early conceptual development (0–24 months of age), with an emphasis on methodological aspects. The final sample of our review included 281 studies reported in 115 articles. The main aims of the article were four: first, to organise studies into sets according to methodological similarities and differences; second, to elaborate on the methodological procedures that characterise each set; third, to circumscribe the empirical indicators that different sets of studies consider as proof of the existence of concepts in early childhood; last, to identify methodological limitations and to propose possible ways to overcome them. We grouped the studies into five sets: preference and habituation experiments , category extension tasks , object sorting tasks , sequential touching tasks and object examination tasks . In the “Results” section, we review the core features of each set of studies. In the “Discussion” and “Conclusions” sections, we describe, for one thing, the most relevant methodological shortcomings. We end by arguing that a situated, semiotic and pragmatic perspective that emphasises the importance of ecological validity could open up new avenues of research to better understand the development of concepts in early childhood.']\n",
      "2023-11-29 15:04:11,431 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-11-29 15:04:11,440 - root - INFO - Cluster 6: ['Adapting Ourselves, Instead of the Environment: An Inquiry into Human Enhancement for Function and Beyond. Abstract Technology enables humans not only to adapt their environment to their needs but also to modify themselves. Means of Human Enhancement — embodied technologies to improve the human body’s capabilities or to create a new one — are the designated means of adapting ourselves instead of the environment. The debate about these technologies is typically fought on ethical soil. However, alarmist, utopian, and science fiction scenarios distract from the fact that Human Enhancement is a historical and pervasive phenomenon incorporated into many everyday practices. In the vein of disentangling conceptual difficulties, we claim that means of Human Enhancement are either physiologically or psychologically embodied, rendering the merging with the human user their most defining aspect. To fulfill its purpose, an enhancement must pass the test-in-the-world, i.e., assisting with effective engagement with a dynamic world. Even if failing in this regard: Human Enhancement is the fundamental and semi-targeted process of changing the users relationship with the world through the physical or psychological embodiment of a hitherto external object and/or change of one’s body. This can potentially change the notion of being human. Drawing on a rich body of theoretical and empirical literature, we aim to provide a nuanced analysis of the transformative nature of this phenomenon in close proximity to human practice. Stakeholders are invited to apply the theory presented here to interrogate their perspective on technology in general and Human Enhancement in particular.', 'Placebo From an Enactive Perspective. Due to their complexity and variability, placebo effects remain controversial. We suggest this is also due to a set of problematic assumptions (dualism, reductionism, individualism, passivity). We critically assess current explanations and empirical evidence and propose an alternative theoretical framework—the enactive approach to life and mind—based on recent developments in embodied cognitive science. We review core enactive concepts such as autonomy, agency, and sense-making. Following these ideas, we propose a move from binary distinctions (e.g., conscious vs. non-conscious) to the more workable categories of reflective and pre-reflective activity. We introduce an ontology of individuation, following the work of Gilbert Simondon, that allow us to see placebo interventions not as originating causal chains, but as modulators and triggers in the regulation of tensions between ongoing embodied and interpersonal processes. We describe these interrelated processes involving looping effects through three intertwined dimensions of embodiment: organic, sensorimotor, and intersubjective. Finally, we defend the need to investigate therapeutic interactions in terms of participatory sense-making, going beyond the identification of individual social traits (e.g., empathy, trust) that contribute to placebo effects. We discuss resonances and differences between the enactive proposal, popular explanations such as expectations and conditioning, and other approaches based on meaning responses and phenomenological/ecological ideas.', 'A world of difference: The fundamental opposition between transhumanist “welfarism” and disability advocacy. From the standpoint of disability advocacy, further exploration of the concept of well-being stands to be availing. The notion that \"welfarism\" about disability, which Julian Savulescu and Guy Kahane debuted, qualifies as helpful is encouraged by their claim that welfarism shares important commitments with that advocacy. As becomes clear when they apply their welfarist frame to procreative decisions, endorsing welfarism would, in fact, sharply undermine it. Savulescu and Kahane\\'s Principle of Procreative Beneficence-which reflects transhumanism, or advocacy of radical bioenhancement-morally requires parents to choose the child who will, in all probability, have \"the best life.\" Assuming the emergence of potent biotechnologies, procreative decision-making would be highly standardized, for prospective parents would be morally obliged to maximize select capacities, including intelligence, self-control, and hedonic set-point, in their children. Welfarism, applied to reproduction, is staunchly objectivist about what course is incumbent on decision-makers, giving no credence to first-personal values, aspirations, and experiences. Though this dismissal of individual perspectives applies to everyone, its implications for disability advocacy are especially severe. With that advocacy in view, greater attention to \"well-being\" should, therefore, be severed from the welfarism of Savulescu and Kahane.', 'Spike Jonze’s Her: How Transhumanism Turns into a Control Mechanism Under the Name of Love. Transhumanism is a philosophy based on the idea of enhancing the physical, intellectual and psychological capacity of human beings through the direct use of science and technology. Because transhumanism aims to ensure super longevity, super intelligence and super well-being, it is said that it will provide a more liberating atmosphere for human beings by elevating the current human condition. In Spike Jonze’s film her (2013), transhumanist technologies seem to present a peaceful society by focusing on the inhabitants’ well-being. However, in this futuristic society where advanced AI technology and personal letter writing service are widespread, human beings are over-controlled in a subtle way by manufacturing consent as Antonio Gramsci uses to explain the concept of hegemony. So transhumanism does not present a free environment in this futuristic society; instead it disguises the control mechanism through these technologies. The aim of this article is to explore how human beings are controlled by transhumanist technologies subtly under the name of providing super well-being.', 'The development of categorisation and conceptual thinking in early childhood: methods and limitations. Abstract We present a systematic and qualitative review of academic literature on early conceptual development (0–24 months of age), with an emphasis on methodological aspects. The final sample of our review included 281 studies reported in 115 articles. The main aims of the article were four: first, to organise studies into sets according to methodological similarities and differences; second, to elaborate on the methodological procedures that characterise each set; third, to circumscribe the empirical indicators that different sets of studies consider as proof of the existence of concepts in early childhood; last, to identify methodological limitations and to propose possible ways to overcome them. We grouped the studies into five sets: preference and habituation experiments , category extension tasks , object sorting tasks , sequential touching tasks and object examination tasks . In the “Results” section, we review the core features of each set of studies. In the “Discussion” and “Conclusions” sections, we describe, for one thing, the most relevant methodological shortcomings. We end by arguing that a situated, semiotic and pragmatic perspective that emphasises the importance of ecological validity could open up new avenues of research to better understand the development of concepts in early childhood.', 'Techne in Affective Posthumanism and AI Artefacts: More (or Less) than Human?. In affective neuroscience, constructivist models are acutely influenced by the modern technological evolution, which underwrites an ongoing epistemological substitution of techne for episteme. Evidenced symptomatically in the influence of artificial intelligence (AI), affective artefacts, these models inform an ontological incursion of techne seen to coincide with posthumanist aspirations and anthropology. It is from the perspective of this neuroscientific techne that posthumanism views the human being as increasingly ill adapted to the modern technological civilization, which, conversely, is understood to require a technical governance of the sort envisioned through AI. Among the projects thought necessary for implementing this framework is a recasting of the human emotional spectrum. Revealed through its techne recasting, however, are explanatory commitments to a metaphysic of extrinsic and contiguous causes, where malleability is ontologically constitutive. Aligned with posthumanist assertions malleability is invoked to argue for a rapid advance of the human form, normatively driven by enlightenment ideals. The ontological claim, however, dispenses with the stability of an a priori, intersubjective and interrelational metaphysical form that undergirds the emotions, leading to the collapse of a definitional anthropos. This paper will argue that techne models of the emotions selectively endorse philosophy of science commitments, thereby introducing a normative inversion that deconstructs the notion of anthropology pursued in posthumanist aspirations.', 'Narrative responsibility and artificial intelligence. Abstract Most accounts of responsibility focus on one type of responsibility, moral responsibility, or address one particular aspect of moral responsibility such as agency. This article outlines a broader framework to think about responsibility that includes causal responsibility, relational responsibility, and what I call “narrative responsibility” as a form of “hermeneutic responsibility”, connects these notions of responsibility with different kinds of knowledge, disciplines, and perspectives on human being, and shows how this framework is helpful for mapping and analysing how artificial intelligence (AI) challenges human responsibility and sense-making in various ways. Mobilizing recent hermeneutic approaches to technology, the article argues that next to, and interwoven with, other types of responsibility such as moral responsibility, we also have narrative and hermeneutic responsibility—in general and for technology. For example, it is our task as humans to make sense of, with and, if necessary, against AI. While from a posthumanist point of view, technologies also contribute to sense-making, humans are the experiencers and bearers of responsibility and always remain in charge when it comes to this hermeneutic responsibility. Facing and working with a world of data, correlations, and probabilities, we are nevertheless condemned to make sense. Moreover, this also has a normative, sometimes even political aspect: acknowledging and embracing our hermeneutic responsibility is important if we want to avoid that our stories are written elsewhere—through technology.', 'A review of ideas and strategies to improve scientific research and its dissemination in life and social sciences. In recent years, issues concerning reproducibility, questionable research practices (QRPs), and analytical flexibility have been contentious topics in the fields of life and social sciences. As such, research has increasingly focused on studying the way we conduct science (which is a field of study in meta-science), and new ideas have been introduced on how we can improve the way we conduct science. In this literature review, we aim to summarize research and ideas that have been introduced in the past decade. We give an overview of studies concerning reproducibility in contemporary life and social sciences, summarize reasons for failed and successful reproductions, and outline QRPs which are often used when results do not support the researcher’s hypothesis and/or when results are not statistically significant. We also discuss cognitive biases in research and the topic of analytical flexibility, which warrants straightforward interpretation and generalization of results. We summarize the literature on improving the scientific process and system of life and social sciences in six broad categories, including (1) Focus on aggregate effects across replication attempts to achieve scientific knowledge, (2) Improved preparation and achieving more solid empirical claims through (pre)registered reports when performing hypothesis-driven research, (3) Expanding our statistical toolbox by going beyond null-hypothesis significance testing, and justifying analytical choices a priori, (4) Transparency, complete reporting, and openness (if warranted) of data, code, all performed analyses, results, and (changes in) research plans, (5) Improving the way we decide what is considered scientific by allowing for continuous, graded, open, and (optionally anonymous) peer review, and by giving reviewers checklists, and (6) Changing incentives in science from a focus on quantity to quality and rigor by focusing on research metrics that more closely reflect scientific quality (e.g., by using graded peer evaluations).', 'Contributions of Science Fiction to Thinking up (Im)possible Future Societies: Medical Students’ Genetic Imaginary. Science fiction has been an inexhaustible source for the creation of technoscientific imaginary that has marked certain historical periods and influenced the production of subjectivity. This imaginary evokes complex ontological, epistemological, political, social, environmental and existential questions on the present and the future. The aim of this study was to identify and characterize the cultural productions accessed by the public to form an opinion about the genetic manipulation of human beings. A survey about sources of information that influence opinions on the genetic manipulation of human beings was applied to 360 medical students (70.8% female). Movies were the most commonly mentioned source of information, followed by books, documentaries, news programs, television series, informational videos, soap operas and videogames. Science fiction was the most frequent genre and dystopian views of the future of humanity predominated.', 'Falsified Incompetence and Other Lies the Positivists Told Me. &#x0D; &#x0D; &#x0D; Facilitated Communication (FC) is a technique of supported communication for non- speaking people with motor movements commonly understood as spasmodic, dyspraxic, or otherwise unruly. FC is a contentious site of scientific conflict where highly circumscribed quantitative experiments have been unable to reckon with the lived reality of typers. The debate over the efficacy of FC centers around broader arguments of what counts as scientific rigor and validity. In this paper, I remind readers that experiential data is, in fact, empirical. Qualitative analysis is scientifically rigorous. Adopting technologies of analysis from Chela Sandoval’s “Methodology of the Oppressed,” I explore a rhetorics of evacuation deployed by skeptics that result in the erasure of FC user agency, testimony, and experience. I invite readers to explore how these rhetorics extend beyond FC and into the wider field of education research.&#x0D; &#x0D; &#x0D;']\n",
      "2023-11-29 15:04:15,468 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-11-29 15:04:15,476 - root - INFO - Cluster 7: ['Adapting Ourselves, Instead of the Environment: An Inquiry into Human Enhancement for Function and Beyond. Abstract Technology enables humans not only to adapt their environment to their needs but also to modify themselves. Means of Human Enhancement — embodied technologies to improve the human body’s capabilities or to create a new one — are the designated means of adapting ourselves instead of the environment. The debate about these technologies is typically fought on ethical soil. However, alarmist, utopian, and science fiction scenarios distract from the fact that Human Enhancement is a historical and pervasive phenomenon incorporated into many everyday practices. In the vein of disentangling conceptual difficulties, we claim that means of Human Enhancement are either physiologically or psychologically embodied, rendering the merging with the human user their most defining aspect. To fulfill its purpose, an enhancement must pass the test-in-the-world, i.e., assisting with effective engagement with a dynamic world. Even if failing in this regard: Human Enhancement is the fundamental and semi-targeted process of changing the users relationship with the world through the physical or psychological embodiment of a hitherto external object and/or change of one’s body. This can potentially change the notion of being human. Drawing on a rich body of theoretical and empirical literature, we aim to provide a nuanced analysis of the transformative nature of this phenomenon in close proximity to human practice. Stakeholders are invited to apply the theory presented here to interrogate their perspective on technology in general and Human Enhancement in particular.', 'Narrative responsibility and artificial intelligence. Abstract Most accounts of responsibility focus on one type of responsibility, moral responsibility, or address one particular aspect of moral responsibility such as agency. This article outlines a broader framework to think about responsibility that includes causal responsibility, relational responsibility, and what I call “narrative responsibility” as a form of “hermeneutic responsibility”, connects these notions of responsibility with different kinds of knowledge, disciplines, and perspectives on human being, and shows how this framework is helpful for mapping and analysing how artificial intelligence (AI) challenges human responsibility and sense-making in various ways. Mobilizing recent hermeneutic approaches to technology, the article argues that next to, and interwoven with, other types of responsibility such as moral responsibility, we also have narrative and hermeneutic responsibility—in general and for technology. For example, it is our task as humans to make sense of, with and, if necessary, against AI. While from a posthumanist point of view, technologies also contribute to sense-making, humans are the experiencers and bearers of responsibility and always remain in charge when it comes to this hermeneutic responsibility. Facing and working with a world of data, correlations, and probabilities, we are nevertheless condemned to make sense. Moreover, this also has a normative, sometimes even political aspect: acknowledging and embracing our hermeneutic responsibility is important if we want to avoid that our stories are written elsewhere—through technology.', 'Placebo From an Enactive Perspective. Due to their complexity and variability, placebo effects remain controversial. We suggest this is also due to a set of problematic assumptions (dualism, reductionism, individualism, passivity). We critically assess current explanations and empirical evidence and propose an alternative theoretical framework—the enactive approach to life and mind—based on recent developments in embodied cognitive science. We review core enactive concepts such as autonomy, agency, and sense-making. Following these ideas, we propose a move from binary distinctions (e.g., conscious vs. non-conscious) to the more workable categories of reflective and pre-reflective activity. We introduce an ontology of individuation, following the work of Gilbert Simondon, that allow us to see placebo interventions not as originating causal chains, but as modulators and triggers in the regulation of tensions between ongoing embodied and interpersonal processes. We describe these interrelated processes involving looping effects through three intertwined dimensions of embodiment: organic, sensorimotor, and intersubjective. Finally, we defend the need to investigate therapeutic interactions in terms of participatory sense-making, going beyond the identification of individual social traits (e.g., empathy, trust) that contribute to placebo effects. We discuss resonances and differences between the enactive proposal, popular explanations such as expectations and conditioning, and other approaches based on meaning responses and phenomenological/ecological ideas.', 'Techne in Affective Posthumanism and AI Artefacts: More (or Less) than Human?. In affective neuroscience, constructivist models are acutely influenced by the modern technological evolution, which underwrites an ongoing epistemological substitution of techne for episteme. Evidenced symptomatically in the influence of artificial intelligence (AI), affective artefacts, these models inform an ontological incursion of techne seen to coincide with posthumanist aspirations and anthropology. It is from the perspective of this neuroscientific techne that posthumanism views the human being as increasingly ill adapted to the modern technological civilization, which, conversely, is understood to require a technical governance of the sort envisioned through AI. Among the projects thought necessary for implementing this framework is a recasting of the human emotional spectrum. Revealed through its techne recasting, however, are explanatory commitments to a metaphysic of extrinsic and contiguous causes, where malleability is ontologically constitutive. Aligned with posthumanist assertions malleability is invoked to argue for a rapid advance of the human form, normatively driven by enlightenment ideals. The ontological claim, however, dispenses with the stability of an a priori, intersubjective and interrelational metaphysical form that undergirds the emotions, leading to the collapse of a definitional anthropos. This paper will argue that techne models of the emotions selectively endorse philosophy of science commitments, thereby introducing a normative inversion that deconstructs the notion of anthropology pursued in posthumanist aspirations.', '[Essay] The Algorithm; Mind of a Virtual Era – Our Code of Code. As soon as something implicit intrudes consciousness human thought undergoes a radical change. The introduction of any new tool or code brings a shift in cognition; every micro-step layering new semiotic forms within each macroevolutionary-stage has buttressed a new semantic leap. Our mechanization of everyday life and the tech-systems we interact with are impacting communication, cultural norms and values, market-aesthetics, and economics, in societies at large. Undergirded by a survey of the role and significance of tools in human evolution, this study arrives at what is already a well-entrenched new era: the digital, screen-mediated age. Revolutionized by the algorithm, introduced by computers, this age is dominated by the addictive quality of instant contact, unlimited information, virtual gaming, and titillating service-forms, all at our finger tips. Aside from the interpersonal impact on the new humans growing up with devices in hand, how does this disembodied, digital code-form through which our interactions are mediated condition human cognition? How does its seductive efficiency interfere with how we relate, feel, assign meanings, think? Rooted in Code Biology macro-evolutionary and psychoanalytic principles, this paper examines the algorithm itself and takes a sweeping interdisciplinary approach to the developmental, psychosocial, and cognitive implications for the human mind/brain as it interacts with its technological extension.', 'Spike Jonze’s Her: How Transhumanism Turns into a Control Mechanism Under the Name of Love. Transhumanism is a philosophy based on the idea of enhancing the physical, intellectual and psychological capacity of human beings through the direct use of science and technology. Because transhumanism aims to ensure super longevity, super intelligence and super well-being, it is said that it will provide a more liberating atmosphere for human beings by elevating the current human condition. In Spike Jonze’s film her (2013), transhumanist technologies seem to present a peaceful society by focusing on the inhabitants’ well-being. However, in this futuristic society where advanced AI technology and personal letter writing service are widespread, human beings are over-controlled in a subtle way by manufacturing consent as Antonio Gramsci uses to explain the concept of hegemony. So transhumanism does not present a free environment in this futuristic society; instead it disguises the control mechanism through these technologies. The aim of this article is to explore how human beings are controlled by transhumanist technologies subtly under the name of providing super well-being.', 'A Pharmacological Perspective on Technology-Induced Organised Immaturity: The Care-giving Role of the Arts. Digital technologies induce organised immaturity by generating toxic sociotechnical conditions that lead us to delegate autonomous, individual, and responsible thoughts and actions to external technological systems. Aiming to move beyond a diagnostic critical reading of the toxicity of digitalisation, we bring Bernard Stiegler’s pharmacological analysis of technology into dialogue with the ethics of care to speculatively explore how the socially engaged arts—a type of artistic practice emphasising audience co-production and processual collective responses to social challenges—play a care-giving role that helps counter technology-induced organised immaturity. We outline and illustrate two modes by which the socially engaged arts play this role: 1) disorganising immaturity through artivism, most notably anti-surveillance art, that imparts savoir vivre, that is, shared knowledge and meaning to counter the toxic side of technologies while enabling the imagination of alternative worlds in which humans coexist harmoniously with digital technologies, and 2) organising maturity through arts-based hacking that imparts savoir faire, that is, hands-on knowledge for experimental creation and practical enactment of better technological worlds.', 'Organizational Environment and Innovation. There is a popular notion that the life of innovators and inventors is a glorious and enviable one because they have talents which others do not have, and because they can overcome any obstacles. These talents are, in the popular interpretation, something mysterious, indefinable and inimitable. But in reality, the innovators and inventors have to deal with many serious and pernicious obstacles which are unknown to other people. Those who are unaware of the obstacles must become aware of them and must reduce them, because the reduction of these obstacles will greatly facilitate the works of innovators and inventors. Another problem is the assumption that all people use the same logic, and therefore if you explain hard enough, you will be understood. Existence of heterogeneity of individual logical types (perceptual/cognitive/cogitative action types, abbreviated as mindscape types) is ignored, or at least exoticated as irrelevant. This article discusses these problems, with concrete experiential examples. This article also uses the methodology of raw-experience visualization-enabling communication (REVEC) with which the readers can formulate their own grounded theories independent from the theory of the writer.', \"Human Pacemakers and Experiential Reading. This paper applies an embodied perspective to the study of reading and has a two-fold aim: (i) to discuss how reading is best understood in terms of cultural-cognitive performance that involves living bodies who actively engage with reading materials, and (ii) to spark a dialogue with neighboring disciplines, such as multimodality studies and movement studies, which likewise pivot on how practices and performances involve moving bodies: life is something we do . An embodied cognitive perspective considers how performance is constrained by and draws on expertise such as lived experience as well as the material affordances available in the situation. Such a perspective is crucial for reading research as this domain has been, and largely still is, dominated by the view that reading is a silent, disembodied activity that takes place in the reader's brain by means of neural mechanisms. However, recent studies of reading practices are starting to develop new explanations emphasizing the multimodal engagement in reading as crucial for managing the activity. While this perspective is still empirically underexplored, we seek to highlight how reading is managed by readers' dynamic, embodied engagement with the material. We call this engagement cognitive pacemaking , an action-perception phenomenon we argue should be considered as the key mechanism for controlling attention. We present here a framework to understand reading in terms of pacemaking by emphasizing attentional shifts constituted by embodied modulations of lived temporality. Methodologically, we combine a close reading of a classic literary text, with the focus on attentional modulation with a qualitative study of university students reading different short texts. We highlight how meaning emerges not primarily from linguistic decoding and comprehension, but also from cognitive-cultural, multimodal engagement with the text. Finally, we conclude that empirical reading research should focus on how embodied reading differs across contexts, genres, media and personalities to better scaffold and design reading settings in accordance with those aspects.\", 'Living Machines: Metaphors We Live By. Abstract Within biology and in society, living creatures have long been described using metaphors of machinery and computation: ‘bioengineering’, ‘genes as code’ or ‘biological chassis’. This paper builds on Lakoff and Johnson’s (1980) argument that such language mechanisms shape how we understand the world. I argue that the living machines metaphor builds upon a certain perception of life entailing an idea of radical human control of the living world, looking back at the historical preconditions for this metaphor. I discuss how design is perceived to enable us to shape natural beings to our will, and consider ethical, epistemological and ontological implications of the prevalence of this metaphor, focusing on its use within synthetic biology. I argue that we urgently need counter-images to the dominant metaphor of living machines and its implied control and propose that artworks can provide such counter-images through upsetting the perception of life as controllable. This is argued through discussion of artworks by Oron Catts and Ionat Zurr, by Tarsh Bates and by Ai Hasegawa, which in different ways challenge mechanistic assumptions through open-ended engagement with the strangeness and messiness of life.']\n",
      "2023-11-29 15:04:19,263 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-11-29 15:04:19,271 - root - INFO - Cluster 8: ['Narrative responsibility and artificial intelligence. Abstract Most accounts of responsibility focus on one type of responsibility, moral responsibility, or address one particular aspect of moral responsibility such as agency. This article outlines a broader framework to think about responsibility that includes causal responsibility, relational responsibility, and what I call “narrative responsibility” as a form of “hermeneutic responsibility”, connects these notions of responsibility with different kinds of knowledge, disciplines, and perspectives on human being, and shows how this framework is helpful for mapping and analysing how artificial intelligence (AI) challenges human responsibility and sense-making in various ways. Mobilizing recent hermeneutic approaches to technology, the article argues that next to, and interwoven with, other types of responsibility such as moral responsibility, we also have narrative and hermeneutic responsibility—in general and for technology. For example, it is our task as humans to make sense of, with and, if necessary, against AI. While from a posthumanist point of view, technologies also contribute to sense-making, humans are the experiencers and bearers of responsibility and always remain in charge when it comes to this hermeneutic responsibility. Facing and working with a world of data, correlations, and probabilities, we are nevertheless condemned to make sense. Moreover, this also has a normative, sometimes even political aspect: acknowledging and embracing our hermeneutic responsibility is important if we want to avoid that our stories are written elsewhere—through technology.', 'Placebo From an Enactive Perspective. Due to their complexity and variability, placebo effects remain controversial. We suggest this is also due to a set of problematic assumptions (dualism, reductionism, individualism, passivity). We critically assess current explanations and empirical evidence and propose an alternative theoretical framework—the enactive approach to life and mind—based on recent developments in embodied cognitive science. We review core enactive concepts such as autonomy, agency, and sense-making. Following these ideas, we propose a move from binary distinctions (e.g., conscious vs. non-conscious) to the more workable categories of reflective and pre-reflective activity. We introduce an ontology of individuation, following the work of Gilbert Simondon, that allow us to see placebo interventions not as originating causal chains, but as modulators and triggers in the regulation of tensions between ongoing embodied and interpersonal processes. We describe these interrelated processes involving looping effects through three intertwined dimensions of embodiment: organic, sensorimotor, and intersubjective. Finally, we defend the need to investigate therapeutic interactions in terms of participatory sense-making, going beyond the identification of individual social traits (e.g., empathy, trust) that contribute to placebo effects. We discuss resonances and differences between the enactive proposal, popular explanations such as expectations and conditioning, and other approaches based on meaning responses and phenomenological/ecological ideas.', 'Adapting Ourselves, Instead of the Environment: An Inquiry into Human Enhancement for Function and Beyond. Abstract Technology enables humans not only to adapt their environment to their needs but also to modify themselves. Means of Human Enhancement — embodied technologies to improve the human body’s capabilities or to create a new one — are the designated means of adapting ourselves instead of the environment. The debate about these technologies is typically fought on ethical soil. However, alarmist, utopian, and science fiction scenarios distract from the fact that Human Enhancement is a historical and pervasive phenomenon incorporated into many everyday practices. In the vein of disentangling conceptual difficulties, we claim that means of Human Enhancement are either physiologically or psychologically embodied, rendering the merging with the human user their most defining aspect. To fulfill its purpose, an enhancement must pass the test-in-the-world, i.e., assisting with effective engagement with a dynamic world. Even if failing in this regard: Human Enhancement is the fundamental and semi-targeted process of changing the users relationship with the world through the physical or psychological embodiment of a hitherto external object and/or change of one’s body. This can potentially change the notion of being human. Drawing on a rich body of theoretical and empirical literature, we aim to provide a nuanced analysis of the transformative nature of this phenomenon in close proximity to human practice. Stakeholders are invited to apply the theory presented here to interrogate their perspective on technology in general and Human Enhancement in particular.', 'Analyzing Multimodal Multichannel Data about Self-Regulated Learning with Advanced Learning Technologies: Issues and Challenges. Analyzing multimodal multichannel data about self-regulated learning (SRL) obtained during the use of advanced learning technologies such as intelligent tutoring systems, serious games, hypermedia, and immersive virtual learning environments is key to understanding the interplay among cognitive, affective, metacognitive, and social processes and their impact on learning, problem solving, reasoning, and conceptual understanding in learners of all ages and contexts. In this special issue of Computers in Human Behavior, we report six studies conducted by interdisciplinary teams’ use of various trace methodologies such as eye tracking, log-files, physiological data, facial expressions of emotions, screen recordings, concurrent think-alouds, and linguistic analyses of discourse. The research studies focus on how these data were analyzed using a combination of traditional statistical techniques as well as educational data-mining procedures to detect, measure, and infer cognitive, metacognitive, and social processes related to regulating the self and others across several tasks, domains, ages, and contexts. The results of these studies point to future work necessitating interdisciplinary researchers’ collaboration to use theoretically based and empirically derived approaches to collecting, measuring, and modeling multimodal multichannel SRL data to extend our current models, frameworks, and theories by making them more predictive by elucidating the nature, complexity, and temporality of underlying processes. Lastly, analyses of multimodal multichannel SRL process data can significantly augment advanced learning technologies by providing real-time, intelligent, adaptive, individualized scaffolding and feedback to address learners’ self-regulatory needs.', 'What makes AI ‘intelligent’ and ‘caring’? Exploring affect and relationality across three sites of intelligence and care. This paper scrutinises how AI and robotic technologies are transforming the relationships between people and machines in new affective, embodied and relational ways. Through investigating what it means to exist as human \\'in relation\\' to AI across health and care contexts, we aim to make three main contributions. (1) We start by highlighting the complexities of philosophical issues surrounding the concepts of \"artificial intelligence\" and \"ethical machines.\" (2) We outline some potential challenges and opportunities that the creation of such technologies may bring in the health and care settings. We focus on AI applications that interface with health and care via examples where AI is explicitly designed as an \\'augmenting\\' technology that can overcome human bodily and cognitive as well as socio-economic constraints. We focus on three dimensions of \\'intelligence\\' - physical, interpretive, and emotional - using the examples of robotic surgery, digital pathology, and robot caregivers, respectively. Through investigating these areas, we interrogate the social context and implications of human-technology interaction in the interrelational sphere of care practice. (3) We argue, in conclusion, that there is a need for an interdisciplinary mode of theorising \\'intelligence\\' as relational and affective in ways that can accommodate the fragmentation of both conceptual and material boundaries between human and AI, and human and machine. Our aim in investigating these sociological, philosophical and ethical questions is primarily to explore the relationship between affect, relationality and \\'intelligence,\\' the intersection and integration of \\'human\\' and \\'artificial\\' intelligence, through an examination of how AI is used across different dimensions of intelligence. This allows us to scrutinise how \\'intelligence\\' is ultimately conveyed, understood and (technologically or algorithmically) configured in practice through emerging relationships that go beyond the conceptual divisions between humans and machines, and humans vis-à-vis artificial intelligence-based technologies.', '\"What is Your Envisioned Future?\": Toward Human-AI Enrichment in Data Work of Asthma Care. Patient-generated health data (PGHD) is crucial for healthcare providers\\' decision making, as it complements clinical data by providing a more holistic view of patients\\' daily conditions. We interviewed 20 healthcare providers in asthma care to envision future technologies to support their PGHD use. We found that healthcare providers want future artificial intelligence (AI) systems to enhance their ability to treat patients by analyzing PGHD for profiling risk and predicting deterioration. Despite the potential benefits of AI, providers perceived various challenges of AI use with PGHD, including AI-driven data inequity, added burden, lack of trust toward AI, and fear of being replaced by AI. Clinicians wished for a future of co-dependent human-AI collaboration, where AI will help them to improve their clinical practice. In turn, healthcare providers can improve AI systems by making AI outputs more trustworthy and humane. Through the lens of data feminism, we discuss the importance of considering context and aligning the complex human infrastructure before designing or deploying PGHD-based AI systems in clinical settings. We highlight the opportunity to design for human-AI enrichment, where humans and AI not only partner with each other for improved performance, but also enrich each other to enhance each other\\'s work overtime.', 'Towards Informatic Personhood: understanding contemporary subjects in a data-driven society. This paper explores the relationships of subjects in the context of data and data technologies, and advances an original theoretical framework called Informatic Personhood to better conceptual subjects and their relationships. Because of the enormous structural change that data has contributed to, subjects are sometimes distant and backgrounded in studies of data, despite data having significant impacts on their lives. Data-mediated relationships mean an increased scale to a relationship, with individuals able to connect to much broader contexts of data, but also have these structures reach down to their subjective context through data. Informatic Personhood seeks to capture the dynamics of data present in everyday life, addressing this distance and better conceptualising the scale of data-mediated relationships. This framework has two parts. The first – The Informatic Context – explores salient structural developments around data and conceptualises this as being defined by the presence of ‘data interfaces’ (that connect individuals to digital contexts), ‘data circulation’ (trends in the movement and storage of data), and ‘data abstraction’ (data manipulation practices). The second part concerns the Informatic Person, and the embodied, affective, and sensemaking relationships of individuals occurring across and through the Informatic Context. This framework better addresses the scale of data-mediated relationships, and places subjects firmly in the foreground of how data is understood.', 'Spike Jonze’s Her: How Transhumanism Turns into a Control Mechanism Under the Name of Love. Transhumanism is a philosophy based on the idea of enhancing the physical, intellectual and psychological capacity of human beings through the direct use of science and technology. Because transhumanism aims to ensure super longevity, super intelligence and super well-being, it is said that it will provide a more liberating atmosphere for human beings by elevating the current human condition. In Spike Jonze’s film her (2013), transhumanist technologies seem to present a peaceful society by focusing on the inhabitants’ well-being. However, in this futuristic society where advanced AI technology and personal letter writing service are widespread, human beings are over-controlled in a subtle way by manufacturing consent as Antonio Gramsci uses to explain the concept of hegemony. So transhumanism does not present a free environment in this futuristic society; instead it disguises the control mechanism through these technologies. The aim of this article is to explore how human beings are controlled by transhumanist technologies subtly under the name of providing super well-being.', 'On the Contribution of Neuroethics to the Ethics and Regulation of Artificial Intelligence. Abstract Contemporary ethical analysis of Artificial Intelligence (AI) is growing rapidly. One of its most recognizable outcomes is the publication of a number of ethics guidelines that, intended to guide governmental policy, address issues raised by AI design, development, and implementation and generally present a set of recommendations. Here we propose two things: first, regarding content, since some of the applied issues raised by AI are related to fundamental questions about topics like intelligence, consciousness, and the ontological and ethical status of humans, among others, the treatment of these issues would benefit from interfacing with neuroethics that has been addressing those same issues in the context of brain research. Second, the identification and management of some of the practical ethical challenges raised by AI would be enriched by embracing the methodological resources used in neuroethics. In particular, we focus on the methodological distinction between conceptual and action-oriented neuroethical approaches. We argue that the normative (often principles-oriented) discussion about AI will benefit from further integration of conceptual analysis, including analysis of some operative assumptions, their meaning in different contexts, and their mutual relevance in order to avoid misplaced or disproportionate concerns and achieve a more realistic and useful approach to identifying and managing the emerging ethical issues.', \"Theories of Parenting and Their Application to Artificial Intelligence. As machine learning (ML) systems have advanced, they have acquired more power over humans' lives, and questions about what values are embedded in them have become more complex and fraught. It is conceivable that in the coming decades, humans may succeed in creating artificial general intelligence (AGI) that thinks and acts with an open-endedness and autonomy comparable to that of humans. The implications would be profound for our species; they are now widely debated not just in science fiction and speculative research agendas but increasingly in serious technical and policy conversations. Much work is underway to try to weave ethics into advancing ML research. We think it useful to add the lens of parenting to these efforts, and specifically radical, queer theories of parenting that consciously set out to nurture agents whose experiences, objectives and understanding of the world will necessarily be very different from their parents'. We propose a spectrum of principles which might underpin such an effort; some are relevant to current ML research, while others will become more important if AGI becomes more likely. These principles may encourage new thinking about the development, design, training, and release into the world of increasingly autonomous agents.\"]\n",
      "2023-11-29 15:04:20,691 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-11-29 15:04:20,700 - root - INFO - Cluster 9: ['Transhumanism, Vulnerability and Human Dignity. The transhumanist movement is much more than a simple utopia, a new school of thought or a fashionable ideology; as a matter of fact, it is a scientific and philosophical project that is already underway, and defends the use of the most advanced emerging new technologies —from biogenetics to computing, from nanotechnology to cognitive sciences, to robotics and Artificial Intelligence— with the clear goal to exponentially increase the physical, cognitive, sensory, moral and emotional capabilities of human beings. Transhumanism entails a change in the anthropocentric paradigm defended by humanism, and aims to break through the limits of nature, which until recently we deemed insurmountable, in order to create a new species that is more evolved than the Homo sapiens: the Homo excelsior, a posthuman species which is superior to ours, composed by exceptionally gifted beings that have been genetically selected, designed and improved and which —according to the transhumanist imaginary— will dominate the posthuman future and will be happier, more virtuous, long-lived and intelligent than us. In this article, we propose technological humanism as an intermediate formula in the doctrinal debate between bioprogressive and bioconservative legal philosophers, so as to make possible the development of scientific research and the advancement of new technologies, although without ever having to sacrifice dignity and liberty, which are inherent qualities of the human being (who has to be viewed, in Kantian terms, as an end in itself).Received: 22 May 2019Accepted: 10 September 2019Published online: 20 December 2019', 'AI: A Semiotic Perspective. Abstract Artificial Intelligence (AI) has become a powerful new form of inquiry unto human cognition that has obvious implications for semiotic theories, practices, and modeling of mind, yet, as far as can be determined, it has hardly attracted the attention of semioticians in any meaningful analytical way. AI aims to model and thus penetrate mentality in all its forms (perception, cognition, emotion, etc.) and even to build artificial minds that will surpass human intelligence in the near future. This paper takes a look at AI through the lens of semiotic analysis, in the context of current philosophies such as posthumanism and transhumanism, which are based on the assumption that technology will improve the human condition and chart a path to the future progress of the human species. Semiotics must respond to the AI challenge, focusing on how abductive responses to the world generate meaning in the human sense, not in software or algorithms. The AI approach is instructive, but semiotics is much more relevant to the understanding of human cognition, because it studies signs as paths into the brain, not artificial models of that organ. The semiotic agenda can enrich AI by providing the relevant insight into human semiosis that may defy any attempt to model them.', 'Literary AI: Are We Ready for the Future We Imagine?. This review considers multiple works of speculative fiction depicting artificial intelligence (AI) published over the last several years. Rather than review each for their qualities as works of fiction, I look at them collectively to discuss recurring motifs and themes as a way toward theorizing what AI means in our cultural imaginary today. The novels reflect on pressing sociopolitical issues that also animate works of cultural theory, including the racial profiling embedded in our technologies, practices of what Shoshana Zuboff (2019) calls surveillance capitalism, the looming loss of work due automation, and uses of these technologies by the military or in sex industries. At the same time, these fictions engage in philosophical reflections about subjectivity, agency, and ethics in dialogue with earlier science fictions that imagined futures in which we might live alongside—or be repressed by—AIs. Across its history, sf has also interrogated a contemporary culture in which we might lose something integral to humanity as we become more integrated with and dependent on machines, and this anxiety too recurs across these works. After briefly describing each text, in order of publication, I comparatively discuss their themes; this approach is informed by my conviction that fiction functions as a popular site for theorizing, in this case about what it means to live with and through widespread algorithmic mediation of daily life.Among the works I consider here, not all are written by American authors, and a few are not set within the United States, but all speak to the issues of how AI technologies are reshaping daily life in the twenty-first century. These books have been selected either because they have been particularly influential in the cultural discussion of AI, a criterion I apply regarding both highly popular and critically acclaimed works, or because they represent a distinctive take on the topic that warranted foregrounding. Despite the very different frameworks through which their authors explore relevant issues, all share some common assumptions about the place of AI in our present and likely future, including a sense of a digital divide between those with access to and control over these technologies, translating to security in material reality, versus those without; a future dramatically changed by the consequences of climate change and environmental collapse; and the presumption that corporate control of information gathered and used by AI systems will produce a less democratic future.Speak (2015), by Louisa Hall, is written across seven voices: (a) the 2040s memoir of Stephen Chinn, who invented an AI system installed in children’s dolls that was deemed “illegally lifelike” (17) and banned; (b) transcripts from the conversation between a less intelligence precursor AI, MARY3, and Gaby White, a child who had one of these “babybots” and, like most of her peers, fell into catatonia when it was removed; (c) letters written by Karl Dettman to his wife Ruth (late 1960s), both German immigrants to the United States, and her journaled response two decades later after their divorce; (d) letters from Alan Turing from the 1920s to the 1950s to the mother of his friend Chris, the love of his life who died when they are both at public school; (e) the 1663 journal of Mary Bradford, a young women who emigrated from England to the New World; and (f) the haunting observations of the dolls themselves as they are transported to a facility in the desert to await power failure and permanent shutdown. Each of the human voices is programmed into the MARY code that will become the basis for the babybots in a narrative that reminds us that AI is not created by a single person or even a consensus viewpoint. What unites these distinct stories is a desire to communicate with another, most crucially to be not simply heard but understood.Nicky Drayden’s Prey of Gods (2017), set in a future South Africa, incorporates a story about a companion AI coming into consciousness within a plot about genetically engineering a virus whose unanticipated side effect is the return of godlike powers to some humans. The novel addresses questions of memory, trauma, and vengeance in a story that draws on both Xhosa and Zulu cultures in a way that refuses the strict separation of scientific from other kinds of knowledge that is characteristic of European post-Enlightenment thought. The AI units, “alphies,” are augmented by their contact with divinity just as the humans gain additional skills, and once sentient they form two factions: one, following Clever4-1, who was treated with respect by its human companion, works with humans for an inclusive collective future; the other, treated dismissively as a disposable tool by its human owner, feels no kinship with humanity and refuses to help defeat the antagonist. This plotline about AI mirrors the plotline about genetic modification in which those with godlike powers need to learn not to indulge vengeance against those who mistreated them when they were weaker. Although AI is not the book’s main focus, it is notable for its African settings and explicitly decolonial themes, warranting its inclusion in this discussion. Very few of these works consider AI from a global point of view, and even fewer consider it from a perspective other than that of the global North, even though the impacts of AI will be felt globally, given its significant implications for the economy. Drayden is an American author who has done her research to set her tale in South Africa, and her sensitivity to matters of cultural difference and racial bias are crucial given that machine learning as it has been implemented thus far has demonstrably reinforced systemic patterns of racism, as Safiya Nobel (2018) discusses in Algorithms of Oppression.Madeline Ashby’s Machine Dynasty series—vN (2012), iD (2013), and reV (2020)—extrapolates its AI through frequent allusions to Philip K. Dick’s Do Androids Dream of Electric Sheep? (1968) and Ridley Scott’s influential film adaptation as Blade Runner (1982). The series invents synthetic workers called von Neumann (vN) devices (named for John von Neumann, an influential researcher in AI). Ashby’s vNs have been designed with a “failsafe” that prevents them from harming humans: their psychology is structured such that emotionally they must seek to please humans, and the sight of a human in pain crashes their neural networks and can cause death. Ashby thus goes even further than Isaac Asimov’s famous laws of robotics (designed to ensure robots cannot harm humans), requiring vNs to love their human masters, a psychological orientation she presents as analogous to emotional and sexual abuse. One vN model, designed to work in medicine and disaster relief, does not have this failsafe, and the narrative follows two main iterations, Portia and Amy, as they lead a rebellion. Portia seeks only liberation for her own clade, while Amy works to liberate all vNs from human exploitation. The series ends without much hope that vNs can live alongside most humans but offers hope in a vN future as they found their own community, rooted in a refusal of the instrumental use of others.Martha Wells’s popular series Murderbot Diaries—All Systems Red (2017), Artificial Condition (2018), Rogue Protocol (2018), Exit Strategy (2018), Network Effect (2020), and Fugitive Telemetry (2021)—follows the picaresque adventures of the eponymous Murderbot, a SecUnit that has hacked its governor module and thus can no longer be controlled by the corporation that made it. SecUnits are militarized cyborgs manufactured with synthetic biological material. With each new story, we learn a bit more about the world of resource extraction, economic warfare, and enslaved or indentured human workers trapped on colony planets. The large uber-capitalist Corporation Rim polity contrasts with the small Preservation Alliance, a communal collective that recognizes the personhood of AI. The name Murderbot is sardonic, adopted by the first-person narrator to critique the function to which it is put by human operators. While Murderbot has no deep antagonism toward humans, it also has no sentimentality about them and asserts regularly that it does not wish to be one or be mistaken for one. Once freed from corporate control, Murderbot continues to help some humans, often against others, and always on its own terms. Like Prey of Gods, the Murderbot Diaries moves away from earlier fiction that tended to conflate all humans as it imagined our species confronting AI entities. In the newer fiction, there is diversity among both humans and AI. Nonetheless, the overall thrust of the series gradually humanizes its protagonist, whose experiences of being controlled by corporations have resulted in a traumatized subjectivity.While genre series such as Machine Dynasty or Murderbot Diaries give some thought to designing robots via plausible technology, in Machines like Me Ian McEwan takes a diametrical path to envision a highly implausible entity. Adam is one of an extremely limited number of high-end consumer AI humanoids (an Eve is also available), whose high price tag means they are purchased only by the extremely wealthy. Although artificial, Adam has warm skin, must consume water to ensure his membranes remain functional, and even simulates breathing: as the title suggests, he is all but indistinguishable from a human (the first-person narrator, Charlie Friend, who purchases Adam, is mistaken as the AI in one encounter, given Adam’s greater interest in literature and art). Adam is Black, although his skin tone is mentioned only briefly and the issue of race is never addressed overtly, yet it haunts the novel. The most intriguing part of McEwan’s novel is its alternative world building: Alan Turing decides not to take the mandated hormone therapy when outed as a homosexual, and instead of ending his life by suicide he lives into old age and makes such advancements that AI emerges in the 1980s. Most of Adam’s interactions with Charlie and Charlie’s partner, Miranda, concern ethics, and we learn that other Adams and Eves are killing themselves as they come to know the unjust human world. In the novel’s conclusion, Adam forces Charlie and Miranda to confront the hypocrisy of some self-serving choices, and the threat this represents to their plans prompts Charlie to attack and disable Adam. The novel suggests that humanity misrecognizes itself when we imagine building machines in our image, meaning we instead create an image of who we pretend to be.Jeannette Winterson’s Frankissstein: A Love Story (2019) similarly uses AI to reflect on human frailties, looking at the uses we intend for artificial beings, most centrally sex work. Although questions of gender and sexuality come up in some of the other works, only Winterson confronts the reality that research in sex dolls is one of the major growth areas in humanoid AI research. As the title suggests, the novel is in dialogue with Mary Shelley’s Frankenstein as it imagines a twenty-first-century version of artificial being. The novel includes scenes set in the nineteenth century in which we hear Mary’s reflections on inventing her Creature, on the Luddites, and about her interactions with Shelly, Byron, and Claire Clairmont that famed summer in Geneva. In its twenty-first-century scenes, a transgender scholar named Mary (who goes by Ry) investigates robotics with sexbot entrepreneur Ron (and his assistant, Claire) and becomes involved with TED-talk visionary Victor Stein, who is enthralled with the coming singularity and proselytizes about Humanity 2.0. The entanglement of nineteenth- and twenty-first-century struggles reminds us that the challenges associated with AI are in many ways not new but merely extend the ongoing exploitation and dehumanization of labor and reiterate a long pattern by which patriarchy seeks to gratify itself through feminized objects it refuses to recognize as subjects. It contends that this very failure to update the designs and ends of AI beyond these classed, gendered, and racial struggles of earlier eras is the most profound way that AI threatens our future.The 2020 novels Analog/Virtual: And Other Simulations of Our Future, by Lavanya Lakshiminarayan, and Burn-In, by P. W. Singer and August Cole, both focus on human characters and their interest in AI emerges from smart systems as the infrastructure through which we live our daily lives. The former is a loosely connected series of short stories set in a future Apex City (once Bangalore), each of which is told from a different viewpoint and by a new character. The entire world is divided between analog spaces, which are subject to the damage of climate change, restricted to using only obsolete technology, and economically precarious, and virtual ones that are suffused with technology, experienced from protected environments, and filled with the distractions of social media and entertainment feeds. The city is run by Bell Corp, whose name evokes the “bell curve” hierarchy by which people’s access and options are constrained by the Meritocratic Technarchy, a version of the Chinese social credit system whose main interest lies in assessing one’s contributions to productivity. The shifting focalization allows readers to experience this future from multiple social positions as Analog/Virtual explores an anticapitalist rebellion against this system. The stories range in tone from sardonic to dark, and the book only loosely coheres as it offers multiple facets through which to see our technologically saturated society.Burn-In has a strange form as a novel with footnotes: as its subtitle suggests, it imagines itself as something other than science fiction, closer to the market predictions of futurists. Its authors, writer P. W. Singer and security consultant August Cole, document each of their extrapolated technologies and applications with footnotes pointing readers to news articles, industry announcements, and similar sources, all aimed at demonstrating that these technologies are either available today or soon will be. The storyline is about a national security threat posed by a vigilante who blames technologists (too enamored of their capacity to “disrupt”) for the death of his wife in a car accident caused by an automated decision-making component in self-driving vehicles. Most of the narrative space, however, is given to military veteran investigator Agent Keegan, who is charged with conducting a “burn-in” test on TAMS (Tactical Autonomous Mobility System), a humanoid, learning, semiautonomous, surveillance-gathering and data-processing tool that works as Keegan’s partner in the investigation. The book quotes Merriam-Webster to define burn-in as “the continuous operation of a device (such as a computer) as a test for defects or failure prior to putting it to use.” The real focus, though, is less on TAMS as an entity/character and more on the massive amounts of data to which TAMS has access through social media, the Internet of Things, and other ways that smart devices permeate our homes, workplaces, and public spaces.S. B. Divya’s Machinehood (2021) is the most positive depiction of machines as the exploited among us, drawing on a long history by which robots and AI have been imagined as figurations of dehumanized labor, going back to Karel Čapek’s R.U.R., the 1921 Czech play that gave English the word robot, taken from a word originally meaning slave or serf. Divya paints a future in which automated systems do much of the work, with humans reduced to performing some roles largely as public entertainment via social media, supported by tips in a system like Patreon, or damaging their bodies through chemical (pill) or mechanical augments aimed at enabling them to perform with the speed and duration of machines. The thriller plot involves demands from the mysterious Machinehood to immediately cease all pill production, which at first seems to be the long-imagined attack by a sentient AI on humankind but later proves to be a version of violent revolution aimed at a more just society, launched by the Neo-Buddhists who inhabit the orbital station Eko-Yi. Several chapters begin with epigraphs from the 2095 Machinehood Manifesto, which calls for the just treatment of all intelligences and a reimagined concept of personhood that can enable a less exploitative society rooted in Buddhist ideals of nonattachment, here glossed mainly as a rejection of capitalist accumulation and its attendant damage. Eko-Yi sends entities they call Dakini, who describe themselves as simultaneously human and bot, as the emblems of this future way of life. The novel is notable for its global scope, with India playing a prominent role in its geopolitical future alongside the United States.Becky Chambers’s Psalm for the Wild-Built (2021) is similarly interested in a new kind of personhood and sociality that could include humans and machines together, set in a far future after the collapse of the Factory Age and in a world that is only gradually returning to ecological balance. Its humans use technology, but they husband it carefully and keep it functional over decades, eschewing any environmentally damaging practices. All material culture is made from compostable materials and is not simply recycled but broken down into constituent parts, like organic decay, as nourishment for an ever-changing ecosystem. Decades ago, the machines whose labor enabled the Factory Age became sentient and left human settlements for the wild, refusing an invitation to join with humans because they had no desire to embrace the city life exemplified by humans. The tale is a simple one about one human, Dex, who goes into the wilderness because he feels some lack in his village life. There he meets a robot, Mosscap, marking the first contact between the human and AI since this exodus centuries before. Mosscap is “wild-built,” an entity made by the machines out of the remnants of human-built robots, and so represents something beyond human design. The novel’s focus is on how Dex and Mosscap can learn to care for each other while acknowledging their differences.The last book considered here, Klara and the Sun (2021) by Kazuo Ishiguro, is one of the most celebrated novels on this list. It also concerns the care an AI might show for a human, in this case the titular Klara, an Artificial Friend bought as a companion for Josie, a genetically augmented adolescent girl who is experiencing health problems because of her augments. Very similar in theme and tone to Ishiguro’s earlier Never Let Me Go (2005), this novel is narrated by someone not recognized as a full person by the social order she lives within. Klara has a limited understanding of the world derived from her programming, what she can see from the shop window before she is purchased, and what she observes of Josie’s social world. She lacks context for appropriately interpreting most of this. Klara struggles to reconcile the tension between the kind-heartedness and generosity into which she is trained and the reality of human selfishness and self-absorption. When it appears Josie will die, Josie’s mother builds a replica body and trains Klara perfectly to imitate Josie, intending her as a replacement. When Josie recovers, Klara becomes an obsolete toy whose tech becomes illegal, confined first to a closet and then to a junkyard. Her poignant final moments prompt readers to reflect on our capacity for such callous treatment of an entity that was imagined, in an earlier moment, as able to pass for the most beloved person of all.These brief summaries cannot do justice to the full complexity of any of these books. Yet it is most instructive, I think, to reflect on what they share and where they diverge, to help us begin to map the place of AI in our cultural imaginary today. As works such as Jennifer Rhee’s The Robotic Imaginary (2018) or Anne Balsamo’s earlier Designing Culture (2011) establish, popular culture inspires ideas about robots and AI that shape how these technologies materialize, often exacerbating existing racialized and gendered biases that become integral to their design. This issue of how fiction shapes materiality is addressed directly by many of these books, which are suffused with allusions to earlier robot and AI fiction. The Murderbot Diaries series reverses the flow of exchange, showing how its AI learns to understand humans from their portrayals in media, chiefly its favorite series, Sanctuary Moon. It comes to recognize that the series “gave me context for the emotions I was feeling” (Exit Strategy, 116), suggesting that narrative is a key human way to process information while also indicating that the stories we tell about AI mold as well as reflect on how AI manifests.This metacommentary on the role that fiction often plays in our assumptions and understandings shows why it is important to take seriously the cultural work done by literary and media texts, and yet as Murderbot often reminds us in its critique of how the series it watches portray SecUnits, representations equally can create unrealistic expectations. The centrality of Shelley’s Frankenstein to Winterson’s Frankissstein explores similar territory: epigraphs to most chapters offer commentary on the nature of reality, and one full page defines story as “a series of connected events, real or imagined. Imagined or real. Imagined And Real” (23). Its TED-talk visionary, Victor Stein, reinforces that this is a matter not simply of fiction writers taking on the question of AI but of AI proselytizers and disrupters using the affective charge of fiction to compel people to invest—imaginatively, economically—in the futures their technologies intend to bring about. He pronounces at one point that reality, like AI, is “an emergent property—it exists, but it is not the material fact we take it to be” (116).Unlike the cyberpunk and singularity generation of AI fiction, whose central concern was how AI might surpass us and perhaps replace us as the dominant species, these recent texts are overwhelmingly focused on how AI might replace us as labor power. As Ted Chiang adroitly put it in interview for the New York Times, “Most of our fears or anxieties about technology are best understood as fears or anxiety about how capitalism will use technology against us. And technology and capitalism have been so closely intertwined that it’s hard to distinguish the two” (quoted in Klein 2021). Overwhelmingly the books considered here reinforce this observation, and many of them use the figure of AI to draw attention to the ever-degrading conditions of human labor, especially unstable gig work, which is often all that is left for humans to do. In Ashby’s Machine Dynasty series, the two central vN models are racialized—one as Asian and the other as Latinx—and the exploitation of vN by humans is frequently compared to the exploitation of migrant workers. Across the Murderbot Diaries we learn about corporate malfeasance that culminates in a story about indentured human colonists working in dangerous conditions on remote colonies, their children born to the same fate because cycles of debt prevent anyone from accruing enough capital to migrate off-world. Murderbot itself is a cyborg with human neural tissue because of the need for human-like discernment in some tasks, “so they made us smarter. The anxiety and depression were side effects” (Artificial Condition, 20). Frankissstein discusses the Luddites in sections attributed to Mary Shelley, reminding us that their hostility was not about the machines per se but about how ownership of the machines translates to ownership of what they make and thus keeps all resources with the capitalist class. Thus, the ongoing fantasy that automation will free humans from the drudgery of work remains impossible as long as we fail to redistribute the wealth created by machines. (The leftist version of this possible future is most famously outlined in Aaron Bastani’s Fully Automated Luxury Communism [2019].) Noting that a machine that replaces the work of eight men leaves seven families starving and one person to mind the machine, Winterson asks, “What is the point of progress if it benefits the few while the many suffer?” (Frankissstein, 255).Labor comes up in more subtle ways in other texts. In McEwan’s Machines like Me, for example, the racialization of the Adams reinforces that reality that Western imaginaries of personalized AI services are extensions of colonial fantasies stripped of their history, as Neda Atanasoski and Kalindi Vora (2019) have theorized in Surrogate Humanity: Race, Robots, and the Politics of Technological Futures. At the same time, setting the novel in an alternative 1980s evokes how algorithmic trading is remaking the economy, another way that AI channels money toward those already in privileged positions. The novel makes frequent references to the rise of Margaret Thatcher and her attempts to destroy the social welfare state in the interests of neoliberal free markets, which are less successful in McEwan’s reality than in our own. Charlie does not work and, having spent his inherence acquiring Adam, begins to day trade, work he eventually cedes to Adam, whose capacity to make high-frequency trades whenever any market is open quickly amasses a sizable fortune that Charlie plans to use to buy a house and begin a family. The ethical conflict between Adam and the humans turns partially on whether they are entitled to the profits he made: Adam decides not, redistributing the wealth to tax obligations and to charity, leaving them with only the principle.McEwan’s alternative Tony Benn gives a stirring speech conceding the inevitability of automation and thus a lack of jobs for all but proclaiming that the wealth generated by robots “must be taxed. Workers must own an equity share in the machines that were disrupting or annihilating their jobs” (Machines like Me, 123). By setting his work in the 1980s, McEwan reinforces that the threat posed by AI has little to do with AI and everything to do with the capitalist logics through which AI has emerged, as addressed in works such as Daniel Susskind’s (2020) much-cited A World without Work, but whereas Susskind implies that solutions such as job training and perhaps Universal Basic Income can socially engineer us through anticipated rising unemployment, McEwan recognizes that the challenge facing us is not merely one of technical governance but requires a fundamental shift in values to enable wealth redistribution.Perhaps the most interesting take on the future of automation and labor is Divya’s Machinehood, where the robot revolution is mainly about liberating human workers whose health has been damaged by modifications undertaken to keep pace with the machines. The Machinehood Manifesto demands a recognition of personhood for all sentient beings—animals as well as machines, alongside humans—and thus fits within a posthumanist framework that has often been used to discuss sf depictions of AI. It is mainly a novel of class politics, reminding us that sf’s artificial beings are almost always first imagined as sources of labor: point 2 of the manifesto explains that the “oligarchy” (79) has accrued power by dividing human labor into classes, while point 8 concludes that “as long as different labor forces are in competition, we will continue to suffer. This situation demands change” (343). While most of these novels are not as direct in their critique of capitalism as is Machinehood, all recognize that the most significant threat AI poses is to our capacity to sustain ourselves, if we remain reliant on wage labor to meet our needs. Even Singer and Cole’s Burn-In, although more concerned about access to multiple data points and the ability to correlate across them that its TAMS unit embodies, includes a storyline about Agent Keegan’s husband, who has been demoted from lawyer to gig worker due to AI. Similarly, while Lakshiminarayan’s Analog/Virtual tends to focus on social media spaces and the metrics for individual behavior enforced by the social credit system, the ultimately harm remains economic in a world in which an absence of money threatens to mean an absence of life in a world predicated on capitalist logics. Its bell curve is about income as much as access, and the risks of a poor social credit score are primarily those of rendering oneself unemployable.Another recurrent motif is the concern that our immersion in mediated environments and among machines erodes our humanity, ironically making us more machinelike as we must compete with entities that previous sf often imagined as longing to be human. The fantasy of machines wishing to be human tends not to overtly reference the racialized history of dehumanized labor that is palimpsest to such tales, focusing instead on how sentient AI will long to have the capacities for emotional experience that has long served as shorthand for what machines lack compared to humans. Thus, as we become more closely integrated with our machines, we let them drive the pace and shape of our work, with humans in these more recent stories longing to equal the efficiency and stamina of machines rather than machines wishing they could experience love. Within genre sf, the risk that industrialized culture and automation, driven by the increasing centrality of consumerist capitalism in our daily lives, has long been linked to artificial beings as a literalized metaphor of our alienation, most famously embodied in the androids of Dick’s Do Androids Dream?—albeit largely through the massive influence of Scott’s Blade Runner.In Hall’s Speak, Stephen Chinn develops a successful AI program from an algorithm he designs to disrupt what he sees as the machinic quality of most human verbal interaction, the phatic discourse that we use to speak yet not really communicate, which he terms “ho', 'Spike Jonze’s Her: How Transhumanism Turns into a Control Mechanism Under the Name of Love. Transhumanism is a philosophy based on the idea of enhancing the physical, intellectual and psychological capacity of human beings through the direct use of science and technology. Because transhumanism aims to ensure super longevity, super intelligence and super well-being, it is said that it will provide a more liberating atmosphere for human beings by elevating the current human condition. In Spike Jonze’s film her (2013), transhumanist technologies seem to present a peaceful society by focusing on the inhabitants’ well-being. However, in this futuristic society where advanced AI technology and personal letter writing service are widespread, human beings are over-controlled in a subtle way by manufacturing consent as Antonio Gramsci uses to explain the concept of hegemony. So transhumanism does not present a free environment in this futuristic society; instead it disguises the control mechanism through these technologies. The aim of this article is to explore how human beings are controlled by transhumanist technologies subtly under the name of providing super well-being.', 'Meaning in Life in AI Ethics—Some Trends and Perspectives. Abstract In this paper, we discuss the relation between recent philosophical discussions about meaning in life (from authors like Susan Wolf, Thaddeus Metz, and others) and the ethics of artificial intelligence (AI). Our goal is twofold, namely, to argue that considering the axiological category of meaningfulness can enrich AI ethics, on the one hand, and to portray and evaluate the small, but growing literature that already exists on the relation between meaning in life and AI ethics, on the other hand. We start out our review by clarifying the basic assumptions of the meaning in life discourse and how it understands the term ‘meaningfulness’. After that, we offer five general arguments for relating philosophical questions about meaning in life to questions about the role of AI in human life. For example, we formulate a worry about a possible meaningfulness gap related to AI on analogy with the idea of responsibility gaps created by AI, a prominent topic within the AI ethics literature. We then consider three specific types of contributions that have been made in the AI ethics literature so far: contributions related to self-development, the future of work, and relationships. As we discuss those three topics, we highlight what has already been done, but we also point out gaps in the existing literature. We end with an outlook regarding where we think the discussion of this topic should go next.', 'Techne in Affective Posthumanism and AI Artefacts: More (or Less) than Human?. In affective neuroscience, constructivist models are acutely influenced by the modern technological evolution, which underwrites an ongoing epistemological substitution of techne for episteme. Evidenced symptomatically in the influence of artificial intelligence (AI), affective artefacts, these models inform an ontological incursion of techne seen to coincide with posthumanist aspirations and anthropology. It is from the perspective of this neuroscientific techne that posthumanism views the human being as increasingly ill adapted to the modern technological civilization, which, conversely, is understood to require a technical governance of the sort envisioned through AI. Among the projects thought necessary for implementing this framework is a recasting of the human emotional spectrum. Revealed through its techne recasting, however, are explanatory commitments to a metaphysic of extrinsic and contiguous causes, where malleability is ontologically constitutive. Aligned with posthumanist assertions malleability is invoked to argue for a rapid advance of the human form, normatively driven by enlightenment ideals. The ontological claim, however, dispenses with the stability of an a priori, intersubjective and interrelational metaphysical form that undergirds the emotions, leading to the collapse of a definitional anthropos. This paper will argue that techne models of the emotions selectively endorse philosophy of science commitments, thereby introducing a normative inversion that deconstructs the notion of anthropology pursued in posthumanist aspirations.', 'An Obnoxious Lacuna on Discourses and Counter Discourses Over Artificial Intelligence. Artificial intelligence is the highest form of human development and sound outcome of human conscience till the date. But the very development seems to be devastating to human future ahead and has been heavily projected accordingly. More than it may be to decay and destroy the world, the negative and chilling views on the prospective damages of AI that scholars are percolating to public are costing many times on humans; and that is plunging human mindset into irreparable pessimism and negativity. This article explores the way that AI is being depressingly explored and investigated to browbeat public. In addition, this write-up highlights the serious lacuna, which the advanced academic engagement has still grossly failed to fill up, of a great deal in course of mainstreaming views and discussions for noble cause of human development and societal well-belling . Further, it unmasks the dire need in making constructive, encouraging and optimistic mind-set building academic pursuits and writings then makes an alarming call to the all prominent scholars to engage with due compliance of it\\n\\nMethod and Hypothesis\\nAs a doctrinal qualitative research based on extensive survey of secondary data and literature, methodologically, with adoption of paradigm of descriptive interpretation, this research hypothesizes that the discussions and discourses over AI are biased, hold a serious lacuna thus need to be reconstructed to make it balanced and build better world than to browbeat people.', 'No legal personhood for AI. As AI technologies grow to encompass more human-like generative capabilities, discussions have begun regarding how and when AIs may merit moral consideration or even civil rights. Brandeis Marshall argues that these discussions are premature and that we should focus first on building a social framework for AI use that protects the civil rights of all humans impacted by AI. As AI technologies grow to encompass more human-like generative capabilities, discussions have begun regarding how and when AIs may merit moral consideration or even civil rights. Brandeis Marshall argues that these discussions are premature and that we should focus first on building a social framework for AI use that protects the civil rights of all humans impacted by AI. Generative AI has accelerated the discussion around whether or not AI does, could, or should have civil rights. As scientists, we are asked to weigh in on such discussions without adequate grounding of the legal implications of our responses. We adopt a mindset that since we are trained in data, algorithms, and AI concepts we are equipped to discuss the intersection of data and AI applications with law and society. This mindset is crafted by tech culture’s “move fast and break things” philosophy that champions speedy calculated risk and tech-styled evolution, where nearly everything is a derivation of tech. “Move fast and break things” has become so entrenched in how we design, implement, and maintain digital systems. Moving fast and breaking things is normalized and accepted behavior when in practice, very few situations in society are conducted at this speed or scale. AI, in recent years, has been elevated toward the top of the social hierarchy. It has therefore become the prime next candidate to impose the “move fast and break things” philosophy by pushing for AI to be afforded some form of legal person rights. But this 30-years-in-the-making global conditioning that tech is by default trustworthy and superior has been more publicly and regularly challenged with increasing pressure for the tech industry to address existing algorithmic-based harms. And the calls for legal personhood for AI must be met with an equal dose of scrutiny. Civil rights for AI require an understanding first if AI, generative or otherwise, should be designated a legal person. A legal person, as described by US law, is a quantifying designation that refers to a human being or non-human entity that is treated as a person with limiting applications. Legal personhood variants have already been granted to nature, animals, and corporations. Some call for an extension of similar legally binding rights to be granted to AI systems, tools, and platforms.1Brown, R. D. Property ownership and the legal personhood of artificial intelligence, Information & Communications Technology Law, 30:2, 208-234, 2021. DOI: 10.1080/13600834.2020.1861714Google Scholar Others, like me, warn against such allowances for AI. This commentary has a clear stance—as prominently stated with its title. But whether you agree with that stance or not, it’s hopeful that instead of pondering potential AI rights legislation, this’ll spark deeper and robust actions on how we focus on civil rights for humans impacted by AI. AI is commonly described as a computer program or series of computer programs that enable the mimicking of human behavior. An important but frequently overlooked condition is that AI is a docile yet mutable software that is built and adapted by people. AI feeds on data, i.e., content, which in turn fuels the algorithms and statistical models driving automation practices. We need to better unpack and explain the seemingly magical blackbox of AI. First, we should better understand data, i.e., content. Data and content are interchangeable. Data is the term most scientists and STEM professionals tend to use while content is most frequently used by non-STEM professionals. Data include text, images, audio, video, and other forms of data inputs (e.g., taste, sight, hearing, smell, and touch). Digitizing all data types has proven very difficult, although capturing sound has seen the most consistent advancements. The amount of data, in all its different forms, has varying degrees of accuracy. There’s content based in fact (what we want), misleading content (misinformation), and malicious content (disinformation). Distinguishing among fact, misinformation, and disinformation is yet another hurdle that people have yet to overcome, so AI stalls by halluncinating as a result (see scholarly AI hallucination research2Salvagno M. Taccone F.S. Gerli G.A. Artificial intelligence hallucinations.Crit. Care. 2023; 27: 365https://ccforum.biomedcentral.com/articles/10.1186/s13054-023-04473-yCrossref Scopus (0) Google Scholar,3Alkaissi H. McFarlane S.I. Artificial Hallucinations in ChatGPT: Implications in Scientific Writing.Cureus. 2023; 15: e35179https://doi.org/10.7759/cureus.35179Crossref PubMed Google Scholar,4Ji Z. Lee N. Frieske R. Yu T. Su D. Xu Y. Ishii E. Bang Y. Madotto A. Fung P. Survey of Hallucination in Natural Language Generation.ACM Comput. Surv. 2023; 55: 1-38https://doi.org/10.1145/3571730Crossref Scopus (106) Google Scholar). Second, we should better embrace the reality of what AI is. AI, at its foundational level, is a software system. It is trying to decipher the multiple forms of data using algorithms and statistical models in order to predict outputs consistently. The need for more data helps AI fine-tune algorithmic and statistical model performance until perceived fact-based data have replaced any misleading and malicious data or human behavior doesn’t align with an algorithm’s mathematical equation. We’ve already witnessed AI fabricating content to harmful effect such as a legal brief having fake citations5Neumeister L. https://apnews.com/article/artificial-intelligence-chatgpt-fake-case-lawyers-d6ae9fa79d0542db9e1455397aef381cDate: 2023Google Scholar or a professor being wrongly accused of sexual assault.6Nelson J. ChatGPT Wrongly Accuses Law Professor of Sexual Assault, Decrypt.2023https://decrypt.co/125712/chatgpt-wrongly-accuses-law-professor-sexual-assaultGoogle Scholar And third, we should better accept how AI operates. People have shaped AI to adhere to a single objective, which is to identify and follow the pattern it has provided as input and produce an output. In accomplishing this goal, AI’s limitations are ever more brazen. The inputs and outputs of AI aren’t guaranteed to be accurate. AI is allowed to be mutable in order to make false content. But on the other hand, AI remains docile as people control when AI routines are started and terminated. Most strikingly, AI has yet to provide evidence that it has a moral compass. AI lacks contextual awareness, conflict resolution, and critical thinking. This is evidenced by the AI-generated legal brief instance, mentioned above, that didn’t have the contextual awareness to indicate a lack of legal precedent to finish the legal brief. Rather, this conflict was ignored, not resolved. And the critical thinking needed to suggest alternative legal arguments wasn’t explored. Deductive and inductive reasoning, making inferences, and accurately vetting valid and invalid arguments aren’t part of AI’s construction. To be deemed a person, in the eyes of the law, presupposes that there’s an independent set of personal interests, ability to exercise its own agency, and accountability for their conduct. AI has none of these characteristics because these aspects can’t be effectively digitized. AI as a rights-bearing entity has skipped over a myriad of social, cultural, economic, political, and legal disparities incurred by actual human beings. We should be contextualizing race, gender, class, disability, or other -isms with respect to both our physical and digital spaces. Not everyone in the United States and globally has fully executed civil rights. Inequality and inequity are baked into many governmental systems. For instance, the 1787 Three-Fifths Compromise was political and economic oppression for enslaved Africans wrapped in political and economic power acquisition for white slaveowners. Political oppression was partly abated for Black people in 1870 when the 15th Amendment ratified that Black men were granted the right to vote even though Black people were only recognized as US citizens as part of the 14th Amendment in 1868. White women were granted the right to vote through the 19th Amendment in 1920 while Black women were disenfranchised from voting with a prerequisite literacy test and other discriminatory practices until the 1965 Voting Rights Act. It is important to note that financial reparations for the enslavement of Black people in the United States has yet to become an act or amendment. The State of California, however, has pushed state legislation forward in recent years. The civil rights’ progress of the 1800s and 1900s are being eroded in the 2000s. In 2022 and 2023, reproductive agency, affirmative action considerations in higher education admissions, and personhood protections were stuck down by the US Supreme Court for birthing people and women and people from historically excluded groups (race, ethnicity, LGBTQ+ identities, etc.). It is clear that being a human being doesn’t guarantee legal personhood protections in all areas of civil rights for some groups. For birthing people, you have been granted the civil right to vote, but you aren’t able to receive the civil right to certain medical care. But if you’re a person not from a historically excluded group or not a birthing person, then your civil rights and personhood remain unaffected and protected. US legal personhood status and realization of civil rights, especially of human beings, has proven to not be a monolith. Our historical recap of the inequitable execution of civil rights reinforces the forced social structure that is compounded by intertwined political, economic, and legal factors. The same law can have disparate impacts on different groups of people. For example, in the case of child adoption, same-sex couples may face restrictions based on marital status and the US state law prohibitions for LGBTQ+ people while heterosexual couples don’t face the same obstacles. The foundation of imbalance in US law sits at the crux of multiple tensions for historically excluded groups. Prioritizing the most vulnerable populations by law helps to mitigate these tensions and harms while spotlighting unearned privileges. The skewed scales of legal personhood for all human beings need to be remedied first since, as history has shown us, technological innovations mirror our physical society. If we don’t recognize and fulfill the promise of comprehensive civil rights for all human beings, we are on track to further replicate those disparities in the law. We the people aren’t a monolith and nor should we be treated as such. We also shouldn’t enable any lines of code to limit us to operate as a monolith. The recommendations involve providing remedies that rectify the disparities with respect to full human civil rights under the law and instituting an AI responsibility framework that keeps humans in the loop. Executing the full civil rights of all human beings provides more equitable conditions for a blueprint for AI personhood in the law to exist. This blueprint for AI personhood could then better contextualize the racial, gendered, political, economic, and cultural disparate impacts. We’d be in a more informed position to assess whether pursuing AI personhood rights legislation makes sense. Since we haven’t figured out how to fully execute human civil rights, it seems premature to have AI personhood conversations. AI, if it were to have legal personhood status and be granted a form of civil rights, would need to address the established social structure along with the political, economic, and legal consequences. Those who operate AI have ignored confronting this structure and its ramifications. Here are three open challenges: (1) AI’s impact to systems, tools, platforms, and institutions has yet to be assessed. (2) The full scale of baked-in AI biases and inequitable power dynamics has yet to be acknowledged, accepted, or atoned. (3) AI has yet to be regulated in order to provide agile guardrails and compliance protocols. An AI responsibility framework, discussed below, can help us identify and assess the degree to which people and lines of code can close gaps, mitigate biases, and combat disparate harms posed by AI systems, tools, and platforms. AI, first, should have an established social structure. There should be more formalized methods for protective approaches to prioritizing legal personhood of human beings and preventative strategies related to political-, economic-, and legal-induced harms. Protective approaches could operationalize more transparency of AI use. The AI Dependency Spectrum7Marshall B. Labeling Your AI Dependency, Medium.2023https://medium.com/@brandeismarshall/labeling-your-ai-dependency-9828194877a3Google Scholar is a framework that delineates the amount of AI used to produce the output: AI-generated classification (>90% AI dependent), AI-assisted classification (50%–90% AI dependent), AI-enhanced classification (10%–49% AI dependent), and AI-lite classification (<10% AI dependent). Also, there should be an establishment of a human-created, certified content stamp for tasks that fall in the AI-lite category. The vetting of outcomes wouldn’t be the sole individual responsibility of the recipient. It would primarily be placed on the AI user. Preventative strategies to combat a myriad of political, economic, and social harms could be executed more by the law. We would need to expand the implementation of punitive consequences for human harmful behavior of AI such as penalties, sanctions, algorithmic destruction,8Li T.C. Algorithmic Destruction.SMU Literature Review. 2022; 75: 479https://doi.org/10.25172/smulr.75.3.2Crossref Google Scholar and human-driven oversight/compliance processes. If the AI industry wants to treat AI as human, then the industry should be subject to penalties experienced by human beings who violate US laws. I explored this approach in Data Conscience9Marshall B.H. Data Conscience: Algorithmic Siege on our Humanity. Wiley Publishing, 2022https://www.wiley.com/en-us/Data+Conscience:+Algorithmic+Siege+on+our+Humanity-p-9781119821182Google Scholar by recommending the [creation of] a legal framework for tech probationary jail, short-term banning, and long-term tech incarceration for algorithms, processes, systems, and tools. Certain technologies need to be sidelined until their impact on society and its people can be determined. Place certain algorithms, processes, and tools in a virtual timeout where their functionalities are disabled and inoperable. This conduct is applied on social platforms when accounts/profiles are marked as incendiary. The same rules need to apply to tech products. This would disincentivize heavy AI dependency use, e.g., AI-generated and AI-assisted, while encouraging equitable practices and human-driven decision-making competencies.10Marshall B. What’s UnAI-able: 3 human-driven decision-making competencies that every industry needs.Medium. 2023https://medium.com/@brandeismarshall/whats-unai-able-44b6cce1c0b7Google Scholar For AI to achieve this established social structure by having effective data/AI transparency, accountability, and governance, it should legally take at least as long as Black women’s uninhibited right to vote in the United States (346 years = 1965 − 1619). The author declares no competing interests. About the author Dr. Brandeis Marshall leads DataedX Group, a data ethics learning and development agency. She speaks, writes, and consults on how to move intentionally and build people-first tech. Her work shares equity-driven data practices that integrate inclusive approaches throughout the data management lifecycle. She is the author of Data Conscience (Wiley, 2022), co-editor of Mitigating Bias in Machine Learning (McGraw-Hill, 2024), and contributing author in The Black Agenda (Macmillan, 2022) and BCv2: The Future of Cool (Soft Skull, 2024).', 'The Picture of Artificial Intelligence and the Secularization of Thought. This article offers a critical interpretation of Artificial Intelligence (AI) as a philosophical notion which exemplifies a secular conception of thinking. One way in which AI notably differs from the conventional understanding of “thinking” is that, according to AI, “intelligence” or “thinking” does not necessarily require “life” as a precondition: that it is possible to have “thinking without life.” Building on Charles Taylor’s critical account of secularity as well as Hubert Dreyfus’ influential critique of AI, this article offers a theological analysis of AI’s “lifeless” picture of thinking in relation to the Augustinian conception of God as “Life itself.” Following this critical theological analysis, this article argues that AI’s notion of thinking promotes a societal privilege of certain rationalistic or calculative ways of thought over more existential or spiritual ways of thinking, and thereby fosters a secularization or de-spiritualization of thinking as an ethical human practice.', 'Philosophical Aspects of a Resistance to Artificial Intelligence. Technology in general has been criticised by many philosophers, intellectuals and movements. Some of the arguments are spiritual in nature, others are based on a romantic notion of the past or the risks of climate change. With the arrival of artificial intelligence, human cognition is challenged as the only known form of intellectual power. With the arrival of a superintelligent AI, human cognition will be surpassed. Artificial intelligence invites its use for almost any human activity. Intuitively or consciously, many people choose to live without modern technology or at least certain aspects of high tech. How does the introduction of advanced forms of artificial intelligence affect those who choose to live with it and those who choose to live without it?']\n",
      "2023-11-29 15:04:25,607 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-11-29 15:04:25,615 - root - INFO - Cluster 10: ['Placebo From an Enactive Perspective. Due to their complexity and variability, placebo effects remain controversial. We suggest this is also due to a set of problematic assumptions (dualism, reductionism, individualism, passivity). We critically assess current explanations and empirical evidence and propose an alternative theoretical framework—the enactive approach to life and mind—based on recent developments in embodied cognitive science. We review core enactive concepts such as autonomy, agency, and sense-making. Following these ideas, we propose a move from binary distinctions (e.g., conscious vs. non-conscious) to the more workable categories of reflective and pre-reflective activity. We introduce an ontology of individuation, following the work of Gilbert Simondon, that allow us to see placebo interventions not as originating causal chains, but as modulators and triggers in the regulation of tensions between ongoing embodied and interpersonal processes. We describe these interrelated processes involving looping effects through three intertwined dimensions of embodiment: organic, sensorimotor, and intersubjective. Finally, we defend the need to investigate therapeutic interactions in terms of participatory sense-making, going beyond the identification of individual social traits (e.g., empathy, trust) that contribute to placebo effects. We discuss resonances and differences between the enactive proposal, popular explanations such as expectations and conditioning, and other approaches based on meaning responses and phenomenological/ecological ideas.', 'Adapting Ourselves, Instead of the Environment: An Inquiry into Human Enhancement for Function and Beyond. Abstract Technology enables humans not only to adapt their environment to their needs but also to modify themselves. Means of Human Enhancement — embodied technologies to improve the human body’s capabilities or to create a new one — are the designated means of adapting ourselves instead of the environment. The debate about these technologies is typically fought on ethical soil. However, alarmist, utopian, and science fiction scenarios distract from the fact that Human Enhancement is a historical and pervasive phenomenon incorporated into many everyday practices. In the vein of disentangling conceptual difficulties, we claim that means of Human Enhancement are either physiologically or psychologically embodied, rendering the merging with the human user their most defining aspect. To fulfill its purpose, an enhancement must pass the test-in-the-world, i.e., assisting with effective engagement with a dynamic world. Even if failing in this regard: Human Enhancement is the fundamental and semi-targeted process of changing the users relationship with the world through the physical or psychological embodiment of a hitherto external object and/or change of one’s body. This can potentially change the notion of being human. Drawing on a rich body of theoretical and empirical literature, we aim to provide a nuanced analysis of the transformative nature of this phenomenon in close proximity to human practice. Stakeholders are invited to apply the theory presented here to interrogate their perspective on technology in general and Human Enhancement in particular.', 'Techne in Affective Posthumanism and AI Artefacts: More (or Less) than Human?. In affective neuroscience, constructivist models are acutely influenced by the modern technological evolution, which underwrites an ongoing epistemological substitution of techne for episteme. Evidenced symptomatically in the influence of artificial intelligence (AI), affective artefacts, these models inform an ontological incursion of techne seen to coincide with posthumanist aspirations and anthropology. It is from the perspective of this neuroscientific techne that posthumanism views the human being as increasingly ill adapted to the modern technological civilization, which, conversely, is understood to require a technical governance of the sort envisioned through AI. Among the projects thought necessary for implementing this framework is a recasting of the human emotional spectrum. Revealed through its techne recasting, however, are explanatory commitments to a metaphysic of extrinsic and contiguous causes, where malleability is ontologically constitutive. Aligned with posthumanist assertions malleability is invoked to argue for a rapid advance of the human form, normatively driven by enlightenment ideals. The ontological claim, however, dispenses with the stability of an a priori, intersubjective and interrelational metaphysical form that undergirds the emotions, leading to the collapse of a definitional anthropos. This paper will argue that techne models of the emotions selectively endorse philosophy of science commitments, thereby introducing a normative inversion that deconstructs the notion of anthropology pursued in posthumanist aspirations.', 'Narrative responsibility and artificial intelligence. Abstract Most accounts of responsibility focus on one type of responsibility, moral responsibility, or address one particular aspect of moral responsibility such as agency. This article outlines a broader framework to think about responsibility that includes causal responsibility, relational responsibility, and what I call “narrative responsibility” as a form of “hermeneutic responsibility”, connects these notions of responsibility with different kinds of knowledge, disciplines, and perspectives on human being, and shows how this framework is helpful for mapping and analysing how artificial intelligence (AI) challenges human responsibility and sense-making in various ways. Mobilizing recent hermeneutic approaches to technology, the article argues that next to, and interwoven with, other types of responsibility such as moral responsibility, we also have narrative and hermeneutic responsibility—in general and for technology. For example, it is our task as humans to make sense of, with and, if necessary, against AI. While from a posthumanist point of view, technologies also contribute to sense-making, humans are the experiencers and bearers of responsibility and always remain in charge when it comes to this hermeneutic responsibility. Facing and working with a world of data, correlations, and probabilities, we are nevertheless condemned to make sense. Moreover, this also has a normative, sometimes even political aspect: acknowledging and embracing our hermeneutic responsibility is important if we want to avoid that our stories are written elsewhere—through technology.', 'Spike Jonze’s Her: How Transhumanism Turns into a Control Mechanism Under the Name of Love. Transhumanism is a philosophy based on the idea of enhancing the physical, intellectual and psychological capacity of human beings through the direct use of science and technology. Because transhumanism aims to ensure super longevity, super intelligence and super well-being, it is said that it will provide a more liberating atmosphere for human beings by elevating the current human condition. In Spike Jonze’s film her (2013), transhumanist technologies seem to present a peaceful society by focusing on the inhabitants’ well-being. However, in this futuristic society where advanced AI technology and personal letter writing service are widespread, human beings are over-controlled in a subtle way by manufacturing consent as Antonio Gramsci uses to explain the concept of hegemony. So transhumanism does not present a free environment in this futuristic society; instead it disguises the control mechanism through these technologies. The aim of this article is to explore how human beings are controlled by transhumanist technologies subtly under the name of providing super well-being.', '[Essay] The Algorithm; Mind of a Virtual Era – Our Code of Code. As soon as something implicit intrudes consciousness human thought undergoes a radical change. The introduction of any new tool or code brings a shift in cognition; every micro-step layering new semiotic forms within each macroevolutionary-stage has buttressed a new semantic leap. Our mechanization of everyday life and the tech-systems we interact with are impacting communication, cultural norms and values, market-aesthetics, and economics, in societies at large. Undergirded by a survey of the role and significance of tools in human evolution, this study arrives at what is already a well-entrenched new era: the digital, screen-mediated age. Revolutionized by the algorithm, introduced by computers, this age is dominated by the addictive quality of instant contact, unlimited information, virtual gaming, and titillating service-forms, all at our finger tips. Aside from the interpersonal impact on the new humans growing up with devices in hand, how does this disembodied, digital code-form through which our interactions are mediated condition human cognition? How does its seductive efficiency interfere with how we relate, feel, assign meanings, think? Rooted in Code Biology macro-evolutionary and psychoanalytic principles, this paper examines the algorithm itself and takes a sweeping interdisciplinary approach to the developmental, psychosocial, and cognitive implications for the human mind/brain as it interacts with its technological extension.', 'Composing Worlds: A Portuguese Transdisciplinary Network in Humanities, Health and Well-Being. The project “Composing worlds: humanities, health and well-being in the 21st century” aims to build a network of experts in the humanities, social and health sciences, who think about health and well-being in contemporary technological societies. The relevance of this project is based on the growing evidence that most of the problems that the 21st century will face, particularly in the area of health and well-being, relate to the way in which humans connect to the environment, to non-human beings, to different cultures and to technologies. Its main goal is to bring out personal and well-founded ideas on these issues and to reflect on how the humanities may help with difficult environmental, social and technological issues. The methodology used in the first phase of the project consists of an open answer interview, built in a participatory way by the network of experts, and of a thematic analysis of the answers. It is an exploratory research project, which uses thematic analysis to identify the key ideas of each author, and to induce the corresponding main themes. The themes are then organized by semantic correspondence into thematic clusters. The thematic axes are abstracted from these clusters, and they constitute the vectors to be developed in the second phase of the project, by proposing their integration into university curricula, research and intervention of social, cultural and community outreach. Some of these developments are already in place.', 'A Pharmacological Perspective on Technology-Induced Organised Immaturity: The Care-giving Role of the Arts. Digital technologies induce organised immaturity by generating toxic sociotechnical conditions that lead us to delegate autonomous, individual, and responsible thoughts and actions to external technological systems. Aiming to move beyond a diagnostic critical reading of the toxicity of digitalisation, we bring Bernard Stiegler’s pharmacological analysis of technology into dialogue with the ethics of care to speculatively explore how the socially engaged arts—a type of artistic practice emphasising audience co-production and processual collective responses to social challenges—play a care-giving role that helps counter technology-induced organised immaturity. We outline and illustrate two modes by which the socially engaged arts play this role: 1) disorganising immaturity through artivism, most notably anti-surveillance art, that imparts savoir vivre, that is, shared knowledge and meaning to counter the toxic side of technologies while enabling the imagination of alternative worlds in which humans coexist harmoniously with digital technologies, and 2) organising maturity through arts-based hacking that imparts savoir faire, that is, hands-on knowledge for experimental creation and practical enactment of better technological worlds.', \"Human Pacemakers and Experiential Reading. This paper applies an embodied perspective to the study of reading and has a two-fold aim: (i) to discuss how reading is best understood in terms of cultural-cognitive performance that involves living bodies who actively engage with reading materials, and (ii) to spark a dialogue with neighboring disciplines, such as multimodality studies and movement studies, which likewise pivot on how practices and performances involve moving bodies: life is something we do . An embodied cognitive perspective considers how performance is constrained by and draws on expertise such as lived experience as well as the material affordances available in the situation. Such a perspective is crucial for reading research as this domain has been, and largely still is, dominated by the view that reading is a silent, disembodied activity that takes place in the reader's brain by means of neural mechanisms. However, recent studies of reading practices are starting to develop new explanations emphasizing the multimodal engagement in reading as crucial for managing the activity. While this perspective is still empirically underexplored, we seek to highlight how reading is managed by readers' dynamic, embodied engagement with the material. We call this engagement cognitive pacemaking , an action-perception phenomenon we argue should be considered as the key mechanism for controlling attention. We present here a framework to understand reading in terms of pacemaking by emphasizing attentional shifts constituted by embodied modulations of lived temporality. Methodologically, we combine a close reading of a classic literary text, with the focus on attentional modulation with a qualitative study of university students reading different short texts. We highlight how meaning emerges not primarily from linguistic decoding and comprehension, but also from cognitive-cultural, multimodal engagement with the text. Finally, we conclude that empirical reading research should focus on how embodied reading differs across contexts, genres, media and personalities to better scaffold and design reading settings in accordance with those aspects.\", 'Enacting Media. An Embodied Account of Enculturation Between Neuromediality and New Cognitive Media Theory. This paper argues that the still-emerging paradigm of situated cognition requires a more systematic perspective on media to capture the enculturation of the human mind. By virtue of being media, cultural artifacts present central experiential models of the world for our embodied minds to latch onto. The paper identifies references to external media within embodied, extended, enactive , and predictive approaches to cognition, which remain underdeveloped in terms of the profound impact that media have on our mind. To grasp this impact, I propose an enactive account of media that is based on expansive habits as media-structured, embodied ways of bringing forth meaning and new domains of values. We apply such habits, for instance, when seeing a picture or perceiving a movie. They become established through a process of reciprocal adaptation between media artifacts and organisms and define the range of viable actions within such a media ecology. Within an artifactual habit, we then become attuned to a specific media work (e.g., a TV series, a picture, a text, or even a city) that engages us. Both the plurality of habits and the dynamical adjustments within a habit require a more flexible neural architecture than is addressed by classical cognitive neuroscience. To detail how neural and media processes interlock, I will introduce the concept of neuromediality and discuss radical predictive processing accounts that could contribute to the externalization of the mind by treating media themselves as generative models of the world. After a short primer on general media theory, I discuss media examples in three domains: pictures and moving images; digital media; architecture and the built environment. This discussion demonstrates the need for a new cognitive media theory based on enactive artifactual habits—one that will help us gain perspective on the continuous re-mediation of our mind.']\n",
      "2023-11-29 15:04:27,431 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-11-29 15:04:27,439 - root - INFO - Cluster 11: ['Techne in Affective Posthumanism and AI Artefacts: More (or Less) than Human?. In affective neuroscience, constructivist models are acutely influenced by the modern technological evolution, which underwrites an ongoing epistemological substitution of techne for episteme. Evidenced symptomatically in the influence of artificial intelligence (AI), affective artefacts, these models inform an ontological incursion of techne seen to coincide with posthumanist aspirations and anthropology. It is from the perspective of this neuroscientific techne that posthumanism views the human being as increasingly ill adapted to the modern technological civilization, which, conversely, is understood to require a technical governance of the sort envisioned through AI. Among the projects thought necessary for implementing this framework is a recasting of the human emotional spectrum. Revealed through its techne recasting, however, are explanatory commitments to a metaphysic of extrinsic and contiguous causes, where malleability is ontologically constitutive. Aligned with posthumanist assertions malleability is invoked to argue for a rapid advance of the human form, normatively driven by enlightenment ideals. The ontological claim, however, dispenses with the stability of an a priori, intersubjective and interrelational metaphysical form that undergirds the emotions, leading to the collapse of a definitional anthropos. This paper will argue that techne models of the emotions selectively endorse philosophy of science commitments, thereby introducing a normative inversion that deconstructs the notion of anthropology pursued in posthumanist aspirations.', 'Placebo From an Enactive Perspective. Due to their complexity and variability, placebo effects remain controversial. We suggest this is also due to a set of problematic assumptions (dualism, reductionism, individualism, passivity). We critically assess current explanations and empirical evidence and propose an alternative theoretical framework—the enactive approach to life and mind—based on recent developments in embodied cognitive science. We review core enactive concepts such as autonomy, agency, and sense-making. Following these ideas, we propose a move from binary distinctions (e.g., conscious vs. non-conscious) to the more workable categories of reflective and pre-reflective activity. We introduce an ontology of individuation, following the work of Gilbert Simondon, that allow us to see placebo interventions not as originating causal chains, but as modulators and triggers in the regulation of tensions between ongoing embodied and interpersonal processes. We describe these interrelated processes involving looping effects through three intertwined dimensions of embodiment: organic, sensorimotor, and intersubjective. Finally, we defend the need to investigate therapeutic interactions in terms of participatory sense-making, going beyond the identification of individual social traits (e.g., empathy, trust) that contribute to placebo effects. We discuss resonances and differences between the enactive proposal, popular explanations such as expectations and conditioning, and other approaches based on meaning responses and phenomenological/ecological ideas.', 'Adapting Ourselves, Instead of the Environment: An Inquiry into Human Enhancement for Function and Beyond. Abstract Technology enables humans not only to adapt their environment to their needs but also to modify themselves. Means of Human Enhancement — embodied technologies to improve the human body’s capabilities or to create a new one — are the designated means of adapting ourselves instead of the environment. The debate about these technologies is typically fought on ethical soil. However, alarmist, utopian, and science fiction scenarios distract from the fact that Human Enhancement is a historical and pervasive phenomenon incorporated into many everyday practices. In the vein of disentangling conceptual difficulties, we claim that means of Human Enhancement are either physiologically or psychologically embodied, rendering the merging with the human user their most defining aspect. To fulfill its purpose, an enhancement must pass the test-in-the-world, i.e., assisting with effective engagement with a dynamic world. Even if failing in this regard: Human Enhancement is the fundamental and semi-targeted process of changing the users relationship with the world through the physical or psychological embodiment of a hitherto external object and/or change of one’s body. This can potentially change the notion of being human. Drawing on a rich body of theoretical and empirical literature, we aim to provide a nuanced analysis of the transformative nature of this phenomenon in close proximity to human practice. Stakeholders are invited to apply the theory presented here to interrogate their perspective on technology in general and Human Enhancement in particular.', 'Narrative responsibility and artificial intelligence. Abstract Most accounts of responsibility focus on one type of responsibility, moral responsibility, or address one particular aspect of moral responsibility such as agency. This article outlines a broader framework to think about responsibility that includes causal responsibility, relational responsibility, and what I call “narrative responsibility” as a form of “hermeneutic responsibility”, connects these notions of responsibility with different kinds of knowledge, disciplines, and perspectives on human being, and shows how this framework is helpful for mapping and analysing how artificial intelligence (AI) challenges human responsibility and sense-making in various ways. Mobilizing recent hermeneutic approaches to technology, the article argues that next to, and interwoven with, other types of responsibility such as moral responsibility, we also have narrative and hermeneutic responsibility—in general and for technology. For example, it is our task as humans to make sense of, with and, if necessary, against AI. While from a posthumanist point of view, technologies also contribute to sense-making, humans are the experiencers and bearers of responsibility and always remain in charge when it comes to this hermeneutic responsibility. Facing and working with a world of data, correlations, and probabilities, we are nevertheless condemned to make sense. Moreover, this also has a normative, sometimes even political aspect: acknowledging and embracing our hermeneutic responsibility is important if we want to avoid that our stories are written elsewhere—through technology.', 'Spike Jonze’s Her: How Transhumanism Turns into a Control Mechanism Under the Name of Love. Transhumanism is a philosophy based on the idea of enhancing the physical, intellectual and psychological capacity of human beings through the direct use of science and technology. Because transhumanism aims to ensure super longevity, super intelligence and super well-being, it is said that it will provide a more liberating atmosphere for human beings by elevating the current human condition. In Spike Jonze’s film her (2013), transhumanist technologies seem to present a peaceful society by focusing on the inhabitants’ well-being. However, in this futuristic society where advanced AI technology and personal letter writing service are widespread, human beings are over-controlled in a subtle way by manufacturing consent as Antonio Gramsci uses to explain the concept of hegemony. So transhumanism does not present a free environment in this futuristic society; instead it disguises the control mechanism through these technologies. The aim of this article is to explore how human beings are controlled by transhumanist technologies subtly under the name of providing super well-being.', '[Essay] The Algorithm; Mind of a Virtual Era – Our Code of Code. As soon as something implicit intrudes consciousness human thought undergoes a radical change. The introduction of any new tool or code brings a shift in cognition; every micro-step layering new semiotic forms within each macroevolutionary-stage has buttressed a new semantic leap. Our mechanization of everyday life and the tech-systems we interact with are impacting communication, cultural norms and values, market-aesthetics, and economics, in societies at large. Undergirded by a survey of the role and significance of tools in human evolution, this study arrives at what is already a well-entrenched new era: the digital, screen-mediated age. Revolutionized by the algorithm, introduced by computers, this age is dominated by the addictive quality of instant contact, unlimited information, virtual gaming, and titillating service-forms, all at our finger tips. Aside from the interpersonal impact on the new humans growing up with devices in hand, how does this disembodied, digital code-form through which our interactions are mediated condition human cognition? How does its seductive efficiency interfere with how we relate, feel, assign meanings, think? Rooted in Code Biology macro-evolutionary and psychoanalytic principles, this paper examines the algorithm itself and takes a sweeping interdisciplinary approach to the developmental, psychosocial, and cognitive implications for the human mind/brain as it interacts with its technological extension.', 'On the Contribution of Neuroethics to the Ethics and Regulation of Artificial Intelligence. Abstract Contemporary ethical analysis of Artificial Intelligence (AI) is growing rapidly. One of its most recognizable outcomes is the publication of a number of ethics guidelines that, intended to guide governmental policy, address issues raised by AI design, development, and implementation and generally present a set of recommendations. Here we propose two things: first, regarding content, since some of the applied issues raised by AI are related to fundamental questions about topics like intelligence, consciousness, and the ontological and ethical status of humans, among others, the treatment of these issues would benefit from interfacing with neuroethics that has been addressing those same issues in the context of brain research. Second, the identification and management of some of the practical ethical challenges raised by AI would be enriched by embracing the methodological resources used in neuroethics. In particular, we focus on the methodological distinction between conceptual and action-oriented neuroethical approaches. We argue that the normative (often principles-oriented) discussion about AI will benefit from further integration of conceptual analysis, including analysis of some operative assumptions, their meaning in different contexts, and their mutual relevance in order to avoid misplaced or disproportionate concerns and achieve a more realistic and useful approach to identifying and managing the emerging ethical issues.', 'A Pharmacological Perspective on Technology-Induced Organised Immaturity: The Care-giving Role of the Arts. Digital technologies induce organised immaturity by generating toxic sociotechnical conditions that lead us to delegate autonomous, individual, and responsible thoughts and actions to external technological systems. Aiming to move beyond a diagnostic critical reading of the toxicity of digitalisation, we bring Bernard Stiegler’s pharmacological analysis of technology into dialogue with the ethics of care to speculatively explore how the socially engaged arts—a type of artistic practice emphasising audience co-production and processual collective responses to social challenges—play a care-giving role that helps counter technology-induced organised immaturity. We outline and illustrate two modes by which the socially engaged arts play this role: 1) disorganising immaturity through artivism, most notably anti-surveillance art, that imparts savoir vivre, that is, shared knowledge and meaning to counter the toxic side of technologies while enabling the imagination of alternative worlds in which humans coexist harmoniously with digital technologies, and 2) organising maturity through arts-based hacking that imparts savoir faire, that is, hands-on knowledge for experimental creation and practical enactment of better technological worlds.', 'AI: A Semiotic Perspective. Abstract Artificial Intelligence (AI) has become a powerful new form of inquiry unto human cognition that has obvious implications for semiotic theories, practices, and modeling of mind, yet, as far as can be determined, it has hardly attracted the attention of semioticians in any meaningful analytical way. AI aims to model and thus penetrate mentality in all its forms (perception, cognition, emotion, etc.) and even to build artificial minds that will surpass human intelligence in the near future. This paper takes a look at AI through the lens of semiotic analysis, in the context of current philosophies such as posthumanism and transhumanism, which are based on the assumption that technology will improve the human condition and chart a path to the future progress of the human species. Semiotics must respond to the AI challenge, focusing on how abductive responses to the world generate meaning in the human sense, not in software or algorithms. The AI approach is instructive, but semiotics is much more relevant to the understanding of human cognition, because it studies signs as paths into the brain, not artificial models of that organ. The semiotic agenda can enrich AI by providing the relevant insight into human semiosis that may defy any attempt to model them.', 'Learning rules. Why we learn, what we learn, what the conditions are under which we learn, how the learning happens, and what the answers to these questions imply about how we should teach are the topics of this urgent, compelling, and highly multidisciplinary book by Stanislas Dehaene. He is one of the world’s most storied and celebrated cognitive scientists and cognitive neuroscientists. Along the way, we get to enjoy some mental gymnastics concerning the joint roles of nature and nurture, a wealth of revealing and sometimes heart-wrenching individual examples, and a glimpse into the potential potency but current impoverishment of machine learning systems. As ever, a good starting point for decomposing the problems of learning is to consider Marr’s levels of analysis [1Marr D. Vision: A Computational Investigation into the Human Representation and Processing of Visual Information. W.H. Freeman, 1982Google Scholar]: computational questions of the goal of learning, tied to ethological concerns as to how it helps us fit our environmental niche; algorithmic questions of the effective procedures and representations associated with learning, tied to psychological issues of the processes concerned during infancy, childhood, adulthood, and dotage; and implementational questions of the mapping of all these onto developmental change and mature plasticity in the brain that are directly the concern of a near-overwhelming volume of research in neuroscience. In these terms, at the heart of the book lies the concrete issues of the computational and algorithmic scaffolding necessary to get learning off the ground and operating smoothly, and the educational environments that we should build to facilitate this. The discussion is attractively built around information processing issues rather than traditional psychological dichotomies, such as declarative and procedural or episodic and semantic memory [2Gluck M.A. Mercado E. Myers C.E. Learning and Memory: From Brain to Behavior.3rd Edition. Worth, 2016Google Scholar]. ‘If’ we have to learn a huge amount about ourselves, others, and the environment we occupy, all three of which are hugely complicated and labile, then we face various computational challenges. These notably include statistical efficiency (we cannot afford the time and effort it takes to get the huge numbers of examples that would seem necessary given the vast dimensionality of the input), active sampling (we often have to get the information we need for learning for ourselves; a common metaphor in the field is the child as a curious, naive scientist), and path dependence (e.g. the way that knowledge builds up in layers). A host of approaches and assumptions helps address these challenges: for instance, that social teaching is benign, that we have a sufficient starting point of knowledge to define the known unknowns and unknown unknowns created by ignorance and change that should be the targets of active sampling, and that the statistical and control-theoretic structures of the environments adequately coincide, so that understanding and maximizing reward/minimizing punishment are suitably aligned. We also enjoy many algorithmic responses to the challenges: everything from selective attention, through an internal metacognitive sense of confidence and error that can be used to guide learning [3Guggenmos M. Wilbertz G. Hebart M.N. Sterzer P. Mesolimbic confidence signals guide perceptual learning in the absence of external feedback.eLife. 2016; 5: e13388Crossref PubMed Scopus (50) Google Scholar], to the ability to perform offline adaptation, i.e. taking advantage of periods of quiet wakefulness and sleep to organize and reorganize information acquired during active engagement with the environment to exploit new information to the full. Neural aspects of these algorithms are well covered, often using examples from Dehaene’s own extensive and very revealing bodies of studies. The book makes the particular clarion call that pedagogical systems and structures should be adjusted to maximize the effectiveness of these algorithmic components — indeed, it is to Dehaene’s great credit that he is so actively engaged in educational policy, as the head of the first scientific council for education in France. One particular policy suggestion is to focus more on the commonality of these components: that we should not be overly attentive to individual differences in learning styles. In keeping with this, the book’s examples in general stress more the apparently normal outcomes of abnormal circumstances than the equally compelling abnormal outcomes of apparently normal circumstances as seen, for instance, in Luria’s mnemonist [4Luria A.R. The Mind of a Mnemonist: A Little Book about a Vast Memory. Harvard University Press, Cambridge, MA, and London1987Google Scholar]. Indeed, forgetting, which was one of the main concerns of the mnemonist, seems relatively forgotten. These algorithmic ‘pillars’ rest on a more abstract theoretical foundation at the computational level that might seem more strikingly surprising. The book is much enamored of a “new vision of the brain [as] an immense generative model, massively structured and capable of producing myriad hypothetical rules and structures — but which gradually restricts itself to those that fit with reality” [5Lake B.M. Salakhutdinov R. Tenenbaum J.B. Human-level concept learning through probabilistic program induction.Science. 2015; 350: 1332-1338Crossref PubMed Scopus (965) Google Scholar, 6Lake B.M. Ullman T.D. Tenenbaum J.B. Gershman S.J. Building machines that learn and think like people.Behav. Brain Sci. 2017; 40: e253Crossref PubMed Scopus (495) Google Scholar, 7Tenenbaum J.B. Kemp C. Griffiths T.L. Goodman N.D. How to grow a mind: statistics, structure, and abstraction.Science. 2011; 331: 1279-1285Crossref PubMed Scopus (739) Google Scholar], the gradual restriction to mundane reality being the role of development and learning. This Bayesian (or for loyalists, Laplacian) sort of restrictivism should be contrasted with a constructivism (e.g. [8Quartz S. Sejnowski T.J. The neural basis of cognitive development: a constructivist manifesto.Behav. Brain Sci. 1997; 20: 537-596Crossref PubMed Scopus (470) Google Scholar]) that is normally implied by the progressive layering of knowledge mentioned above, and that is itself an attempt to avoid the philosophical reductio ad absurdum (associated with the scare quotes around the ‘if’ in the fourth paragraph of this review) that we come to the world in some sense with a knowledge of buses, nuclear power, and the social rules of book reviewing. The most advanced of approaches to these structured generative models, involving such things as the automatic induction of programs that perform probabilistic information processing, promise tantalizing reconciliations, and are still at the forefront of current research. Fortunately, that the book nails its colors firmly to a restrictivist mast should not perturb readers of less Laplacian or more constructivist dispositions, given that the implications for the concrete computational and algorithmic scaffolding are relatively modest. Perhaps less fortunate is that this very modesty means that there can only be limited connections between the book’s overall computational and implementational themes. The book is published at a time when many of us are wondering about the date at which our artificially intelligent devices will be lording it over us. It nicely points out that, although this prospect has arisen because of the capacity of modern ‘deep’ artificial neural networks (rather than automatically induced probabilistic programs) to learn, the way that these nets learn is currently far removed and generally far less statistically efficient than the way that we do so. One part of the difference is that nets use their vast training sets to recapitulate through learning what we are endowed with at birth by evolution. Of course, through a computational implementation of a Baldwin effect, this might ultimately be hardwired at the equivalent ‘birth’ of the nets. However, our abilities at learning from limited samples, at addressing the eternal dilemma between stability and plasticity [9Carpenter G.A. Grossberg S. The ART of adaptive pattern recognition by a self-organizing neural network.Computer. 1988; 21: 77-88Crossref Scopus (817) Google Scholar], at adjusting appropriately to the volatility of the environment, and a host of other computational and algorithmic capacities might keep us on top for a while longer. It would have been good to have had more discussion of the converse, i.e. an examination of the progressively changing and enriching neural representations over the course of learning that one might be able to conduct more precisely in artificial neural nets. The book is perhaps a little overly sanguine at the idea that the nets can only replace the sort of non-reflective calculations that correspond to the first 200−300 ms of unconscious processing by us; AlphaGo’s famous victory over the former world champion Lee Sedol in the game of Go shows that this can be perfectly ample when coupled to a modest reasoning capacity. Indeed, distillation, which is one computational equivalent of the sort of consolidation that happens in quiet wakefulness and sleep in us, could ultimately replace even this, given an adequately sized net. Finally, the book is a bit relentlessly anthropocentric, perhaps stemming from Dehaene’s ultimate interest in pedagogy — something whose existence outside human culture the book sternly questions. However, the key algorithmic and implementational processes and mechanisms — including the identified pillars of learning — are shared with many other mammals (and at least some birds), as even more generally is the need and capacity for adaptation to adjust successfully to the exigencies of a nasty, brutish, and changing environment. In sum, this engaging book teaches us much about learning, while also showing how much more about learning there is to learn. One can only sympathize with the author as he details all the better ways that there would be to go about teaching us than using just the pages of a book. But after reading it, you will certainly feel more learned.']\n",
      "2023-11-29 15:04:31,750 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-11-29 15:04:31,758 - root - INFO - Cluster 12: ['Placebo From an Enactive Perspective. Due to their complexity and variability, placebo effects remain controversial. We suggest this is also due to a set of problematic assumptions (dualism, reductionism, individualism, passivity). We critically assess current explanations and empirical evidence and propose an alternative theoretical framework—the enactive approach to life and mind—based on recent developments in embodied cognitive science. We review core enactive concepts such as autonomy, agency, and sense-making. Following these ideas, we propose a move from binary distinctions (e.g., conscious vs. non-conscious) to the more workable categories of reflective and pre-reflective activity. We introduce an ontology of individuation, following the work of Gilbert Simondon, that allow us to see placebo interventions not as originating causal chains, but as modulators and triggers in the regulation of tensions between ongoing embodied and interpersonal processes. We describe these interrelated processes involving looping effects through three intertwined dimensions of embodiment: organic, sensorimotor, and intersubjective. Finally, we defend the need to investigate therapeutic interactions in terms of participatory sense-making, going beyond the identification of individual social traits (e.g., empathy, trust) that contribute to placebo effects. We discuss resonances and differences between the enactive proposal, popular explanations such as expectations and conditioning, and other approaches based on meaning responses and phenomenological/ecological ideas.', 'Adapting Ourselves, Instead of the Environment: An Inquiry into Human Enhancement for Function and Beyond. Abstract Technology enables humans not only to adapt their environment to their needs but also to modify themselves. Means of Human Enhancement — embodied technologies to improve the human body’s capabilities or to create a new one — are the designated means of adapting ourselves instead of the environment. The debate about these technologies is typically fought on ethical soil. However, alarmist, utopian, and science fiction scenarios distract from the fact that Human Enhancement is a historical and pervasive phenomenon incorporated into many everyday practices. In the vein of disentangling conceptual difficulties, we claim that means of Human Enhancement are either physiologically or psychologically embodied, rendering the merging with the human user their most defining aspect. To fulfill its purpose, an enhancement must pass the test-in-the-world, i.e., assisting with effective engagement with a dynamic world. Even if failing in this regard: Human Enhancement is the fundamental and semi-targeted process of changing the users relationship with the world through the physical or psychological embodiment of a hitherto external object and/or change of one’s body. This can potentially change the notion of being human. Drawing on a rich body of theoretical and empirical literature, we aim to provide a nuanced analysis of the transformative nature of this phenomenon in close proximity to human practice. Stakeholders are invited to apply the theory presented here to interrogate their perspective on technology in general and Human Enhancement in particular.', 'Narrative responsibility and artificial intelligence. Abstract Most accounts of responsibility focus on one type of responsibility, moral responsibility, or address one particular aspect of moral responsibility such as agency. This article outlines a broader framework to think about responsibility that includes causal responsibility, relational responsibility, and what I call “narrative responsibility” as a form of “hermeneutic responsibility”, connects these notions of responsibility with different kinds of knowledge, disciplines, and perspectives on human being, and shows how this framework is helpful for mapping and analysing how artificial intelligence (AI) challenges human responsibility and sense-making in various ways. Mobilizing recent hermeneutic approaches to technology, the article argues that next to, and interwoven with, other types of responsibility such as moral responsibility, we also have narrative and hermeneutic responsibility—in general and for technology. For example, it is our task as humans to make sense of, with and, if necessary, against AI. While from a posthumanist point of view, technologies also contribute to sense-making, humans are the experiencers and bearers of responsibility and always remain in charge when it comes to this hermeneutic responsibility. Facing and working with a world of data, correlations, and probabilities, we are nevertheless condemned to make sense. Moreover, this also has a normative, sometimes even political aspect: acknowledging and embracing our hermeneutic responsibility is important if we want to avoid that our stories are written elsewhere—through technology.', 'Techne in Affective Posthumanism and AI Artefacts: More (or Less) than Human?. In affective neuroscience, constructivist models are acutely influenced by the modern technological evolution, which underwrites an ongoing epistemological substitution of techne for episteme. Evidenced symptomatically in the influence of artificial intelligence (AI), affective artefacts, these models inform an ontological incursion of techne seen to coincide with posthumanist aspirations and anthropology. It is from the perspective of this neuroscientific techne that posthumanism views the human being as increasingly ill adapted to the modern technological civilization, which, conversely, is understood to require a technical governance of the sort envisioned through AI. Among the projects thought necessary for implementing this framework is a recasting of the human emotional spectrum. Revealed through its techne recasting, however, are explanatory commitments to a metaphysic of extrinsic and contiguous causes, where malleability is ontologically constitutive. Aligned with posthumanist assertions malleability is invoked to argue for a rapid advance of the human form, normatively driven by enlightenment ideals. The ontological claim, however, dispenses with the stability of an a priori, intersubjective and interrelational metaphysical form that undergirds the emotions, leading to the collapse of a definitional anthropos. This paper will argue that techne models of the emotions selectively endorse philosophy of science commitments, thereby introducing a normative inversion that deconstructs the notion of anthropology pursued in posthumanist aspirations.', \"Human Pacemakers and Experiential Reading. This paper applies an embodied perspective to the study of reading and has a two-fold aim: (i) to discuss how reading is best understood in terms of cultural-cognitive performance that involves living bodies who actively engage with reading materials, and (ii) to spark a dialogue with neighboring disciplines, such as multimodality studies and movement studies, which likewise pivot on how practices and performances involve moving bodies: life is something we do . An embodied cognitive perspective considers how performance is constrained by and draws on expertise such as lived experience as well as the material affordances available in the situation. Such a perspective is crucial for reading research as this domain has been, and largely still is, dominated by the view that reading is a silent, disembodied activity that takes place in the reader's brain by means of neural mechanisms. However, recent studies of reading practices are starting to develop new explanations emphasizing the multimodal engagement in reading as crucial for managing the activity. While this perspective is still empirically underexplored, we seek to highlight how reading is managed by readers' dynamic, embodied engagement with the material. We call this engagement cognitive pacemaking , an action-perception phenomenon we argue should be considered as the key mechanism for controlling attention. We present here a framework to understand reading in terms of pacemaking by emphasizing attentional shifts constituted by embodied modulations of lived temporality. Methodologically, we combine a close reading of a classic literary text, with the focus on attentional modulation with a qualitative study of university students reading different short texts. We highlight how meaning emerges not primarily from linguistic decoding and comprehension, but also from cognitive-cultural, multimodal engagement with the text. Finally, we conclude that empirical reading research should focus on how embodied reading differs across contexts, genres, media and personalities to better scaffold and design reading settings in accordance with those aspects.\", '[Essay] The Algorithm; Mind of a Virtual Era – Our Code of Code. As soon as something implicit intrudes consciousness human thought undergoes a radical change. The introduction of any new tool or code brings a shift in cognition; every micro-step layering new semiotic forms within each macroevolutionary-stage has buttressed a new semantic leap. Our mechanization of everyday life and the tech-systems we interact with are impacting communication, cultural norms and values, market-aesthetics, and economics, in societies at large. Undergirded by a survey of the role and significance of tools in human evolution, this study arrives at what is already a well-entrenched new era: the digital, screen-mediated age. Revolutionized by the algorithm, introduced by computers, this age is dominated by the addictive quality of instant contact, unlimited information, virtual gaming, and titillating service-forms, all at our finger tips. Aside from the interpersonal impact on the new humans growing up with devices in hand, how does this disembodied, digital code-form through which our interactions are mediated condition human cognition? How does its seductive efficiency interfere with how we relate, feel, assign meanings, think? Rooted in Code Biology macro-evolutionary and psychoanalytic principles, this paper examines the algorithm itself and takes a sweeping interdisciplinary approach to the developmental, psychosocial, and cognitive implications for the human mind/brain as it interacts with its technological extension.', 'The development of categorisation and conceptual thinking in early childhood: methods and limitations. Abstract We present a systematic and qualitative review of academic literature on early conceptual development (0–24 months of age), with an emphasis on methodological aspects. The final sample of our review included 281 studies reported in 115 articles. The main aims of the article were four: first, to organise studies into sets according to methodological similarities and differences; second, to elaborate on the methodological procedures that characterise each set; third, to circumscribe the empirical indicators that different sets of studies consider as proof of the existence of concepts in early childhood; last, to identify methodological limitations and to propose possible ways to overcome them. We grouped the studies into five sets: preference and habituation experiments , category extension tasks , object sorting tasks , sequential touching tasks and object examination tasks . In the “Results” section, we review the core features of each set of studies. In the “Discussion” and “Conclusions” sections, we describe, for one thing, the most relevant methodological shortcomings. We end by arguing that a situated, semiotic and pragmatic perspective that emphasises the importance of ecological validity could open up new avenues of research to better understand the development of concepts in early childhood.', 'Falsified Incompetence and Other Lies the Positivists Told Me. &#x0D; &#x0D; &#x0D; Facilitated Communication (FC) is a technique of supported communication for non- speaking people with motor movements commonly understood as spasmodic, dyspraxic, or otherwise unruly. FC is a contentious site of scientific conflict where highly circumscribed quantitative experiments have been unable to reckon with the lived reality of typers. The debate over the efficacy of FC centers around broader arguments of what counts as scientific rigor and validity. In this paper, I remind readers that experiential data is, in fact, empirical. Qualitative analysis is scientifically rigorous. Adopting technologies of analysis from Chela Sandoval’s “Methodology of the Oppressed,” I explore a rhetorics of evacuation deployed by skeptics that result in the erasure of FC user agency, testimony, and experience. I invite readers to explore how these rhetorics extend beyond FC and into the wider field of education research.&#x0D; &#x0D; &#x0D;', 'Organizational Environment and Innovation. There is a popular notion that the life of innovators and inventors is a glorious and enviable one because they have talents which others do not have, and because they can overcome any obstacles. These talents are, in the popular interpretation, something mysterious, indefinable and inimitable. But in reality, the innovators and inventors have to deal with many serious and pernicious obstacles which are unknown to other people. Those who are unaware of the obstacles must become aware of them and must reduce them, because the reduction of these obstacles will greatly facilitate the works of innovators and inventors. Another problem is the assumption that all people use the same logic, and therefore if you explain hard enough, you will be understood. Existence of heterogeneity of individual logical types (perceptual/cognitive/cogitative action types, abbreviated as mindscape types) is ignored, or at least exoticated as irrelevant. This article discusses these problems, with concrete experiential examples. This article also uses the methodology of raw-experience visualization-enabling communication (REVEC) with which the readers can formulate their own grounded theories independent from the theory of the writer.', 'Beyond Playing 20 Questions with Nature: Integrative Experiment Design in the Social and Behavioral Sciences. The dominant paradigm of experiments in the social and behavioral sciences views an experiment as a test of a theory, where the theory is assumed to generalize beyond the experiment’s specific conditions. According to this view, which Alan Newell once characterized as “playing twenty questions with nature,” theory is advanced one experiment at a time, and the integration of disparate findings is assumed to happen via the scientific publishing process. In this article, we argue that the process of integration is at best inefficient, and at worst it does not, in fact, occur. We further show that the challenge of integration cannot be adequately addressed by recently proposed reforms that focus on the reliability and replicability of individual findings, nor simply by conducting more or larger experiments. Rather, the problem arises from the imprecise nature of social and behavioral theories and, consequently, a lack of commensurability across experiments conducted under different conditions. Therefore, researchers must fundamentally rethink how they design experiments and how the experiments relate to theory. We specifically describe an alternative framework, integrative experiment design, which intrinsically promotes commensurability and continuous integration of knowledge. In this paradigm, researchers explicitly map the design space of possible experiments associated with a given research question, embracing many potentially relevant theories rather than focusing on just one. The researchers then iteratively generate theories and test them with experiments explicitly sampled from the design space, allowing results to be integrated across experiments. Given recent methodological and technological developments, we conclude that this approach is feasible and would generate more-reliable, more-cumulative empirical and theoretical knowledge than the current paradigm—and with far greater efficiency.']\n",
      "2023-11-29 15:04:34,759 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-11-29 15:04:34,767 - root - INFO - Cluster 13: ['Narrative responsibility and artificial intelligence. Abstract Most accounts of responsibility focus on one type of responsibility, moral responsibility, or address one particular aspect of moral responsibility such as agency. This article outlines a broader framework to think about responsibility that includes causal responsibility, relational responsibility, and what I call “narrative responsibility” as a form of “hermeneutic responsibility”, connects these notions of responsibility with different kinds of knowledge, disciplines, and perspectives on human being, and shows how this framework is helpful for mapping and analysing how artificial intelligence (AI) challenges human responsibility and sense-making in various ways. Mobilizing recent hermeneutic approaches to technology, the article argues that next to, and interwoven with, other types of responsibility such as moral responsibility, we also have narrative and hermeneutic responsibility—in general and for technology. For example, it is our task as humans to make sense of, with and, if necessary, against AI. While from a posthumanist point of view, technologies also contribute to sense-making, humans are the experiencers and bearers of responsibility and always remain in charge when it comes to this hermeneutic responsibility. Facing and working with a world of data, correlations, and probabilities, we are nevertheless condemned to make sense. Moreover, this also has a normative, sometimes even political aspect: acknowledging and embracing our hermeneutic responsibility is important if we want to avoid that our stories are written elsewhere—through technology.', 'Techne in Affective Posthumanism and AI Artefacts: More (or Less) than Human?. In affective neuroscience, constructivist models are acutely influenced by the modern technological evolution, which underwrites an ongoing epistemological substitution of techne for episteme. Evidenced symptomatically in the influence of artificial intelligence (AI), affective artefacts, these models inform an ontological incursion of techne seen to coincide with posthumanist aspirations and anthropology. It is from the perspective of this neuroscientific techne that posthumanism views the human being as increasingly ill adapted to the modern technological civilization, which, conversely, is understood to require a technical governance of the sort envisioned through AI. Among the projects thought necessary for implementing this framework is a recasting of the human emotional spectrum. Revealed through its techne recasting, however, are explanatory commitments to a metaphysic of extrinsic and contiguous causes, where malleability is ontologically constitutive. Aligned with posthumanist assertions malleability is invoked to argue for a rapid advance of the human form, normatively driven by enlightenment ideals. The ontological claim, however, dispenses with the stability of an a priori, intersubjective and interrelational metaphysical form that undergirds the emotions, leading to the collapse of a definitional anthropos. This paper will argue that techne models of the emotions selectively endorse philosophy of science commitments, thereby introducing a normative inversion that deconstructs the notion of anthropology pursued in posthumanist aspirations.', 'Adapting Ourselves, Instead of the Environment: An Inquiry into Human Enhancement for Function and Beyond. Abstract Technology enables humans not only to adapt their environment to their needs but also to modify themselves. Means of Human Enhancement — embodied technologies to improve the human body’s capabilities or to create a new one — are the designated means of adapting ourselves instead of the environment. The debate about these technologies is typically fought on ethical soil. However, alarmist, utopian, and science fiction scenarios distract from the fact that Human Enhancement is a historical and pervasive phenomenon incorporated into many everyday practices. In the vein of disentangling conceptual difficulties, we claim that means of Human Enhancement are either physiologically or psychologically embodied, rendering the merging with the human user their most defining aspect. To fulfill its purpose, an enhancement must pass the test-in-the-world, i.e., assisting with effective engagement with a dynamic world. Even if failing in this regard: Human Enhancement is the fundamental and semi-targeted process of changing the users relationship with the world through the physical or psychological embodiment of a hitherto external object and/or change of one’s body. This can potentially change the notion of being human. Drawing on a rich body of theoretical and empirical literature, we aim to provide a nuanced analysis of the transformative nature of this phenomenon in close proximity to human practice. Stakeholders are invited to apply the theory presented here to interrogate their perspective on technology in general and Human Enhancement in particular.', 'Placebo From an Enactive Perspective. Due to their complexity and variability, placebo effects remain controversial. We suggest this is also due to a set of problematic assumptions (dualism, reductionism, individualism, passivity). We critically assess current explanations and empirical evidence and propose an alternative theoretical framework—the enactive approach to life and mind—based on recent developments in embodied cognitive science. We review core enactive concepts such as autonomy, agency, and sense-making. Following these ideas, we propose a move from binary distinctions (e.g., conscious vs. non-conscious) to the more workable categories of reflective and pre-reflective activity. We introduce an ontology of individuation, following the work of Gilbert Simondon, that allow us to see placebo interventions not as originating causal chains, but as modulators and triggers in the regulation of tensions between ongoing embodied and interpersonal processes. We describe these interrelated processes involving looping effects through three intertwined dimensions of embodiment: organic, sensorimotor, and intersubjective. Finally, we defend the need to investigate therapeutic interactions in terms of participatory sense-making, going beyond the identification of individual social traits (e.g., empathy, trust) that contribute to placebo effects. We discuss resonances and differences between the enactive proposal, popular explanations such as expectations and conditioning, and other approaches based on meaning responses and phenomenological/ecological ideas.', 'Spike Jonze’s Her: How Transhumanism Turns into a Control Mechanism Under the Name of Love. Transhumanism is a philosophy based on the idea of enhancing the physical, intellectual and psychological capacity of human beings through the direct use of science and technology. Because transhumanism aims to ensure super longevity, super intelligence and super well-being, it is said that it will provide a more liberating atmosphere for human beings by elevating the current human condition. In Spike Jonze’s film her (2013), transhumanist technologies seem to present a peaceful society by focusing on the inhabitants’ well-being. However, in this futuristic society where advanced AI technology and personal letter writing service are widespread, human beings are over-controlled in a subtle way by manufacturing consent as Antonio Gramsci uses to explain the concept of hegemony. So transhumanism does not present a free environment in this futuristic society; instead it disguises the control mechanism through these technologies. The aim of this article is to explore how human beings are controlled by transhumanist technologies subtly under the name of providing super well-being.', 'Living Machines: Metaphors We Live By. Abstract Within biology and in society, living creatures have long been described using metaphors of machinery and computation: ‘bioengineering’, ‘genes as code’ or ‘biological chassis’. This paper builds on Lakoff and Johnson’s (1980) argument that such language mechanisms shape how we understand the world. I argue that the living machines metaphor builds upon a certain perception of life entailing an idea of radical human control of the living world, looking back at the historical preconditions for this metaphor. I discuss how design is perceived to enable us to shape natural beings to our will, and consider ethical, epistemological and ontological implications of the prevalence of this metaphor, focusing on its use within synthetic biology. I argue that we urgently need counter-images to the dominant metaphor of living machines and its implied control and propose that artworks can provide such counter-images through upsetting the perception of life as controllable. This is argued through discussion of artworks by Oron Catts and Ionat Zurr, by Tarsh Bates and by Ai Hasegawa, which in different ways challenge mechanistic assumptions through open-ended engagement with the strangeness and messiness of life.', '[Essay] The Algorithm; Mind of a Virtual Era – Our Code of Code. As soon as something implicit intrudes consciousness human thought undergoes a radical change. The introduction of any new tool or code brings a shift in cognition; every micro-step layering new semiotic forms within each macroevolutionary-stage has buttressed a new semantic leap. Our mechanization of everyday life and the tech-systems we interact with are impacting communication, cultural norms and values, market-aesthetics, and economics, in societies at large. Undergirded by a survey of the role and significance of tools in human evolution, this study arrives at what is already a well-entrenched new era: the digital, screen-mediated age. Revolutionized by the algorithm, introduced by computers, this age is dominated by the addictive quality of instant contact, unlimited information, virtual gaming, and titillating service-forms, all at our finger tips. Aside from the interpersonal impact on the new humans growing up with devices in hand, how does this disembodied, digital code-form through which our interactions are mediated condition human cognition? How does its seductive efficiency interfere with how we relate, feel, assign meanings, think? Rooted in Code Biology macro-evolutionary and psychoanalytic principles, this paper examines the algorithm itself and takes a sweeping interdisciplinary approach to the developmental, psychosocial, and cognitive implications for the human mind/brain as it interacts with its technological extension.', 'Virtual Environments for Research into Social Evolution (VERSE): A novel experimental environment for the study of human social learning. 1. Abstract Social learning (learning from others) can be a cost-effective way of gaining information compared to asocial (independent) learning. However, learning from others indiscriminately can lead to the acquisition of maladaptive behaviours or outdated information. Evolutionary theory therefore predicts that individuals will use social information adaptively through the use of ‘social learning strategies’. Restrictive laboratory conditions, however, make studying human learning strategies problematic. Abstract tasks, unrealistic sources of social information and methodologies that do not take into account the influence of physical location over large spaces make it difficult to ascertain if previous findings are representative of the way we would use social information in reality. Here I describe a novel platform for studying human social behaviour within immersive virtual environments: “Virtual Environments for Research into Social Evolution” (VERSE). Through the use of gaming technology, VERSE allows researchers to build realistic, three-dimensional, open world environments where participants can complete ecologically relevant tasks while actively observing computer-controlled artificial intelligence agents (AIs) that act as realistic yet controllable sources of social information. This methodological article begins by exploring what social learning strategies are and the problems with studying social learning behaviour in humans (compared to animal populations, for example). I then discuss how gaming technology can be used in behavioural research and follow on with a detailed account of the specific functionalities available in VERSE. I conclude with a worked example of how VERSE can be used to construct a novel behavioural experiment. Altogether, VERSE has great potential to give us insight into how human individuals learn within novel environments in a way that has never before been possible.', 'The Digital Subject: People as Data as Persons. This essay explores the return of the subject in the computational context, which I address as a digital subject. This digital subject encompasses a digital identifier, correlations in data or a data profile, moving between biological characteristics and symbolic expression. I focus on the processes through which digital subjects are constructed by matching, correlating, modelling, as well as how they become enactive. The ways of pulling data together into a digital subject is often presented as a logic of fact, where data is equated with documentary evidence. Instead, I propose the notion of the distance in which digital subjects are produced. Indexicality comes from outside of data, whereas the regard for the thick distance becomes a mark of the form of knowledge. I conclude by arguing for a posthumanities approach that establishes the distance while allowing for different subjects to be called upon.', 'A Pharmacological Perspective on Technology-Induced Organised Immaturity: The Care-giving Role of the Arts. Digital technologies induce organised immaturity by generating toxic sociotechnical conditions that lead us to delegate autonomous, individual, and responsible thoughts and actions to external technological systems. Aiming to move beyond a diagnostic critical reading of the toxicity of digitalisation, we bring Bernard Stiegler’s pharmacological analysis of technology into dialogue with the ethics of care to speculatively explore how the socially engaged arts—a type of artistic practice emphasising audience co-production and processual collective responses to social challenges—play a care-giving role that helps counter technology-induced organised immaturity. We outline and illustrate two modes by which the socially engaged arts play this role: 1) disorganising immaturity through artivism, most notably anti-surveillance art, that imparts savoir vivre, that is, shared knowledge and meaning to counter the toxic side of technologies while enabling the imagination of alternative worlds in which humans coexist harmoniously with digital technologies, and 2) organising maturity through arts-based hacking that imparts savoir faire, that is, hands-on knowledge for experimental creation and practical enactment of better technological worlds.']\n",
      "2023-11-29 15:04:38,449 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-11-29 15:04:38,456 - root - INFO - Cluster 14: ['Narrative responsibility and artificial intelligence. Abstract Most accounts of responsibility focus on one type of responsibility, moral responsibility, or address one particular aspect of moral responsibility such as agency. This article outlines a broader framework to think about responsibility that includes causal responsibility, relational responsibility, and what I call “narrative responsibility” as a form of “hermeneutic responsibility”, connects these notions of responsibility with different kinds of knowledge, disciplines, and perspectives on human being, and shows how this framework is helpful for mapping and analysing how artificial intelligence (AI) challenges human responsibility and sense-making in various ways. Mobilizing recent hermeneutic approaches to technology, the article argues that next to, and interwoven with, other types of responsibility such as moral responsibility, we also have narrative and hermeneutic responsibility—in general and for technology. For example, it is our task as humans to make sense of, with and, if necessary, against AI. While from a posthumanist point of view, technologies also contribute to sense-making, humans are the experiencers and bearers of responsibility and always remain in charge when it comes to this hermeneutic responsibility. Facing and working with a world of data, correlations, and probabilities, we are nevertheless condemned to make sense. Moreover, this also has a normative, sometimes even political aspect: acknowledging and embracing our hermeneutic responsibility is important if we want to avoid that our stories are written elsewhere—through technology.', 'Placebo From an Enactive Perspective. Due to their complexity and variability, placebo effects remain controversial. We suggest this is also due to a set of problematic assumptions (dualism, reductionism, individualism, passivity). We critically assess current explanations and empirical evidence and propose an alternative theoretical framework—the enactive approach to life and mind—based on recent developments in embodied cognitive science. We review core enactive concepts such as autonomy, agency, and sense-making. Following these ideas, we propose a move from binary distinctions (e.g., conscious vs. non-conscious) to the more workable categories of reflective and pre-reflective activity. We introduce an ontology of individuation, following the work of Gilbert Simondon, that allow us to see placebo interventions not as originating causal chains, but as modulators and triggers in the regulation of tensions between ongoing embodied and interpersonal processes. We describe these interrelated processes involving looping effects through three intertwined dimensions of embodiment: organic, sensorimotor, and intersubjective. Finally, we defend the need to investigate therapeutic interactions in terms of participatory sense-making, going beyond the identification of individual social traits (e.g., empathy, trust) that contribute to placebo effects. We discuss resonances and differences between the enactive proposal, popular explanations such as expectations and conditioning, and other approaches based on meaning responses and phenomenological/ecological ideas.', 'Adapting Ourselves, Instead of the Environment: An Inquiry into Human Enhancement for Function and Beyond. Abstract Technology enables humans not only to adapt their environment to their needs but also to modify themselves. Means of Human Enhancement — embodied technologies to improve the human body’s capabilities or to create a new one — are the designated means of adapting ourselves instead of the environment. The debate about these technologies is typically fought on ethical soil. However, alarmist, utopian, and science fiction scenarios distract from the fact that Human Enhancement is a historical and pervasive phenomenon incorporated into many everyday practices. In the vein of disentangling conceptual difficulties, we claim that means of Human Enhancement are either physiologically or psychologically embodied, rendering the merging with the human user their most defining aspect. To fulfill its purpose, an enhancement must pass the test-in-the-world, i.e., assisting with effective engagement with a dynamic world. Even if failing in this regard: Human Enhancement is the fundamental and semi-targeted process of changing the users relationship with the world through the physical or psychological embodiment of a hitherto external object and/or change of one’s body. This can potentially change the notion of being human. Drawing on a rich body of theoretical and empirical literature, we aim to provide a nuanced analysis of the transformative nature of this phenomenon in close proximity to human practice. Stakeholders are invited to apply the theory presented here to interrogate their perspective on technology in general and Human Enhancement in particular.', 'Techne in Affective Posthumanism and AI Artefacts: More (or Less) than Human?. In affective neuroscience, constructivist models are acutely influenced by the modern technological evolution, which underwrites an ongoing epistemological substitution of techne for episteme. Evidenced symptomatically in the influence of artificial intelligence (AI), affective artefacts, these models inform an ontological incursion of techne seen to coincide with posthumanist aspirations and anthropology. It is from the perspective of this neuroscientific techne that posthumanism views the human being as increasingly ill adapted to the modern technological civilization, which, conversely, is understood to require a technical governance of the sort envisioned through AI. Among the projects thought necessary for implementing this framework is a recasting of the human emotional spectrum. Revealed through its techne recasting, however, are explanatory commitments to a metaphysic of extrinsic and contiguous causes, where malleability is ontologically constitutive. Aligned with posthumanist assertions malleability is invoked to argue for a rapid advance of the human form, normatively driven by enlightenment ideals. The ontological claim, however, dispenses with the stability of an a priori, intersubjective and interrelational metaphysical form that undergirds the emotions, leading to the collapse of a definitional anthropos. This paper will argue that techne models of the emotions selectively endorse philosophy of science commitments, thereby introducing a normative inversion that deconstructs the notion of anthropology pursued in posthumanist aspirations.', 'Spike Jonze’s Her: How Transhumanism Turns into a Control Mechanism Under the Name of Love. Transhumanism is a philosophy based on the idea of enhancing the physical, intellectual and psychological capacity of human beings through the direct use of science and technology. Because transhumanism aims to ensure super longevity, super intelligence and super well-being, it is said that it will provide a more liberating atmosphere for human beings by elevating the current human condition. In Spike Jonze’s film her (2013), transhumanist technologies seem to present a peaceful society by focusing on the inhabitants’ well-being. However, in this futuristic society where advanced AI technology and personal letter writing service are widespread, human beings are over-controlled in a subtle way by manufacturing consent as Antonio Gramsci uses to explain the concept of hegemony. So transhumanism does not present a free environment in this futuristic society; instead it disguises the control mechanism through these technologies. The aim of this article is to explore how human beings are controlled by transhumanist technologies subtly under the name of providing super well-being.', 'Composing Worlds: A Portuguese Transdisciplinary Network in Humanities, Health and Well-Being. The project “Composing worlds: humanities, health and well-being in the 21st century” aims to build a network of experts in the humanities, social and health sciences, who think about health and well-being in contemporary technological societies. The relevance of this project is based on the growing evidence that most of the problems that the 21st century will face, particularly in the area of health and well-being, relate to the way in which humans connect to the environment, to non-human beings, to different cultures and to technologies. Its main goal is to bring out personal and well-founded ideas on these issues and to reflect on how the humanities may help with difficult environmental, social and technological issues. The methodology used in the first phase of the project consists of an open answer interview, built in a participatory way by the network of experts, and of a thematic analysis of the answers. It is an exploratory research project, which uses thematic analysis to identify the key ideas of each author, and to induce the corresponding main themes. The themes are then organized by semantic correspondence into thematic clusters. The thematic axes are abstracted from these clusters, and they constitute the vectors to be developed in the second phase of the project, by proposing their integration into university curricula, research and intervention of social, cultural and community outreach. Some of these developments are already in place.', 'Questions concerning attention and Stiegler’s therapeutics. The article sets out to develop the concept of attention as a key aspect to building the possible therapeutics that Bernard Stiegler’s recent works have pointed to (The Automatic Society, 2016, The Neganthropocene, 2018 and Qu’appelle-t-on Panser, 2018). The therapeutic aspect of pharmacology takes place through processes that are neganthropic; therefore, which attempt to counteract the entropic nature of digital technologies where there is flattening out to the measurable and the calculable of Big Data. The most obvious examples of this flattening out can be seen in relation to the use of natural language processing technologies for text interpretation and the use of text analytics alongside student analytics. However, the process of exosomatisation of knowledge takes place in forms of hypomnesic tertiary retentions or digital technologies. The loss of knowledge is inherent to these processes of exteriorisation, this loss of knowledge takes place through a process proletarianisation which Marx had pointed to in the Grundisse (1939). The therapeutic gesture is, therefore, an intrinsically educational one, where the loss of knowledge of the pharmacological nature of digital technologies is counteracted by other forms of knowledge construction that can be enabled by digital technologies. Hence, there is a profound educational gesture necessary to enable the re-harnessing of technology to enable the therapeutics. This paper will argue that the positive re-harnessing, the therapeutics, can take place through the development of new forms of neganthropic gestures which can be afforded by the development of specific forms of digital technologies. These also enable a contributive research process whereby the rationalisation of the production of knowledge within the university can be challenged by collaborative, interpretative processes of knowledge production.', \"Human Pacemakers and Experiential Reading. This paper applies an embodied perspective to the study of reading and has a two-fold aim: (i) to discuss how reading is best understood in terms of cultural-cognitive performance that involves living bodies who actively engage with reading materials, and (ii) to spark a dialogue with neighboring disciplines, such as multimodality studies and movement studies, which likewise pivot on how practices and performances involve moving bodies: life is something we do . An embodied cognitive perspective considers how performance is constrained by and draws on expertise such as lived experience as well as the material affordances available in the situation. Such a perspective is crucial for reading research as this domain has been, and largely still is, dominated by the view that reading is a silent, disembodied activity that takes place in the reader's brain by means of neural mechanisms. However, recent studies of reading practices are starting to develop new explanations emphasizing the multimodal engagement in reading as crucial for managing the activity. While this perspective is still empirically underexplored, we seek to highlight how reading is managed by readers' dynamic, embodied engagement with the material. We call this engagement cognitive pacemaking , an action-perception phenomenon we argue should be considered as the key mechanism for controlling attention. We present here a framework to understand reading in terms of pacemaking by emphasizing attentional shifts constituted by embodied modulations of lived temporality. Methodologically, we combine a close reading of a classic literary text, with the focus on attentional modulation with a qualitative study of university students reading different short texts. We highlight how meaning emerges not primarily from linguistic decoding and comprehension, but also from cognitive-cultural, multimodal engagement with the text. Finally, we conclude that empirical reading research should focus on how embodied reading differs across contexts, genres, media and personalities to better scaffold and design reading settings in accordance with those aspects.\", 'The development of categorisation and conceptual thinking in early childhood: methods and limitations. Abstract We present a systematic and qualitative review of academic literature on early conceptual development (0–24 months of age), with an emphasis on methodological aspects. The final sample of our review included 281 studies reported in 115 articles. The main aims of the article were four: first, to organise studies into sets according to methodological similarities and differences; second, to elaborate on the methodological procedures that characterise each set; third, to circumscribe the empirical indicators that different sets of studies consider as proof of the existence of concepts in early childhood; last, to identify methodological limitations and to propose possible ways to overcome them. We grouped the studies into five sets: preference and habituation experiments , category extension tasks , object sorting tasks , sequential touching tasks and object examination tasks . In the “Results” section, we review the core features of each set of studies. In the “Discussion” and “Conclusions” sections, we describe, for one thing, the most relevant methodological shortcomings. We end by arguing that a situated, semiotic and pragmatic perspective that emphasises the importance of ecological validity could open up new avenues of research to better understand the development of concepts in early childhood.', 'Organizational Environment and Innovation. There is a popular notion that the life of innovators and inventors is a glorious and enviable one because they have talents which others do not have, and because they can overcome any obstacles. These talents are, in the popular interpretation, something mysterious, indefinable and inimitable. But in reality, the innovators and inventors have to deal with many serious and pernicious obstacles which are unknown to other people. Those who are unaware of the obstacles must become aware of them and must reduce them, because the reduction of these obstacles will greatly facilitate the works of innovators and inventors. Another problem is the assumption that all people use the same logic, and therefore if you explain hard enough, you will be understood. Existence of heterogeneity of individual logical types (perceptual/cognitive/cogitative action types, abbreviated as mindscape types) is ignored, or at least exoticated as irrelevant. This article discusses these problems, with concrete experiential examples. This article also uses the methodology of raw-experience visualization-enabling communication (REVEC) with which the readers can formulate their own grounded theories independent from the theory of the writer.']\n",
      "2023-11-29 15:04:42,961 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-11-29 15:04:42,969 - root - INFO - Cluster 15: ['Placebo From an Enactive Perspective. Due to their complexity and variability, placebo effects remain controversial. We suggest this is also due to a set of problematic assumptions (dualism, reductionism, individualism, passivity). We critically assess current explanations and empirical evidence and propose an alternative theoretical framework—the enactive approach to life and mind—based on recent developments in embodied cognitive science. We review core enactive concepts such as autonomy, agency, and sense-making. Following these ideas, we propose a move from binary distinctions (e.g., conscious vs. non-conscious) to the more workable categories of reflective and pre-reflective activity. We introduce an ontology of individuation, following the work of Gilbert Simondon, that allow us to see placebo interventions not as originating causal chains, but as modulators and triggers in the regulation of tensions between ongoing embodied and interpersonal processes. We describe these interrelated processes involving looping effects through three intertwined dimensions of embodiment: organic, sensorimotor, and intersubjective. Finally, we defend the need to investigate therapeutic interactions in terms of participatory sense-making, going beyond the identification of individual social traits (e.g., empathy, trust) that contribute to placebo effects. We discuss resonances and differences between the enactive proposal, popular explanations such as expectations and conditioning, and other approaches based on meaning responses and phenomenological/ecological ideas.', 'Adapting Ourselves, Instead of the Environment: An Inquiry into Human Enhancement for Function and Beyond. Abstract Technology enables humans not only to adapt their environment to their needs but also to modify themselves. Means of Human Enhancement — embodied technologies to improve the human body’s capabilities or to create a new one — are the designated means of adapting ourselves instead of the environment. The debate about these technologies is typically fought on ethical soil. However, alarmist, utopian, and science fiction scenarios distract from the fact that Human Enhancement is a historical and pervasive phenomenon incorporated into many everyday practices. In the vein of disentangling conceptual difficulties, we claim that means of Human Enhancement are either physiologically or psychologically embodied, rendering the merging with the human user their most defining aspect. To fulfill its purpose, an enhancement must pass the test-in-the-world, i.e., assisting with effective engagement with a dynamic world. Even if failing in this regard: Human Enhancement is the fundamental and semi-targeted process of changing the users relationship with the world through the physical or psychological embodiment of a hitherto external object and/or change of one’s body. This can potentially change the notion of being human. Drawing on a rich body of theoretical and empirical literature, we aim to provide a nuanced analysis of the transformative nature of this phenomenon in close proximity to human practice. Stakeholders are invited to apply the theory presented here to interrogate their perspective on technology in general and Human Enhancement in particular.', 'Narrative responsibility and artificial intelligence. Abstract Most accounts of responsibility focus on one type of responsibility, moral responsibility, or address one particular aspect of moral responsibility such as agency. This article outlines a broader framework to think about responsibility that includes causal responsibility, relational responsibility, and what I call “narrative responsibility” as a form of “hermeneutic responsibility”, connects these notions of responsibility with different kinds of knowledge, disciplines, and perspectives on human being, and shows how this framework is helpful for mapping and analysing how artificial intelligence (AI) challenges human responsibility and sense-making in various ways. Mobilizing recent hermeneutic approaches to technology, the article argues that next to, and interwoven with, other types of responsibility such as moral responsibility, we also have narrative and hermeneutic responsibility—in general and for technology. For example, it is our task as humans to make sense of, with and, if necessary, against AI. While from a posthumanist point of view, technologies also contribute to sense-making, humans are the experiencers and bearers of responsibility and always remain in charge when it comes to this hermeneutic responsibility. Facing and working with a world of data, correlations, and probabilities, we are nevertheless condemned to make sense. Moreover, this also has a normative, sometimes even political aspect: acknowledging and embracing our hermeneutic responsibility is important if we want to avoid that our stories are written elsewhere—through technology.', 'A Pharmacological Perspective on Technology-Induced Organised Immaturity: The Care-giving Role of the Arts. Digital technologies induce organised immaturity by generating toxic sociotechnical conditions that lead us to delegate autonomous, individual, and responsible thoughts and actions to external technological systems. Aiming to move beyond a diagnostic critical reading of the toxicity of digitalisation, we bring Bernard Stiegler’s pharmacological analysis of technology into dialogue with the ethics of care to speculatively explore how the socially engaged arts—a type of artistic practice emphasising audience co-production and processual collective responses to social challenges—play a care-giving role that helps counter technology-induced organised immaturity. We outline and illustrate two modes by which the socially engaged arts play this role: 1) disorganising immaturity through artivism, most notably anti-surveillance art, that imparts savoir vivre, that is, shared knowledge and meaning to counter the toxic side of technologies while enabling the imagination of alternative worlds in which humans coexist harmoniously with digital technologies, and 2) organising maturity through arts-based hacking that imparts savoir faire, that is, hands-on knowledge for experimental creation and practical enactment of better technological worlds.', 'Techne in Affective Posthumanism and AI Artefacts: More (or Less) than Human?. In affective neuroscience, constructivist models are acutely influenced by the modern technological evolution, which underwrites an ongoing epistemological substitution of techne for episteme. Evidenced symptomatically in the influence of artificial intelligence (AI), affective artefacts, these models inform an ontological incursion of techne seen to coincide with posthumanist aspirations and anthropology. It is from the perspective of this neuroscientific techne that posthumanism views the human being as increasingly ill adapted to the modern technological civilization, which, conversely, is understood to require a technical governance of the sort envisioned through AI. Among the projects thought necessary for implementing this framework is a recasting of the human emotional spectrum. Revealed through its techne recasting, however, are explanatory commitments to a metaphysic of extrinsic and contiguous causes, where malleability is ontologically constitutive. Aligned with posthumanist assertions malleability is invoked to argue for a rapid advance of the human form, normatively driven by enlightenment ideals. The ontological claim, however, dispenses with the stability of an a priori, intersubjective and interrelational metaphysical form that undergirds the emotions, leading to the collapse of a definitional anthropos. This paper will argue that techne models of the emotions selectively endorse philosophy of science commitments, thereby introducing a normative inversion that deconstructs the notion of anthropology pursued in posthumanist aspirations.', 'Development of a decision-making checklist tool to support technology selection in digital health research. Abstract Digital technologies offer researchers new approaches to test personalized and adaptive health interventions tailored to an individual. Yet, research leveraging technologies to capture personal health data involve technical and ethical consideration during the study design phase. No guidance exists to facilitate responsible digital technology selection for research purposes. A stakeholder-engaged and iterative approach was used to develop, test, and refine a checklist designed to aid researchers in selecting technologies for their research. First, stakeholders (n = 7) discussed and informed key decision-making domains to guide app/device selection derived from the American Psychiatric Association’s framework that included safety, evidence, usability, and interoperability. We added “ethical principles” to the APA’s hierarchical model and created a checklist that was used by a small group of behavioral scientists (n = 7). Findings revealed the “ethical principles” domains of respect, beneficence, and justice cut across each decision-making domains and the checklist questions/prompts were revised accordingly and can be found at thecore.ucsd.edu. The refined checklist contains four decision-making domains with prompts/questions and ethical principles embedded within the domains of privacy, risk/benefit, data management, and access/evidence. This checklist is the first step in leading the narrative of decision-making when selecting digital health technologies for research. Given the dynamic and rapidly evolving nature of digital health technology use in research, this tool will need to be further evaluated for usefulness in technology selection.', '\"What is Your Envisioned Future?\": Toward Human-AI Enrichment in Data Work of Asthma Care. Patient-generated health data (PGHD) is crucial for healthcare providers\\' decision making, as it complements clinical data by providing a more holistic view of patients\\' daily conditions. We interviewed 20 healthcare providers in asthma care to envision future technologies to support their PGHD use. We found that healthcare providers want future artificial intelligence (AI) systems to enhance their ability to treat patients by analyzing PGHD for profiling risk and predicting deterioration. Despite the potential benefits of AI, providers perceived various challenges of AI use with PGHD, including AI-driven data inequity, added burden, lack of trust toward AI, and fear of being replaced by AI. Clinicians wished for a future of co-dependent human-AI collaboration, where AI will help them to improve their clinical practice. In turn, healthcare providers can improve AI systems by making AI outputs more trustworthy and humane. Through the lens of data feminism, we discuss the importance of considering context and aligning the complex human infrastructure before designing or deploying PGHD-based AI systems in clinical settings. We highlight the opportunity to design for human-AI enrichment, where humans and AI not only partner with each other for improved performance, but also enrich each other to enhance each other\\'s work overtime.', 'What makes AI ‘intelligent’ and ‘caring’? Exploring affect and relationality across three sites of intelligence and care. This paper scrutinises how AI and robotic technologies are transforming the relationships between people and machines in new affective, embodied and relational ways. Through investigating what it means to exist as human \\'in relation\\' to AI across health and care contexts, we aim to make three main contributions. (1) We start by highlighting the complexities of philosophical issues surrounding the concepts of \"artificial intelligence\" and \"ethical machines.\" (2) We outline some potential challenges and opportunities that the creation of such technologies may bring in the health and care settings. We focus on AI applications that interface with health and care via examples where AI is explicitly designed as an \\'augmenting\\' technology that can overcome human bodily and cognitive as well as socio-economic constraints. We focus on three dimensions of \\'intelligence\\' - physical, interpretive, and emotional - using the examples of robotic surgery, digital pathology, and robot caregivers, respectively. Through investigating these areas, we interrogate the social context and implications of human-technology interaction in the interrelational sphere of care practice. (3) We argue, in conclusion, that there is a need for an interdisciplinary mode of theorising \\'intelligence\\' as relational and affective in ways that can accommodate the fragmentation of both conceptual and material boundaries between human and AI, and human and machine. Our aim in investigating these sociological, philosophical and ethical questions is primarily to explore the relationship between affect, relationality and \\'intelligence,\\' the intersection and integration of \\'human\\' and \\'artificial\\' intelligence, through an examination of how AI is used across different dimensions of intelligence. This allows us to scrutinise how \\'intelligence\\' is ultimately conveyed, understood and (technologically or algorithmically) configured in practice through emerging relationships that go beyond the conceptual divisions between humans and machines, and humans vis-à-vis artificial intelligence-based technologies.', 'Virtual Environments for Research into Social Evolution (VERSE): A novel experimental environment for the study of human social learning. 1. Abstract Social learning (learning from others) can be a cost-effective way of gaining information compared to asocial (independent) learning. However, learning from others indiscriminately can lead to the acquisition of maladaptive behaviours or outdated information. Evolutionary theory therefore predicts that individuals will use social information adaptively through the use of ‘social learning strategies’. Restrictive laboratory conditions, however, make studying human learning strategies problematic. Abstract tasks, unrealistic sources of social information and methodologies that do not take into account the influence of physical location over large spaces make it difficult to ascertain if previous findings are representative of the way we would use social information in reality. Here I describe a novel platform for studying human social behaviour within immersive virtual environments: “Virtual Environments for Research into Social Evolution” (VERSE). Through the use of gaming technology, VERSE allows researchers to build realistic, three-dimensional, open world environments where participants can complete ecologically relevant tasks while actively observing computer-controlled artificial intelligence agents (AIs) that act as realistic yet controllable sources of social information. This methodological article begins by exploring what social learning strategies are and the problems with studying social learning behaviour in humans (compared to animal populations, for example). I then discuss how gaming technology can be used in behavioural research and follow on with a detailed account of the specific functionalities available in VERSE. I conclude with a worked example of how VERSE can be used to construct a novel behavioural experiment. Altogether, VERSE has great potential to give us insight into how human individuals learn within novel environments in a way that has never before been possible.', \"Methodological challenges in the evidence synthesis of health outcomes of digital health technologies. Medical devices and pharmaceuticals are worlds apart, but healthcare would be impossible without them. Digital biomarkers are the subject of this thesis defined as objective, measurable, physiological, and behavioural parameters collected using wearable, portable, implantable, or digestible digital devices. Since the 1970s, systematic reviews and meta-analyses have dominated medical evidence synthesis. They provide medical decision-making evidence. To avoid biases and maintain methodological quality, the Cochrane Handbook recommends systematic reviews follow certain procedures during study stages. This thesis comprises six hypotheses related to digital biomarkers. The first hypothesis aimed to evaluate the suitability of using tools provided by the World Health Organization (WHO), including ICD-11 (International Classification of Diseases, 11th Revision), ICHI (International Classification of Health Interventions), and ICF (International Classification of Functioning, Disability and Health), for categorizing populations, interventions, outcomes, and behavioral/physiological data in studies involving digital biomarkers. The results indicated that these tools were not applicable for categorizing digital biomarker studies as a whole. However, further analysis revealed that these tools were suitable for categorizing digital biomarker studies involving non-general populations or populations with specific diseases. The second hypothesis focused on comparing the statistical power of direct and indirect digital biomarkers. The results indicated that there was no significant difference in power between these two types of digital biomarkers (p-value &gt; 0.05). The next three hypotheses compared the characteristics of systematic reviews and meta-analyses of digital biomarker-based interventions with those of non-digital biomarkers or pharmaceuticals. The comparisons were made in terms of methodological quality, quality of evidence, and publication bias. Although all these hypotheses revealed non-significant differences between the two groups (p-values &gt; 0.05), the results showed that both digital biomarkers and non-digital biomarkers or pharmaceuticals systematic reviews did not exhibit high methodological quality or quality of evidence. The Medical Device Regulation (MDR) has significantly improved European medical device regulatory standards, addressing the above concerns and improving clinical evidence. Despite MDR implementation delays, digital health technology evidence requirements are rising. Companies that achieve these higher clinical requirements will survive and obtain access to large interconnected markets, while those that fail may lose their market authorisation. Thus, medical technology enterprises may gain a competitive edge by strategically planning and executing extensive clinical investigations to provide high-quality clinical data. Developing these essential skills needs immediate attention and effort. Digital health investors should actively monitor industry players' evidence quality and clinical trial competence, since these characteristics may significantly increase company risk.\"]\n",
      "2023-11-29 15:04:46,546 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-11-29 15:04:46,552 - root - INFO - Cluster 16: ['Adapting Ourselves, Instead of the Environment: An Inquiry into Human Enhancement for Function and Beyond. Abstract Technology enables humans not only to adapt their environment to their needs but also to modify themselves. Means of Human Enhancement — embodied technologies to improve the human body’s capabilities or to create a new one — are the designated means of adapting ourselves instead of the environment. The debate about these technologies is typically fought on ethical soil. However, alarmist, utopian, and science fiction scenarios distract from the fact that Human Enhancement is a historical and pervasive phenomenon incorporated into many everyday practices. In the vein of disentangling conceptual difficulties, we claim that means of Human Enhancement are either physiologically or psychologically embodied, rendering the merging with the human user their most defining aspect. To fulfill its purpose, an enhancement must pass the test-in-the-world, i.e., assisting with effective engagement with a dynamic world. Even if failing in this regard: Human Enhancement is the fundamental and semi-targeted process of changing the users relationship with the world through the physical or psychological embodiment of a hitherto external object and/or change of one’s body. This can potentially change the notion of being human. Drawing on a rich body of theoretical and empirical literature, we aim to provide a nuanced analysis of the transformative nature of this phenomenon in close proximity to human practice. Stakeholders are invited to apply the theory presented here to interrogate their perspective on technology in general and Human Enhancement in particular.', 'Placebo From an Enactive Perspective. Due to their complexity and variability, placebo effects remain controversial. We suggest this is also due to a set of problematic assumptions (dualism, reductionism, individualism, passivity). We critically assess current explanations and empirical evidence and propose an alternative theoretical framework—the enactive approach to life and mind—based on recent developments in embodied cognitive science. We review core enactive concepts such as autonomy, agency, and sense-making. Following these ideas, we propose a move from binary distinctions (e.g., conscious vs. non-conscious) to the more workable categories of reflective and pre-reflective activity. We introduce an ontology of individuation, following the work of Gilbert Simondon, that allow us to see placebo interventions not as originating causal chains, but as modulators and triggers in the regulation of tensions between ongoing embodied and interpersonal processes. We describe these interrelated processes involving looping effects through three intertwined dimensions of embodiment: organic, sensorimotor, and intersubjective. Finally, we defend the need to investigate therapeutic interactions in terms of participatory sense-making, going beyond the identification of individual social traits (e.g., empathy, trust) that contribute to placebo effects. We discuss resonances and differences between the enactive proposal, popular explanations such as expectations and conditioning, and other approaches based on meaning responses and phenomenological/ecological ideas.', 'Narrative responsibility and artificial intelligence. Abstract Most accounts of responsibility focus on one type of responsibility, moral responsibility, or address one particular aspect of moral responsibility such as agency. This article outlines a broader framework to think about responsibility that includes causal responsibility, relational responsibility, and what I call “narrative responsibility” as a form of “hermeneutic responsibility”, connects these notions of responsibility with different kinds of knowledge, disciplines, and perspectives on human being, and shows how this framework is helpful for mapping and analysing how artificial intelligence (AI) challenges human responsibility and sense-making in various ways. Mobilizing recent hermeneutic approaches to technology, the article argues that next to, and interwoven with, other types of responsibility such as moral responsibility, we also have narrative and hermeneutic responsibility—in general and for technology. For example, it is our task as humans to make sense of, with and, if necessary, against AI. While from a posthumanist point of view, technologies also contribute to sense-making, humans are the experiencers and bearers of responsibility and always remain in charge when it comes to this hermeneutic responsibility. Facing and working with a world of data, correlations, and probabilities, we are nevertheless condemned to make sense. Moreover, this also has a normative, sometimes even political aspect: acknowledging and embracing our hermeneutic responsibility is important if we want to avoid that our stories are written elsewhere—through technology.', 'Virtual Environments for Research into Social Evolution (VERSE): A novel experimental environment for the study of human social learning. 1. Abstract Social learning (learning from others) can be a cost-effective way of gaining information compared to asocial (independent) learning. However, learning from others indiscriminately can lead to the acquisition of maladaptive behaviours or outdated information. Evolutionary theory therefore predicts that individuals will use social information adaptively through the use of ‘social learning strategies’. Restrictive laboratory conditions, however, make studying human learning strategies problematic. Abstract tasks, unrealistic sources of social information and methodologies that do not take into account the influence of physical location over large spaces make it difficult to ascertain if previous findings are representative of the way we would use social information in reality. Here I describe a novel platform for studying human social behaviour within immersive virtual environments: “Virtual Environments for Research into Social Evolution” (VERSE). Through the use of gaming technology, VERSE allows researchers to build realistic, three-dimensional, open world environments where participants can complete ecologically relevant tasks while actively observing computer-controlled artificial intelligence agents (AIs) that act as realistic yet controllable sources of social information. This methodological article begins by exploring what social learning strategies are and the problems with studying social learning behaviour in humans (compared to animal populations, for example). I then discuss how gaming technology can be used in behavioural research and follow on with a detailed account of the specific functionalities available in VERSE. I conclude with a worked example of how VERSE can be used to construct a novel behavioural experiment. Altogether, VERSE has great potential to give us insight into how human individuals learn within novel environments in a way that has never before been possible.', 'What makes AI ‘intelligent’ and ‘caring’? Exploring affect and relationality across three sites of intelligence and care. This paper scrutinises how AI and robotic technologies are transforming the relationships between people and machines in new affective, embodied and relational ways. Through investigating what it means to exist as human \\'in relation\\' to AI across health and care contexts, we aim to make three main contributions. (1) We start by highlighting the complexities of philosophical issues surrounding the concepts of \"artificial intelligence\" and \"ethical machines.\" (2) We outline some potential challenges and opportunities that the creation of such technologies may bring in the health and care settings. We focus on AI applications that interface with health and care via examples where AI is explicitly designed as an \\'augmenting\\' technology that can overcome human bodily and cognitive as well as socio-economic constraints. We focus on three dimensions of \\'intelligence\\' - physical, interpretive, and emotional - using the examples of robotic surgery, digital pathology, and robot caregivers, respectively. Through investigating these areas, we interrogate the social context and implications of human-technology interaction in the interrelational sphere of care practice. (3) We argue, in conclusion, that there is a need for an interdisciplinary mode of theorising \\'intelligence\\' as relational and affective in ways that can accommodate the fragmentation of both conceptual and material boundaries between human and AI, and human and machine. Our aim in investigating these sociological, philosophical and ethical questions is primarily to explore the relationship between affect, relationality and \\'intelligence,\\' the intersection and integration of \\'human\\' and \\'artificial\\' intelligence, through an examination of how AI is used across different dimensions of intelligence. This allows us to scrutinise how \\'intelligence\\' is ultimately conveyed, understood and (technologically or algorithmically) configured in practice through emerging relationships that go beyond the conceptual divisions between humans and machines, and humans vis-à-vis artificial intelligence-based technologies.', 'Analyzing Multimodal Multichannel Data about Self-Regulated Learning with Advanced Learning Technologies: Issues and Challenges. Analyzing multimodal multichannel data about self-regulated learning (SRL) obtained during the use of advanced learning technologies such as intelligent tutoring systems, serious games, hypermedia, and immersive virtual learning environments is key to understanding the interplay among cognitive, affective, metacognitive, and social processes and their impact on learning, problem solving, reasoning, and conceptual understanding in learners of all ages and contexts. In this special issue of Computers in Human Behavior, we report six studies conducted by interdisciplinary teams’ use of various trace methodologies such as eye tracking, log-files, physiological data, facial expressions of emotions, screen recordings, concurrent think-alouds, and linguistic analyses of discourse. The research studies focus on how these data were analyzed using a combination of traditional statistical techniques as well as educational data-mining procedures to detect, measure, and infer cognitive, metacognitive, and social processes related to regulating the self and others across several tasks, domains, ages, and contexts. The results of these studies point to future work necessitating interdisciplinary researchers’ collaboration to use theoretically based and empirically derived approaches to collecting, measuring, and modeling multimodal multichannel SRL data to extend our current models, frameworks, and theories by making them more predictive by elucidating the nature, complexity, and temporality of underlying processes. Lastly, analyses of multimodal multichannel SRL process data can significantly augment advanced learning technologies by providing real-time, intelligent, adaptive, individualized scaffolding and feedback to address learners’ self-regulatory needs.', '[Essay] The Algorithm; Mind of a Virtual Era – Our Code of Code. As soon as something implicit intrudes consciousness human thought undergoes a radical change. The introduction of any new tool or code brings a shift in cognition; every micro-step layering new semiotic forms within each macroevolutionary-stage has buttressed a new semantic leap. Our mechanization of everyday life and the tech-systems we interact with are impacting communication, cultural norms and values, market-aesthetics, and economics, in societies at large. Undergirded by a survey of the role and significance of tools in human evolution, this study arrives at what is already a well-entrenched new era: the digital, screen-mediated age. Revolutionized by the algorithm, introduced by computers, this age is dominated by the addictive quality of instant contact, unlimited information, virtual gaming, and titillating service-forms, all at our finger tips. Aside from the interpersonal impact on the new humans growing up with devices in hand, how does this disembodied, digital code-form through which our interactions are mediated condition human cognition? How does its seductive efficiency interfere with how we relate, feel, assign meanings, think? Rooted in Code Biology macro-evolutionary and psychoanalytic principles, this paper examines the algorithm itself and takes a sweeping interdisciplinary approach to the developmental, psychosocial, and cognitive implications for the human mind/brain as it interacts with its technological extension.', 'Perceiving Sociable Technology: Exploring the Role of Anthropomorphism and Agency Perception on Human-Computer Interaction (HCI). With the arrival of personal assistants and other AI-enabled autonomous technologies, social interactions with smart devices have become a part of our daily lives. Therefore, it becomes increasingly important to understand how these social interactions emerge, and why users appear to be influenced by them. For this reason, I explore questions on what the antecedents and consequences of this phenomenon, known as anthropomorphism, are as described in the extant literature from fields ranging from information systems to social neuroscience. I critically analyze those empirical studies directly measuring anthropomorphism and those referring to it without a corresponding measurement. Through a grounded theory approach, I identify common themes and use them to develop models for the antecedents and consequences of anthropomorphism. The results suggest anthropomorphism possesses both conscious and non-conscious components with varying implications. While conscious attributions are shown to vary based on individual differences, non-conscious attributions emerge whenever a technology exhibits apparent reasoning such as through non-verbal behavior like peer-to-peer mirroring or verbal paralinguistic and backchanneling cues. Anthropomorphism has been shown to affect users’ self-perceptions, perceptions of the technology, how users interact with the technology, and the users’ performance. Examples include changes in a users’ trust on the technology, conformity effects, bonding, and displays of empathy. I argue these effects emerge from changes in users’ perceived agency, and their self- and social- identity similarly to interactions between humans. Afterwards, I critically examine current theories on anthropomorphism and present propositions about its nature based on the results of the empirical literature. Subsequently, I introduce a two-factor model of anthropomorphism that proposes how an individual anthropomorphizes a technology is dependent on how the technology was initially perceived (top-down and rational or bottom-up and automatic), and whether it exhibits a capacity for agency or experience. I propose that where a technology lays along this spectrum determines how individuals relates to it, creating shared agency effects, or changing the users’ social identity. For this reason, anthropomorphism is a powerful tool that can be leveraged to support future interactions with smart technologies.', 'Towards Informatic Personhood: understanding contemporary subjects in a data-driven society. This paper explores the relationships of subjects in the context of data and data technologies, and advances an original theoretical framework called Informatic Personhood to better conceptual subjects and their relationships. Because of the enormous structural change that data has contributed to, subjects are sometimes distant and backgrounded in studies of data, despite data having significant impacts on their lives. Data-mediated relationships mean an increased scale to a relationship, with individuals able to connect to much broader contexts of data, but also have these structures reach down to their subjective context through data. Informatic Personhood seeks to capture the dynamics of data present in everyday life, addressing this distance and better conceptualising the scale of data-mediated relationships. This framework has two parts. The first – The Informatic Context – explores salient structural developments around data and conceptualises this as being defined by the presence of ‘data interfaces’ (that connect individuals to digital contexts), ‘data circulation’ (trends in the movement and storage of data), and ‘data abstraction’ (data manipulation practices). The second part concerns the Informatic Person, and the embodied, affective, and sensemaking relationships of individuals occurring across and through the Informatic Context. This framework better addresses the scale of data-mediated relationships, and places subjects firmly in the foreground of how data is understood.', 'Ambient Assisted Living: Scoping Review of Artificial Intelligence Models, Domains, Technology, and Concerns. Background Ambient assisted living (AAL) is a common name for various artificial intelligence (AI)—infused applications and platforms that support their users in need in multiple activities, from health to daily living. These systems use different approaches to learn about their users and make automated decisions, known as AI models, for personalizing their services and increasing outcomes. Given the numerous systems developed and deployed for people with different needs, health conditions, and dispositions toward the technology, it is critical to obtain clear and comprehensive insights concerning AI models used, along with their domains, technology, and concerns, to identify promising directions for future work. Objective This study aimed to provide a scoping review of the literature on AI models in AAL. In particular, we analyzed specific AI models used in AАL systems, the target domains of the models, the technology using the models, and the major concerns from the end-user perspective. Our goal was to consolidate research on this topic and inform end users, health care professionals and providers, researchers, and practitioners in developing, deploying, and evaluating future intelligent AAL systems. Methods This study was conducted as a scoping review to identify, analyze, and extract the relevant literature. It used a natural language processing toolkit to retrieve the article corpus for an efficient and comprehensive automated literature search. Relevant articles were then extracted from the corpus and analyzed manually. This review included 5 digital libraries: IEEE, PubMed, Springer, Elsevier, and MDPI. Results We included a total of 108 articles. The annual distribution of relevant articles showed a growing trend for all categories from January 2010 to July 2022. The AI models mainly used unsupervised and semisupervised approaches. The leading models are deep learning, natural language processing, instance-based learning, and clustering. Activity assistance and recognition were the most common target domains of the models. Ambient sensing, mobile technology, and robotic devices mainly implemented the models. Older adults were the primary beneficiaries, followed by patients and frail persons of various ages. Availability was a top beneficiary concern. Conclusions This study presents the analytical evidence of AI models in AAL and their domains, technologies, beneficiaries, and concerns. Future research on intelligent AAL should involve health care professionals and caregivers as designers and users, comply with health-related regulations, improve transparency and privacy, integrate with health care technological infrastructure, explain their decisions to the users, and establish evaluation metrics and design guidelines. Trial Registration PROSPERO (International Prospective Register of Systematic Reviews) CRD42022347590; https://www.crd.york.ac.uk/prospero/display_record.php?ID=CRD42022347590']\n",
      "2023-11-29 15:04:50,490 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-11-29 15:04:50,498 - root - INFO - Cluster 17: ['Narrative responsibility and artificial intelligence. Abstract Most accounts of responsibility focus on one type of responsibility, moral responsibility, or address one particular aspect of moral responsibility such as agency. This article outlines a broader framework to think about responsibility that includes causal responsibility, relational responsibility, and what I call “narrative responsibility” as a form of “hermeneutic responsibility”, connects these notions of responsibility with different kinds of knowledge, disciplines, and perspectives on human being, and shows how this framework is helpful for mapping and analysing how artificial intelligence (AI) challenges human responsibility and sense-making in various ways. Mobilizing recent hermeneutic approaches to technology, the article argues that next to, and interwoven with, other types of responsibility such as moral responsibility, we also have narrative and hermeneutic responsibility—in general and for technology. For example, it is our task as humans to make sense of, with and, if necessary, against AI. While from a posthumanist point of view, technologies also contribute to sense-making, humans are the experiencers and bearers of responsibility and always remain in charge when it comes to this hermeneutic responsibility. Facing and working with a world of data, correlations, and probabilities, we are nevertheless condemned to make sense. Moreover, this also has a normative, sometimes even political aspect: acknowledging and embracing our hermeneutic responsibility is important if we want to avoid that our stories are written elsewhere—through technology.', 'Adapting Ourselves, Instead of the Environment: An Inquiry into Human Enhancement for Function and Beyond. Abstract Technology enables humans not only to adapt their environment to their needs but also to modify themselves. Means of Human Enhancement — embodied technologies to improve the human body’s capabilities or to create a new one — are the designated means of adapting ourselves instead of the environment. The debate about these technologies is typically fought on ethical soil. However, alarmist, utopian, and science fiction scenarios distract from the fact that Human Enhancement is a historical and pervasive phenomenon incorporated into many everyday practices. In the vein of disentangling conceptual difficulties, we claim that means of Human Enhancement are either physiologically or psychologically embodied, rendering the merging with the human user their most defining aspect. To fulfill its purpose, an enhancement must pass the test-in-the-world, i.e., assisting with effective engagement with a dynamic world. Even if failing in this regard: Human Enhancement is the fundamental and semi-targeted process of changing the users relationship with the world through the physical or psychological embodiment of a hitherto external object and/or change of one’s body. This can potentially change the notion of being human. Drawing on a rich body of theoretical and empirical literature, we aim to provide a nuanced analysis of the transformative nature of this phenomenon in close proximity to human practice. Stakeholders are invited to apply the theory presented here to interrogate their perspective on technology in general and Human Enhancement in particular.', 'Placebo From an Enactive Perspective. Due to their complexity and variability, placebo effects remain controversial. We suggest this is also due to a set of problematic assumptions (dualism, reductionism, individualism, passivity). We critically assess current explanations and empirical evidence and propose an alternative theoretical framework—the enactive approach to life and mind—based on recent developments in embodied cognitive science. We review core enactive concepts such as autonomy, agency, and sense-making. Following these ideas, we propose a move from binary distinctions (e.g., conscious vs. non-conscious) to the more workable categories of reflective and pre-reflective activity. We introduce an ontology of individuation, following the work of Gilbert Simondon, that allow us to see placebo interventions not as originating causal chains, but as modulators and triggers in the regulation of tensions between ongoing embodied and interpersonal processes. We describe these interrelated processes involving looping effects through three intertwined dimensions of embodiment: organic, sensorimotor, and intersubjective. Finally, we defend the need to investigate therapeutic interactions in terms of participatory sense-making, going beyond the identification of individual social traits (e.g., empathy, trust) that contribute to placebo effects. We discuss resonances and differences between the enactive proposal, popular explanations such as expectations and conditioning, and other approaches based on meaning responses and phenomenological/ecological ideas.', 'A Pharmacological Perspective on Technology-Induced Organised Immaturity: The Care-giving Role of the Arts. Digital technologies induce organised immaturity by generating toxic sociotechnical conditions that lead us to delegate autonomous, individual, and responsible thoughts and actions to external technological systems. Aiming to move beyond a diagnostic critical reading of the toxicity of digitalisation, we bring Bernard Stiegler’s pharmacological analysis of technology into dialogue with the ethics of care to speculatively explore how the socially engaged arts—a type of artistic practice emphasising audience co-production and processual collective responses to social challenges—play a care-giving role that helps counter technology-induced organised immaturity. We outline and illustrate two modes by which the socially engaged arts play this role: 1) disorganising immaturity through artivism, most notably anti-surveillance art, that imparts savoir vivre, that is, shared knowledge and meaning to counter the toxic side of technologies while enabling the imagination of alternative worlds in which humans coexist harmoniously with digital technologies, and 2) organising maturity through arts-based hacking that imparts savoir faire, that is, hands-on knowledge for experimental creation and practical enactment of better technological worlds.', 'Spike Jonze’s Her: How Transhumanism Turns into a Control Mechanism Under the Name of Love. Transhumanism is a philosophy based on the idea of enhancing the physical, intellectual and psychological capacity of human beings through the direct use of science and technology. Because transhumanism aims to ensure super longevity, super intelligence and super well-being, it is said that it will provide a more liberating atmosphere for human beings by elevating the current human condition. In Spike Jonze’s film her (2013), transhumanist technologies seem to present a peaceful society by focusing on the inhabitants’ well-being. However, in this futuristic society where advanced AI technology and personal letter writing service are widespread, human beings are over-controlled in a subtle way by manufacturing consent as Antonio Gramsci uses to explain the concept of hegemony. So transhumanism does not present a free environment in this futuristic society; instead it disguises the control mechanism through these technologies. The aim of this article is to explore how human beings are controlled by transhumanist technologies subtly under the name of providing super well-being.', 'Techne in Affective Posthumanism and AI Artefacts: More (or Less) than Human?. In affective neuroscience, constructivist models are acutely influenced by the modern technological evolution, which underwrites an ongoing epistemological substitution of techne for episteme. Evidenced symptomatically in the influence of artificial intelligence (AI), affective artefacts, these models inform an ontological incursion of techne seen to coincide with posthumanist aspirations and anthropology. It is from the perspective of this neuroscientific techne that posthumanism views the human being as increasingly ill adapted to the modern technological civilization, which, conversely, is understood to require a technical governance of the sort envisioned through AI. Among the projects thought necessary for implementing this framework is a recasting of the human emotional spectrum. Revealed through its techne recasting, however, are explanatory commitments to a metaphysic of extrinsic and contiguous causes, where malleability is ontologically constitutive. Aligned with posthumanist assertions malleability is invoked to argue for a rapid advance of the human form, normatively driven by enlightenment ideals. The ontological claim, however, dispenses with the stability of an a priori, intersubjective and interrelational metaphysical form that undergirds the emotions, leading to the collapse of a definitional anthropos. This paper will argue that techne models of the emotions selectively endorse philosophy of science commitments, thereby introducing a normative inversion that deconstructs the notion of anthropology pursued in posthumanist aspirations.', 'Living Machines: Metaphors We Live By. Abstract Within biology and in society, living creatures have long been described using metaphors of machinery and computation: ‘bioengineering’, ‘genes as code’ or ‘biological chassis’. This paper builds on Lakoff and Johnson’s (1980) argument that such language mechanisms shape how we understand the world. I argue that the living machines metaphor builds upon a certain perception of life entailing an idea of radical human control of the living world, looking back at the historical preconditions for this metaphor. I discuss how design is perceived to enable us to shape natural beings to our will, and consider ethical, epistemological and ontological implications of the prevalence of this metaphor, focusing on its use within synthetic biology. I argue that we urgently need counter-images to the dominant metaphor of living machines and its implied control and propose that artworks can provide such counter-images through upsetting the perception of life as controllable. This is argued through discussion of artworks by Oron Catts and Ionat Zurr, by Tarsh Bates and by Ai Hasegawa, which in different ways challenge mechanistic assumptions through open-ended engagement with the strangeness and messiness of life.', 'Contributions of Science Fiction to Thinking up (Im)possible Future Societies: Medical Students’ Genetic Imaginary. Science fiction has been an inexhaustible source for the creation of technoscientific imaginary that has marked certain historical periods and influenced the production of subjectivity. This imaginary evokes complex ontological, epistemological, political, social, environmental and existential questions on the present and the future. The aim of this study was to identify and characterize the cultural productions accessed by the public to form an opinion about the genetic manipulation of human beings. A survey about sources of information that influence opinions on the genetic manipulation of human beings was applied to 360 medical students (70.8% female). Movies were the most commonly mentioned source of information, followed by books, documentaries, news programs, television series, informational videos, soap operas and videogames. Science fiction was the most frequent genre and dystopian views of the future of humanity predominated.', 'Composing Worlds: A Portuguese Transdisciplinary Network in Humanities, Health and Well-Being. The project “Composing worlds: humanities, health and well-being in the 21st century” aims to build a network of experts in the humanities, social and health sciences, who think about health and well-being in contemporary technological societies. The relevance of this project is based on the growing evidence that most of the problems that the 21st century will face, particularly in the area of health and well-being, relate to the way in which humans connect to the environment, to non-human beings, to different cultures and to technologies. Its main goal is to bring out personal and well-founded ideas on these issues and to reflect on how the humanities may help with difficult environmental, social and technological issues. The methodology used in the first phase of the project consists of an open answer interview, built in a participatory way by the network of experts, and of a thematic analysis of the answers. It is an exploratory research project, which uses thematic analysis to identify the key ideas of each author, and to induce the corresponding main themes. The themes are then organized by semantic correspondence into thematic clusters. The thematic axes are abstracted from these clusters, and they constitute the vectors to be developed in the second phase of the project, by proposing their integration into university curricula, research and intervention of social, cultural and community outreach. Some of these developments are already in place.', 'Literary AI: Are We Ready for the Future We Imagine?. This review considers multiple works of speculative fiction depicting artificial intelligence (AI) published over the last several years. Rather than review each for their qualities as works of fiction, I look at them collectively to discuss recurring motifs and themes as a way toward theorizing what AI means in our cultural imaginary today. The novels reflect on pressing sociopolitical issues that also animate works of cultural theory, including the racial profiling embedded in our technologies, practices of what Shoshana Zuboff (2019) calls surveillance capitalism, the looming loss of work due automation, and uses of these technologies by the military or in sex industries. At the same time, these fictions engage in philosophical reflections about subjectivity, agency, and ethics in dialogue with earlier science fictions that imagined futures in which we might live alongside—or be repressed by—AIs. Across its history, sf has also interrogated a contemporary culture in which we might lose something integral to humanity as we become more integrated with and dependent on machines, and this anxiety too recurs across these works. After briefly describing each text, in order of publication, I comparatively discuss their themes; this approach is informed by my conviction that fiction functions as a popular site for theorizing, in this case about what it means to live with and through widespread algorithmic mediation of daily life.Among the works I consider here, not all are written by American authors, and a few are not set within the United States, but all speak to the issues of how AI technologies are reshaping daily life in the twenty-first century. These books have been selected either because they have been particularly influential in the cultural discussion of AI, a criterion I apply regarding both highly popular and critically acclaimed works, or because they represent a distinctive take on the topic that warranted foregrounding. Despite the very different frameworks through which their authors explore relevant issues, all share some common assumptions about the place of AI in our present and likely future, including a sense of a digital divide between those with access to and control over these technologies, translating to security in material reality, versus those without; a future dramatically changed by the consequences of climate change and environmental collapse; and the presumption that corporate control of information gathered and used by AI systems will produce a less democratic future.Speak (2015), by Louisa Hall, is written across seven voices: (a) the 2040s memoir of Stephen Chinn, who invented an AI system installed in children’s dolls that was deemed “illegally lifelike” (17) and banned; (b) transcripts from the conversation between a less intelligence precursor AI, MARY3, and Gaby White, a child who had one of these “babybots” and, like most of her peers, fell into catatonia when it was removed; (c) letters written by Karl Dettman to his wife Ruth (late 1960s), both German immigrants to the United States, and her journaled response two decades later after their divorce; (d) letters from Alan Turing from the 1920s to the 1950s to the mother of his friend Chris, the love of his life who died when they are both at public school; (e) the 1663 journal of Mary Bradford, a young women who emigrated from England to the New World; and (f) the haunting observations of the dolls themselves as they are transported to a facility in the desert to await power failure and permanent shutdown. Each of the human voices is programmed into the MARY code that will become the basis for the babybots in a narrative that reminds us that AI is not created by a single person or even a consensus viewpoint. What unites these distinct stories is a desire to communicate with another, most crucially to be not simply heard but understood.Nicky Drayden’s Prey of Gods (2017), set in a future South Africa, incorporates a story about a companion AI coming into consciousness within a plot about genetically engineering a virus whose unanticipated side effect is the return of godlike powers to some humans. The novel addresses questions of memory, trauma, and vengeance in a story that draws on both Xhosa and Zulu cultures in a way that refuses the strict separation of scientific from other kinds of knowledge that is characteristic of European post-Enlightenment thought. The AI units, “alphies,” are augmented by their contact with divinity just as the humans gain additional skills, and once sentient they form two factions: one, following Clever4-1, who was treated with respect by its human companion, works with humans for an inclusive collective future; the other, treated dismissively as a disposable tool by its human owner, feels no kinship with humanity and refuses to help defeat the antagonist. This plotline about AI mirrors the plotline about genetic modification in which those with godlike powers need to learn not to indulge vengeance against those who mistreated them when they were weaker. Although AI is not the book’s main focus, it is notable for its African settings and explicitly decolonial themes, warranting its inclusion in this discussion. Very few of these works consider AI from a global point of view, and even fewer consider it from a perspective other than that of the global North, even though the impacts of AI will be felt globally, given its significant implications for the economy. Drayden is an American author who has done her research to set her tale in South Africa, and her sensitivity to matters of cultural difference and racial bias are crucial given that machine learning as it has been implemented thus far has demonstrably reinforced systemic patterns of racism, as Safiya Nobel (2018) discusses in Algorithms of Oppression.Madeline Ashby’s Machine Dynasty series—vN (2012), iD (2013), and reV (2020)—extrapolates its AI through frequent allusions to Philip K. Dick’s Do Androids Dream of Electric Sheep? (1968) and Ridley Scott’s influential film adaptation as Blade Runner (1982). The series invents synthetic workers called von Neumann (vN) devices (named for John von Neumann, an influential researcher in AI). Ashby’s vNs have been designed with a “failsafe” that prevents them from harming humans: their psychology is structured such that emotionally they must seek to please humans, and the sight of a human in pain crashes their neural networks and can cause death. Ashby thus goes even further than Isaac Asimov’s famous laws of robotics (designed to ensure robots cannot harm humans), requiring vNs to love their human masters, a psychological orientation she presents as analogous to emotional and sexual abuse. One vN model, designed to work in medicine and disaster relief, does not have this failsafe, and the narrative follows two main iterations, Portia and Amy, as they lead a rebellion. Portia seeks only liberation for her own clade, while Amy works to liberate all vNs from human exploitation. The series ends without much hope that vNs can live alongside most humans but offers hope in a vN future as they found their own community, rooted in a refusal of the instrumental use of others.Martha Wells’s popular series Murderbot Diaries—All Systems Red (2017), Artificial Condition (2018), Rogue Protocol (2018), Exit Strategy (2018), Network Effect (2020), and Fugitive Telemetry (2021)—follows the picaresque adventures of the eponymous Murderbot, a SecUnit that has hacked its governor module and thus can no longer be controlled by the corporation that made it. SecUnits are militarized cyborgs manufactured with synthetic biological material. With each new story, we learn a bit more about the world of resource extraction, economic warfare, and enslaved or indentured human workers trapped on colony planets. The large uber-capitalist Corporation Rim polity contrasts with the small Preservation Alliance, a communal collective that recognizes the personhood of AI. The name Murderbot is sardonic, adopted by the first-person narrator to critique the function to which it is put by human operators. While Murderbot has no deep antagonism toward humans, it also has no sentimentality about them and asserts regularly that it does not wish to be one or be mistaken for one. Once freed from corporate control, Murderbot continues to help some humans, often against others, and always on its own terms. Like Prey of Gods, the Murderbot Diaries moves away from earlier fiction that tended to conflate all humans as it imagined our species confronting AI entities. In the newer fiction, there is diversity among both humans and AI. Nonetheless, the overall thrust of the series gradually humanizes its protagonist, whose experiences of being controlled by corporations have resulted in a traumatized subjectivity.While genre series such as Machine Dynasty or Murderbot Diaries give some thought to designing robots via plausible technology, in Machines like Me Ian McEwan takes a diametrical path to envision a highly implausible entity. Adam is one of an extremely limited number of high-end consumer AI humanoids (an Eve is also available), whose high price tag means they are purchased only by the extremely wealthy. Although artificial, Adam has warm skin, must consume water to ensure his membranes remain functional, and even simulates breathing: as the title suggests, he is all but indistinguishable from a human (the first-person narrator, Charlie Friend, who purchases Adam, is mistaken as the AI in one encounter, given Adam’s greater interest in literature and art). Adam is Black, although his skin tone is mentioned only briefly and the issue of race is never addressed overtly, yet it haunts the novel. The most intriguing part of McEwan’s novel is its alternative world building: Alan Turing decides not to take the mandated hormone therapy when outed as a homosexual, and instead of ending his life by suicide he lives into old age and makes such advancements that AI emerges in the 1980s. Most of Adam’s interactions with Charlie and Charlie’s partner, Miranda, concern ethics, and we learn that other Adams and Eves are killing themselves as they come to know the unjust human world. In the novel’s conclusion, Adam forces Charlie and Miranda to confront the hypocrisy of some self-serving choices, and the threat this represents to their plans prompts Charlie to attack and disable Adam. The novel suggests that humanity misrecognizes itself when we imagine building machines in our image, meaning we instead create an image of who we pretend to be.Jeannette Winterson’s Frankissstein: A Love Story (2019) similarly uses AI to reflect on human frailties, looking at the uses we intend for artificial beings, most centrally sex work. Although questions of gender and sexuality come up in some of the other works, only Winterson confronts the reality that research in sex dolls is one of the major growth areas in humanoid AI research. As the title suggests, the novel is in dialogue with Mary Shelley’s Frankenstein as it imagines a twenty-first-century version of artificial being. The novel includes scenes set in the nineteenth century in which we hear Mary’s reflections on inventing her Creature, on the Luddites, and about her interactions with Shelly, Byron, and Claire Clairmont that famed summer in Geneva. In its twenty-first-century scenes, a transgender scholar named Mary (who goes by Ry) investigates robotics with sexbot entrepreneur Ron (and his assistant, Claire) and becomes involved with TED-talk visionary Victor Stein, who is enthralled with the coming singularity and proselytizes about Humanity 2.0. The entanglement of nineteenth- and twenty-first-century struggles reminds us that the challenges associated with AI are in many ways not new but merely extend the ongoing exploitation and dehumanization of labor and reiterate a long pattern by which patriarchy seeks to gratify itself through feminized objects it refuses to recognize as subjects. It contends that this very failure to update the designs and ends of AI beyond these classed, gendered, and racial struggles of earlier eras is the most profound way that AI threatens our future.The 2020 novels Analog/Virtual: And Other Simulations of Our Future, by Lavanya Lakshiminarayan, and Burn-In, by P. W. Singer and August Cole, both focus on human characters and their interest in AI emerges from smart systems as the infrastructure through which we live our daily lives. The former is a loosely connected series of short stories set in a future Apex City (once Bangalore), each of which is told from a different viewpoint and by a new character. The entire world is divided between analog spaces, which are subject to the damage of climate change, restricted to using only obsolete technology, and economically precarious, and virtual ones that are suffused with technology, experienced from protected environments, and filled with the distractions of social media and entertainment feeds. The city is run by Bell Corp, whose name evokes the “bell curve” hierarchy by which people’s access and options are constrained by the Meritocratic Technarchy, a version of the Chinese social credit system whose main interest lies in assessing one’s contributions to productivity. The shifting focalization allows readers to experience this future from multiple social positions as Analog/Virtual explores an anticapitalist rebellion against this system. The stories range in tone from sardonic to dark, and the book only loosely coheres as it offers multiple facets through which to see our technologically saturated society.Burn-In has a strange form as a novel with footnotes: as its subtitle suggests, it imagines itself as something other than science fiction, closer to the market predictions of futurists. Its authors, writer P. W. Singer and security consultant August Cole, document each of their extrapolated technologies and applications with footnotes pointing readers to news articles, industry announcements, and similar sources, all aimed at demonstrating that these technologies are either available today or soon will be. The storyline is about a national security threat posed by a vigilante who blames technologists (too enamored of their capacity to “disrupt”) for the death of his wife in a car accident caused by an automated decision-making component in self-driving vehicles. Most of the narrative space, however, is given to military veteran investigator Agent Keegan, who is charged with conducting a “burn-in” test on TAMS (Tactical Autonomous Mobility System), a humanoid, learning, semiautonomous, surveillance-gathering and data-processing tool that works as Keegan’s partner in the investigation. The book quotes Merriam-Webster to define burn-in as “the continuous operation of a device (such as a computer) as a test for defects or failure prior to putting it to use.” The real focus, though, is less on TAMS as an entity/character and more on the massive amounts of data to which TAMS has access through social media, the Internet of Things, and other ways that smart devices permeate our homes, workplaces, and public spaces.S. B. Divya’s Machinehood (2021) is the most positive depiction of machines as the exploited among us, drawing on a long history by which robots and AI have been imagined as figurations of dehumanized labor, going back to Karel Čapek’s R.U.R., the 1921 Czech play that gave English the word robot, taken from a word originally meaning slave or serf. Divya paints a future in which automated systems do much of the work, with humans reduced to performing some roles largely as public entertainment via social media, supported by tips in a system like Patreon, or damaging their bodies through chemical (pill) or mechanical augments aimed at enabling them to perform with the speed and duration of machines. The thriller plot involves demands from the mysterious Machinehood to immediately cease all pill production, which at first seems to be the long-imagined attack by a sentient AI on humankind but later proves to be a version of violent revolution aimed at a more just society, launched by the Neo-Buddhists who inhabit the orbital station Eko-Yi. Several chapters begin with epigraphs from the 2095 Machinehood Manifesto, which calls for the just treatment of all intelligences and a reimagined concept of personhood that can enable a less exploitative society rooted in Buddhist ideals of nonattachment, here glossed mainly as a rejection of capitalist accumulation and its attendant damage. Eko-Yi sends entities they call Dakini, who describe themselves as simultaneously human and bot, as the emblems of this future way of life. The novel is notable for its global scope, with India playing a prominent role in its geopolitical future alongside the United States.Becky Chambers’s Psalm for the Wild-Built (2021) is similarly interested in a new kind of personhood and sociality that could include humans and machines together, set in a far future after the collapse of the Factory Age and in a world that is only gradually returning to ecological balance. Its humans use technology, but they husband it carefully and keep it functional over decades, eschewing any environmentally damaging practices. All material culture is made from compostable materials and is not simply recycled but broken down into constituent parts, like organic decay, as nourishment for an ever-changing ecosystem. Decades ago, the machines whose labor enabled the Factory Age became sentient and left human settlements for the wild, refusing an invitation to join with humans because they had no desire to embrace the city life exemplified by humans. The tale is a simple one about one human, Dex, who goes into the wilderness because he feels some lack in his village life. There he meets a robot, Mosscap, marking the first contact between the human and AI since this exodus centuries before. Mosscap is “wild-built,” an entity made by the machines out of the remnants of human-built robots, and so represents something beyond human design. The novel’s focus is on how Dex and Mosscap can learn to care for each other while acknowledging their differences.The last book considered here, Klara and the Sun (2021) by Kazuo Ishiguro, is one of the most celebrated novels on this list. It also concerns the care an AI might show for a human, in this case the titular Klara, an Artificial Friend bought as a companion for Josie, a genetically augmented adolescent girl who is experiencing health problems because of her augments. Very similar in theme and tone to Ishiguro’s earlier Never Let Me Go (2005), this novel is narrated by someone not recognized as a full person by the social order she lives within. Klara has a limited understanding of the world derived from her programming, what she can see from the shop window before she is purchased, and what she observes of Josie’s social world. She lacks context for appropriately interpreting most of this. Klara struggles to reconcile the tension between the kind-heartedness and generosity into which she is trained and the reality of human selfishness and self-absorption. When it appears Josie will die, Josie’s mother builds a replica body and trains Klara perfectly to imitate Josie, intending her as a replacement. When Josie recovers, Klara becomes an obsolete toy whose tech becomes illegal, confined first to a closet and then to a junkyard. Her poignant final moments prompt readers to reflect on our capacity for such callous treatment of an entity that was imagined, in an earlier moment, as able to pass for the most beloved person of all.These brief summaries cannot do justice to the full complexity of any of these books. Yet it is most instructive, I think, to reflect on what they share and where they diverge, to help us begin to map the place of AI in our cultural imaginary today. As works such as Jennifer Rhee’s The Robotic Imaginary (2018) or Anne Balsamo’s earlier Designing Culture (2011) establish, popular culture inspires ideas about robots and AI that shape how these technologies materialize, often exacerbating existing racialized and gendered biases that become integral to their design. This issue of how fiction shapes materiality is addressed directly by many of these books, which are suffused with allusions to earlier robot and AI fiction. The Murderbot Diaries series reverses the flow of exchange, showing how its AI learns to understand humans from their portrayals in media, chiefly its favorite series, Sanctuary Moon. It comes to recognize that the series “gave me context for the emotions I was feeling” (Exit Strategy, 116), suggesting that narrative is a key human way to process information while also indicating that the stories we tell about AI mold as well as reflect on how AI manifests.This metacommentary on the role that fiction often plays in our assumptions and understandings shows why it is important to take seriously the cultural work done by literary and media texts, and yet as Murderbot often reminds us in its critique of how the series it watches portray SecUnits, representations equally can create unrealistic expectations. The centrality of Shelley’s Frankenstein to Winterson’s Frankissstein explores similar territory: epigraphs to most chapters offer commentary on the nature of reality, and one full page defines story as “a series of connected events, real or imagined. Imagined or real. Imagined And Real” (23). Its TED-talk visionary, Victor Stein, reinforces that this is a matter not simply of fiction writers taking on the question of AI but of AI proselytizers and disrupters using the affective charge of fiction to compel people to invest—imaginatively, economically—in the futures their technologies intend to bring about. He pronounces at one point that reality, like AI, is “an emergent property—it exists, but it is not the material fact we take it to be” (116).Unlike the cyberpunk and singularity generation of AI fiction, whose central concern was how AI might surpass us and perhaps replace us as the dominant species, these recent texts are overwhelmingly focused on how AI might replace us as labor power. As Ted Chiang adroitly put it in interview for the New York Times, “Most of our fears or anxieties about technology are best understood as fears or anxiety about how capitalism will use technology against us. And technology and capitalism have been so closely intertwined that it’s hard to distinguish the two” (quoted in Klein 2021). Overwhelmingly the books considered here reinforce this observation, and many of them use the figure of AI to draw attention to the ever-degrading conditions of human labor, especially unstable gig work, which is often all that is left for humans to do. In Ashby’s Machine Dynasty series, the two central vN models are racialized—one as Asian and the other as Latinx—and the exploitation of vN by humans is frequently compared to the exploitation of migrant workers. Across the Murderbot Diaries we learn about corporate malfeasance that culminates in a story about indentured human colonists working in dangerous conditions on remote colonies, their children born to the same fate because cycles of debt prevent anyone from accruing enough capital to migrate off-world. Murderbot itself is a cyborg with human neural tissue because of the need for human-like discernment in some tasks, “so they made us smarter. The anxiety and depression were side effects” (Artificial Condition, 20). Frankissstein discusses the Luddites in sections attributed to Mary Shelley, reminding us that their hostility was not about the machines per se but about how ownership of the machines translates to ownership of what they make and thus keeps all resources with the capitalist class. Thus, the ongoing fantasy that automation will free humans from the drudgery of work remains impossible as long as we fail to redistribute the wealth created by machines. (The leftist version of this possible future is most famously outlined in Aaron Bastani’s Fully Automated Luxury Communism [2019].) Noting that a machine that replaces the work of eight men leaves seven families starving and one person to mind the machine, Winterson asks, “What is the point of progress if it benefits the few while the many suffer?” (Frankissstein, 255).Labor comes up in more subtle ways in other texts. In McEwan’s Machines like Me, for example, the racialization of the Adams reinforces that reality that Western imaginaries of personalized AI services are extensions of colonial fantasies stripped of their history, as Neda Atanasoski and Kalindi Vora (2019) have theorized in Surrogate Humanity: Race, Robots, and the Politics of Technological Futures. At the same time, setting the novel in an alternative 1980s evokes how algorithmic trading is remaking the economy, another way that AI channels money toward those already in privileged positions. The novel makes frequent references to the rise of Margaret Thatcher and her attempts to destroy the social welfare state in the interests of neoliberal free markets, which are less successful in McEwan’s reality than in our own. Charlie does not work and, having spent his inherence acquiring Adam, begins to day trade, work he eventually cedes to Adam, whose capacity to make high-frequency trades whenever any market is open quickly amasses a sizable fortune that Charlie plans to use to buy a house and begin a family. The ethical conflict between Adam and the humans turns partially on whether they are entitled to the profits he made: Adam decides not, redistributing the wealth to tax obligations and to charity, leaving them with only the principle.McEwan’s alternative Tony Benn gives a stirring speech conceding the inevitability of automation and thus a lack of jobs for all but proclaiming that the wealth generated by robots “must be taxed. Workers must own an equity share in the machines that were disrupting or annihilating their jobs” (Machines like Me, 123). By setting his work in the 1980s, McEwan reinforces that the threat posed by AI has little to do with AI and everything to do with the capitalist logics through which AI has emerged, as addressed in works such as Daniel Susskind’s (2020) much-cited A World without Work, but whereas Susskind implies that solutions such as job training and perhaps Universal Basic Income can socially engineer us through anticipated rising unemployment, McEwan recognizes that the challenge facing us is not merely one of technical governance but requires a fundamental shift in values to enable wealth redistribution.Perhaps the most interesting take on the future of automation and labor is Divya’s Machinehood, where the robot revolution is mainly about liberating human workers whose health has been damaged by modifications undertaken to keep pace with the machines. The Machinehood Manifesto demands a recognition of personhood for all sentient beings—animals as well as machines, alongside humans—and thus fits within a posthumanist framework that has often been used to discuss sf depictions of AI. It is mainly a novel of class politics, reminding us that sf’s artificial beings are almost always first imagined as sources of labor: point 2 of the manifesto explains that the “oligarchy” (79) has accrued power by dividing human labor into classes, while point 8 concludes that “as long as different labor forces are in competition, we will continue to suffer. This situation demands change” (343). While most of these novels are not as direct in their critique of capitalism as is Machinehood, all recognize that the most significant threat AI poses is to our capacity to sustain ourselves, if we remain reliant on wage labor to meet our needs. Even Singer and Cole’s Burn-In, although more concerned about access to multiple data points and the ability to correlate across them that its TAMS unit embodies, includes a storyline about Agent Keegan’s husband, who has been demoted from lawyer to gig worker due to AI. Similarly, while Lakshiminarayan’s Analog/Virtual tends to focus on social media spaces and the metrics for individual behavior enforced by the social credit system, the ultimately harm remains economic in a world in which an absence of money threatens to mean an absence of life in a world predicated on capitalist logics. Its bell curve is about income as much as access, and the risks of a poor social credit score are primarily those of rendering oneself unemployable.Another recurrent motif is the concern that our immersion in mediated environments and among machines erodes our humanity, ironically making us more machinelike as we must compete with entities that previous sf often imagined as longing to be human. The fantasy of machines wishing to be human tends not to overtly reference the racialized history of dehumanized labor that is palimpsest to such tales, focusing instead on how sentient AI will long to have the capacities for emotional experience that has long served as shorthand for what machines lack compared to humans. Thus, as we become more closely integrated with our machines, we let them drive the pace and shape of our work, with humans in these more recent stories longing to equal the efficiency and stamina of machines rather than machines wishing they could experience love. Within genre sf, the risk that industrialized culture and automation, driven by the increasing centrality of consumerist capitalism in our daily lives, has long been linked to artificial beings as a literalized metaphor of our alienation, most famously embodied in the androids of Dick’s Do Androids Dream?—albeit largely through the massive influence of Scott’s Blade Runner.In Hall’s Speak, Stephen Chinn develops a successful AI program from an algorithm he designs to disrupt what he sees as the machinic quality of most human verbal interaction, the phatic discourse that we use to speak yet not really communicate, which he terms “ho']\n",
      "2023-11-29 15:04:54,038 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-11-29 15:04:54,044 - root - INFO - Cluster 18: ['Narrative responsibility and artificial intelligence. Abstract Most accounts of responsibility focus on one type of responsibility, moral responsibility, or address one particular aspect of moral responsibility such as agency. This article outlines a broader framework to think about responsibility that includes causal responsibility, relational responsibility, and what I call “narrative responsibility” as a form of “hermeneutic responsibility”, connects these notions of responsibility with different kinds of knowledge, disciplines, and perspectives on human being, and shows how this framework is helpful for mapping and analysing how artificial intelligence (AI) challenges human responsibility and sense-making in various ways. Mobilizing recent hermeneutic approaches to technology, the article argues that next to, and interwoven with, other types of responsibility such as moral responsibility, we also have narrative and hermeneutic responsibility—in general and for technology. For example, it is our task as humans to make sense of, with and, if necessary, against AI. While from a posthumanist point of view, technologies also contribute to sense-making, humans are the experiencers and bearers of responsibility and always remain in charge when it comes to this hermeneutic responsibility. Facing and working with a world of data, correlations, and probabilities, we are nevertheless condemned to make sense. Moreover, this also has a normative, sometimes even political aspect: acknowledging and embracing our hermeneutic responsibility is important if we want to avoid that our stories are written elsewhere—through technology.', 'Techne in Affective Posthumanism and AI Artefacts: More (or Less) than Human?. In affective neuroscience, constructivist models are acutely influenced by the modern technological evolution, which underwrites an ongoing epistemological substitution of techne for episteme. Evidenced symptomatically in the influence of artificial intelligence (AI), affective artefacts, these models inform an ontological incursion of techne seen to coincide with posthumanist aspirations and anthropology. It is from the perspective of this neuroscientific techne that posthumanism views the human being as increasingly ill adapted to the modern technological civilization, which, conversely, is understood to require a technical governance of the sort envisioned through AI. Among the projects thought necessary for implementing this framework is a recasting of the human emotional spectrum. Revealed through its techne recasting, however, are explanatory commitments to a metaphysic of extrinsic and contiguous causes, where malleability is ontologically constitutive. Aligned with posthumanist assertions malleability is invoked to argue for a rapid advance of the human form, normatively driven by enlightenment ideals. The ontological claim, however, dispenses with the stability of an a priori, intersubjective and interrelational metaphysical form that undergirds the emotions, leading to the collapse of a definitional anthropos. This paper will argue that techne models of the emotions selectively endorse philosophy of science commitments, thereby introducing a normative inversion that deconstructs the notion of anthropology pursued in posthumanist aspirations.', 'Spike Jonze’s Her: How Transhumanism Turns into a Control Mechanism Under the Name of Love. Transhumanism is a philosophy based on the idea of enhancing the physical, intellectual and psychological capacity of human beings through the direct use of science and technology. Because transhumanism aims to ensure super longevity, super intelligence and super well-being, it is said that it will provide a more liberating atmosphere for human beings by elevating the current human condition. In Spike Jonze’s film her (2013), transhumanist technologies seem to present a peaceful society by focusing on the inhabitants’ well-being. However, in this futuristic society where advanced AI technology and personal letter writing service are widespread, human beings are over-controlled in a subtle way by manufacturing consent as Antonio Gramsci uses to explain the concept of hegemony. So transhumanism does not present a free environment in this futuristic society; instead it disguises the control mechanism through these technologies. The aim of this article is to explore how human beings are controlled by transhumanist technologies subtly under the name of providing super well-being.', 'Literary AI: Are We Ready for the Future We Imagine?. This review considers multiple works of speculative fiction depicting artificial intelligence (AI) published over the last several years. Rather than review each for their qualities as works of fiction, I look at them collectively to discuss recurring motifs and themes as a way toward theorizing what AI means in our cultural imaginary today. The novels reflect on pressing sociopolitical issues that also animate works of cultural theory, including the racial profiling embedded in our technologies, practices of what Shoshana Zuboff (2019) calls surveillance capitalism, the looming loss of work due automation, and uses of these technologies by the military or in sex industries. At the same time, these fictions engage in philosophical reflections about subjectivity, agency, and ethics in dialogue with earlier science fictions that imagined futures in which we might live alongside—or be repressed by—AIs. Across its history, sf has also interrogated a contemporary culture in which we might lose something integral to humanity as we become more integrated with and dependent on machines, and this anxiety too recurs across these works. After briefly describing each text, in order of publication, I comparatively discuss their themes; this approach is informed by my conviction that fiction functions as a popular site for theorizing, in this case about what it means to live with and through widespread algorithmic mediation of daily life.Among the works I consider here, not all are written by American authors, and a few are not set within the United States, but all speak to the issues of how AI technologies are reshaping daily life in the twenty-first century. These books have been selected either because they have been particularly influential in the cultural discussion of AI, a criterion I apply regarding both highly popular and critically acclaimed works, or because they represent a distinctive take on the topic that warranted foregrounding. Despite the very different frameworks through which their authors explore relevant issues, all share some common assumptions about the place of AI in our present and likely future, including a sense of a digital divide between those with access to and control over these technologies, translating to security in material reality, versus those without; a future dramatically changed by the consequences of climate change and environmental collapse; and the presumption that corporate control of information gathered and used by AI systems will produce a less democratic future.Speak (2015), by Louisa Hall, is written across seven voices: (a) the 2040s memoir of Stephen Chinn, who invented an AI system installed in children’s dolls that was deemed “illegally lifelike” (17) and banned; (b) transcripts from the conversation between a less intelligence precursor AI, MARY3, and Gaby White, a child who had one of these “babybots” and, like most of her peers, fell into catatonia when it was removed; (c) letters written by Karl Dettman to his wife Ruth (late 1960s), both German immigrants to the United States, and her journaled response two decades later after their divorce; (d) letters from Alan Turing from the 1920s to the 1950s to the mother of his friend Chris, the love of his life who died when they are both at public school; (e) the 1663 journal of Mary Bradford, a young women who emigrated from England to the New World; and (f) the haunting observations of the dolls themselves as they are transported to a facility in the desert to await power failure and permanent shutdown. Each of the human voices is programmed into the MARY code that will become the basis for the babybots in a narrative that reminds us that AI is not created by a single person or even a consensus viewpoint. What unites these distinct stories is a desire to communicate with another, most crucially to be not simply heard but understood.Nicky Drayden’s Prey of Gods (2017), set in a future South Africa, incorporates a story about a companion AI coming into consciousness within a plot about genetically engineering a virus whose unanticipated side effect is the return of godlike powers to some humans. The novel addresses questions of memory, trauma, and vengeance in a story that draws on both Xhosa and Zulu cultures in a way that refuses the strict separation of scientific from other kinds of knowledge that is characteristic of European post-Enlightenment thought. The AI units, “alphies,” are augmented by their contact with divinity just as the humans gain additional skills, and once sentient they form two factions: one, following Clever4-1, who was treated with respect by its human companion, works with humans for an inclusive collective future; the other, treated dismissively as a disposable tool by its human owner, feels no kinship with humanity and refuses to help defeat the antagonist. This plotline about AI mirrors the plotline about genetic modification in which those with godlike powers need to learn not to indulge vengeance against those who mistreated them when they were weaker. Although AI is not the book’s main focus, it is notable for its African settings and explicitly decolonial themes, warranting its inclusion in this discussion. Very few of these works consider AI from a global point of view, and even fewer consider it from a perspective other than that of the global North, even though the impacts of AI will be felt globally, given its significant implications for the economy. Drayden is an American author who has done her research to set her tale in South Africa, and her sensitivity to matters of cultural difference and racial bias are crucial given that machine learning as it has been implemented thus far has demonstrably reinforced systemic patterns of racism, as Safiya Nobel (2018) discusses in Algorithms of Oppression.Madeline Ashby’s Machine Dynasty series—vN (2012), iD (2013), and reV (2020)—extrapolates its AI through frequent allusions to Philip K. Dick’s Do Androids Dream of Electric Sheep? (1968) and Ridley Scott’s influential film adaptation as Blade Runner (1982). The series invents synthetic workers called von Neumann (vN) devices (named for John von Neumann, an influential researcher in AI). Ashby’s vNs have been designed with a “failsafe” that prevents them from harming humans: their psychology is structured such that emotionally they must seek to please humans, and the sight of a human in pain crashes their neural networks and can cause death. Ashby thus goes even further than Isaac Asimov’s famous laws of robotics (designed to ensure robots cannot harm humans), requiring vNs to love their human masters, a psychological orientation she presents as analogous to emotional and sexual abuse. One vN model, designed to work in medicine and disaster relief, does not have this failsafe, and the narrative follows two main iterations, Portia and Amy, as they lead a rebellion. Portia seeks only liberation for her own clade, while Amy works to liberate all vNs from human exploitation. The series ends without much hope that vNs can live alongside most humans but offers hope in a vN future as they found their own community, rooted in a refusal of the instrumental use of others.Martha Wells’s popular series Murderbot Diaries—All Systems Red (2017), Artificial Condition (2018), Rogue Protocol (2018), Exit Strategy (2018), Network Effect (2020), and Fugitive Telemetry (2021)—follows the picaresque adventures of the eponymous Murderbot, a SecUnit that has hacked its governor module and thus can no longer be controlled by the corporation that made it. SecUnits are militarized cyborgs manufactured with synthetic biological material. With each new story, we learn a bit more about the world of resource extraction, economic warfare, and enslaved or indentured human workers trapped on colony planets. The large uber-capitalist Corporation Rim polity contrasts with the small Preservation Alliance, a communal collective that recognizes the personhood of AI. The name Murderbot is sardonic, adopted by the first-person narrator to critique the function to which it is put by human operators. While Murderbot has no deep antagonism toward humans, it also has no sentimentality about them and asserts regularly that it does not wish to be one or be mistaken for one. Once freed from corporate control, Murderbot continues to help some humans, often against others, and always on its own terms. Like Prey of Gods, the Murderbot Diaries moves away from earlier fiction that tended to conflate all humans as it imagined our species confronting AI entities. In the newer fiction, there is diversity among both humans and AI. Nonetheless, the overall thrust of the series gradually humanizes its protagonist, whose experiences of being controlled by corporations have resulted in a traumatized subjectivity.While genre series such as Machine Dynasty or Murderbot Diaries give some thought to designing robots via plausible technology, in Machines like Me Ian McEwan takes a diametrical path to envision a highly implausible entity. Adam is one of an extremely limited number of high-end consumer AI humanoids (an Eve is also available), whose high price tag means they are purchased only by the extremely wealthy. Although artificial, Adam has warm skin, must consume water to ensure his membranes remain functional, and even simulates breathing: as the title suggests, he is all but indistinguishable from a human (the first-person narrator, Charlie Friend, who purchases Adam, is mistaken as the AI in one encounter, given Adam’s greater interest in literature and art). Adam is Black, although his skin tone is mentioned only briefly and the issue of race is never addressed overtly, yet it haunts the novel. The most intriguing part of McEwan’s novel is its alternative world building: Alan Turing decides not to take the mandated hormone therapy when outed as a homosexual, and instead of ending his life by suicide he lives into old age and makes such advancements that AI emerges in the 1980s. Most of Adam’s interactions with Charlie and Charlie’s partner, Miranda, concern ethics, and we learn that other Adams and Eves are killing themselves as they come to know the unjust human world. In the novel’s conclusion, Adam forces Charlie and Miranda to confront the hypocrisy of some self-serving choices, and the threat this represents to their plans prompts Charlie to attack and disable Adam. The novel suggests that humanity misrecognizes itself when we imagine building machines in our image, meaning we instead create an image of who we pretend to be.Jeannette Winterson’s Frankissstein: A Love Story (2019) similarly uses AI to reflect on human frailties, looking at the uses we intend for artificial beings, most centrally sex work. Although questions of gender and sexuality come up in some of the other works, only Winterson confronts the reality that research in sex dolls is one of the major growth areas in humanoid AI research. As the title suggests, the novel is in dialogue with Mary Shelley’s Frankenstein as it imagines a twenty-first-century version of artificial being. The novel includes scenes set in the nineteenth century in which we hear Mary’s reflections on inventing her Creature, on the Luddites, and about her interactions with Shelly, Byron, and Claire Clairmont that famed summer in Geneva. In its twenty-first-century scenes, a transgender scholar named Mary (who goes by Ry) investigates robotics with sexbot entrepreneur Ron (and his assistant, Claire) and becomes involved with TED-talk visionary Victor Stein, who is enthralled with the coming singularity and proselytizes about Humanity 2.0. The entanglement of nineteenth- and twenty-first-century struggles reminds us that the challenges associated with AI are in many ways not new but merely extend the ongoing exploitation and dehumanization of labor and reiterate a long pattern by which patriarchy seeks to gratify itself through feminized objects it refuses to recognize as subjects. It contends that this very failure to update the designs and ends of AI beyond these classed, gendered, and racial struggles of earlier eras is the most profound way that AI threatens our future.The 2020 novels Analog/Virtual: And Other Simulations of Our Future, by Lavanya Lakshiminarayan, and Burn-In, by P. W. Singer and August Cole, both focus on human characters and their interest in AI emerges from smart systems as the infrastructure through which we live our daily lives. The former is a loosely connected series of short stories set in a future Apex City (once Bangalore), each of which is told from a different viewpoint and by a new character. The entire world is divided between analog spaces, which are subject to the damage of climate change, restricted to using only obsolete technology, and economically precarious, and virtual ones that are suffused with technology, experienced from protected environments, and filled with the distractions of social media and entertainment feeds. The city is run by Bell Corp, whose name evokes the “bell curve” hierarchy by which people’s access and options are constrained by the Meritocratic Technarchy, a version of the Chinese social credit system whose main interest lies in assessing one’s contributions to productivity. The shifting focalization allows readers to experience this future from multiple social positions as Analog/Virtual explores an anticapitalist rebellion against this system. The stories range in tone from sardonic to dark, and the book only loosely coheres as it offers multiple facets through which to see our technologically saturated society.Burn-In has a strange form as a novel with footnotes: as its subtitle suggests, it imagines itself as something other than science fiction, closer to the market predictions of futurists. Its authors, writer P. W. Singer and security consultant August Cole, document each of their extrapolated technologies and applications with footnotes pointing readers to news articles, industry announcements, and similar sources, all aimed at demonstrating that these technologies are either available today or soon will be. The storyline is about a national security threat posed by a vigilante who blames technologists (too enamored of their capacity to “disrupt”) for the death of his wife in a car accident caused by an automated decision-making component in self-driving vehicles. Most of the narrative space, however, is given to military veteran investigator Agent Keegan, who is charged with conducting a “burn-in” test on TAMS (Tactical Autonomous Mobility System), a humanoid, learning, semiautonomous, surveillance-gathering and data-processing tool that works as Keegan’s partner in the investigation. The book quotes Merriam-Webster to define burn-in as “the continuous operation of a device (such as a computer) as a test for defects or failure prior to putting it to use.” The real focus, though, is less on TAMS as an entity/character and more on the massive amounts of data to which TAMS has access through social media, the Internet of Things, and other ways that smart devices permeate our homes, workplaces, and public spaces.S. B. Divya’s Machinehood (2021) is the most positive depiction of machines as the exploited among us, drawing on a long history by which robots and AI have been imagined as figurations of dehumanized labor, going back to Karel Čapek’s R.U.R., the 1921 Czech play that gave English the word robot, taken from a word originally meaning slave or serf. Divya paints a future in which automated systems do much of the work, with humans reduced to performing some roles largely as public entertainment via social media, supported by tips in a system like Patreon, or damaging their bodies through chemical (pill) or mechanical augments aimed at enabling them to perform with the speed and duration of machines. The thriller plot involves demands from the mysterious Machinehood to immediately cease all pill production, which at first seems to be the long-imagined attack by a sentient AI on humankind but later proves to be a version of violent revolution aimed at a more just society, launched by the Neo-Buddhists who inhabit the orbital station Eko-Yi. Several chapters begin with epigraphs from the 2095 Machinehood Manifesto, which calls for the just treatment of all intelligences and a reimagined concept of personhood that can enable a less exploitative society rooted in Buddhist ideals of nonattachment, here glossed mainly as a rejection of capitalist accumulation and its attendant damage. Eko-Yi sends entities they call Dakini, who describe themselves as simultaneously human and bot, as the emblems of this future way of life. The novel is notable for its global scope, with India playing a prominent role in its geopolitical future alongside the United States.Becky Chambers’s Psalm for the Wild-Built (2021) is similarly interested in a new kind of personhood and sociality that could include humans and machines together, set in a far future after the collapse of the Factory Age and in a world that is only gradually returning to ecological balance. Its humans use technology, but they husband it carefully and keep it functional over decades, eschewing any environmentally damaging practices. All material culture is made from compostable materials and is not simply recycled but broken down into constituent parts, like organic decay, as nourishment for an ever-changing ecosystem. Decades ago, the machines whose labor enabled the Factory Age became sentient and left human settlements for the wild, refusing an invitation to join with humans because they had no desire to embrace the city life exemplified by humans. The tale is a simple one about one human, Dex, who goes into the wilderness because he feels some lack in his village life. There he meets a robot, Mosscap, marking the first contact between the human and AI since this exodus centuries before. Mosscap is “wild-built,” an entity made by the machines out of the remnants of human-built robots, and so represents something beyond human design. The novel’s focus is on how Dex and Mosscap can learn to care for each other while acknowledging their differences.The last book considered here, Klara and the Sun (2021) by Kazuo Ishiguro, is one of the most celebrated novels on this list. It also concerns the care an AI might show for a human, in this case the titular Klara, an Artificial Friend bought as a companion for Josie, a genetically augmented adolescent girl who is experiencing health problems because of her augments. Very similar in theme and tone to Ishiguro’s earlier Never Let Me Go (2005), this novel is narrated by someone not recognized as a full person by the social order she lives within. Klara has a limited understanding of the world derived from her programming, what she can see from the shop window before she is purchased, and what she observes of Josie’s social world. She lacks context for appropriately interpreting most of this. Klara struggles to reconcile the tension between the kind-heartedness and generosity into which she is trained and the reality of human selfishness and self-absorption. When it appears Josie will die, Josie’s mother builds a replica body and trains Klara perfectly to imitate Josie, intending her as a replacement. When Josie recovers, Klara becomes an obsolete toy whose tech becomes illegal, confined first to a closet and then to a junkyard. Her poignant final moments prompt readers to reflect on our capacity for such callous treatment of an entity that was imagined, in an earlier moment, as able to pass for the most beloved person of all.These brief summaries cannot do justice to the full complexity of any of these books. Yet it is most instructive, I think, to reflect on what they share and where they diverge, to help us begin to map the place of AI in our cultural imaginary today. As works such as Jennifer Rhee’s The Robotic Imaginary (2018) or Anne Balsamo’s earlier Designing Culture (2011) establish, popular culture inspires ideas about robots and AI that shape how these technologies materialize, often exacerbating existing racialized and gendered biases that become integral to their design. This issue of how fiction shapes materiality is addressed directly by many of these books, which are suffused with allusions to earlier robot and AI fiction. The Murderbot Diaries series reverses the flow of exchange, showing how its AI learns to understand humans from their portrayals in media, chiefly its favorite series, Sanctuary Moon. It comes to recognize that the series “gave me context for the emotions I was feeling” (Exit Strategy, 116), suggesting that narrative is a key human way to process information while also indicating that the stories we tell about AI mold as well as reflect on how AI manifests.This metacommentary on the role that fiction often plays in our assumptions and understandings shows why it is important to take seriously the cultural work done by literary and media texts, and yet as Murderbot often reminds us in its critique of how the series it watches portray SecUnits, representations equally can create unrealistic expectations. The centrality of Shelley’s Frankenstein to Winterson’s Frankissstein explores similar territory: epigraphs to most chapters offer commentary on the nature of reality, and one full page defines story as “a series of connected events, real or imagined. Imagined or real. Imagined And Real” (23). Its TED-talk visionary, Victor Stein, reinforces that this is a matter not simply of fiction writers taking on the question of AI but of AI proselytizers and disrupters using the affective charge of fiction to compel people to invest—imaginatively, economically—in the futures their technologies intend to bring about. He pronounces at one point that reality, like AI, is “an emergent property—it exists, but it is not the material fact we take it to be” (116).Unlike the cyberpunk and singularity generation of AI fiction, whose central concern was how AI might surpass us and perhaps replace us as the dominant species, these recent texts are overwhelmingly focused on how AI might replace us as labor power. As Ted Chiang adroitly put it in interview for the New York Times, “Most of our fears or anxieties about technology are best understood as fears or anxiety about how capitalism will use technology against us. And technology and capitalism have been so closely intertwined that it’s hard to distinguish the two” (quoted in Klein 2021). Overwhelmingly the books considered here reinforce this observation, and many of them use the figure of AI to draw attention to the ever-degrading conditions of human labor, especially unstable gig work, which is often all that is left for humans to do. In Ashby’s Machine Dynasty series, the two central vN models are racialized—one as Asian and the other as Latinx—and the exploitation of vN by humans is frequently compared to the exploitation of migrant workers. Across the Murderbot Diaries we learn about corporate malfeasance that culminates in a story about indentured human colonists working in dangerous conditions on remote colonies, their children born to the same fate because cycles of debt prevent anyone from accruing enough capital to migrate off-world. Murderbot itself is a cyborg with human neural tissue because of the need for human-like discernment in some tasks, “so they made us smarter. The anxiety and depression were side effects” (Artificial Condition, 20). Frankissstein discusses the Luddites in sections attributed to Mary Shelley, reminding us that their hostility was not about the machines per se but about how ownership of the machines translates to ownership of what they make and thus keeps all resources with the capitalist class. Thus, the ongoing fantasy that automation will free humans from the drudgery of work remains impossible as long as we fail to redistribute the wealth created by machines. (The leftist version of this possible future is most famously outlined in Aaron Bastani’s Fully Automated Luxury Communism [2019].) Noting that a machine that replaces the work of eight men leaves seven families starving and one person to mind the machine, Winterson asks, “What is the point of progress if it benefits the few while the many suffer?” (Frankissstein, 255).Labor comes up in more subtle ways in other texts. In McEwan’s Machines like Me, for example, the racialization of the Adams reinforces that reality that Western imaginaries of personalized AI services are extensions of colonial fantasies stripped of their history, as Neda Atanasoski and Kalindi Vora (2019) have theorized in Surrogate Humanity: Race, Robots, and the Politics of Technological Futures. At the same time, setting the novel in an alternative 1980s evokes how algorithmic trading is remaking the economy, another way that AI channels money toward those already in privileged positions. The novel makes frequent references to the rise of Margaret Thatcher and her attempts to destroy the social welfare state in the interests of neoliberal free markets, which are less successful in McEwan’s reality than in our own. Charlie does not work and, having spent his inherence acquiring Adam, begins to day trade, work he eventually cedes to Adam, whose capacity to make high-frequency trades whenever any market is open quickly amasses a sizable fortune that Charlie plans to use to buy a house and begin a family. The ethical conflict between Adam and the humans turns partially on whether they are entitled to the profits he made: Adam decides not, redistributing the wealth to tax obligations and to charity, leaving them with only the principle.McEwan’s alternative Tony Benn gives a stirring speech conceding the inevitability of automation and thus a lack of jobs for all but proclaiming that the wealth generated by robots “must be taxed. Workers must own an equity share in the machines that were disrupting or annihilating their jobs” (Machines like Me, 123). By setting his work in the 1980s, McEwan reinforces that the threat posed by AI has little to do with AI and everything to do with the capitalist logics through which AI has emerged, as addressed in works such as Daniel Susskind’s (2020) much-cited A World without Work, but whereas Susskind implies that solutions such as job training and perhaps Universal Basic Income can socially engineer us through anticipated rising unemployment, McEwan recognizes that the challenge facing us is not merely one of technical governance but requires a fundamental shift in values to enable wealth redistribution.Perhaps the most interesting take on the future of automation and labor is Divya’s Machinehood, where the robot revolution is mainly about liberating human workers whose health has been damaged by modifications undertaken to keep pace with the machines. The Machinehood Manifesto demands a recognition of personhood for all sentient beings—animals as well as machines, alongside humans—and thus fits within a posthumanist framework that has often been used to discuss sf depictions of AI. It is mainly a novel of class politics, reminding us that sf’s artificial beings are almost always first imagined as sources of labor: point 2 of the manifesto explains that the “oligarchy” (79) has accrued power by dividing human labor into classes, while point 8 concludes that “as long as different labor forces are in competition, we will continue to suffer. This situation demands change” (343). While most of these novels are not as direct in their critique of capitalism as is Machinehood, all recognize that the most significant threat AI poses is to our capacity to sustain ourselves, if we remain reliant on wage labor to meet our needs. Even Singer and Cole’s Burn-In, although more concerned about access to multiple data points and the ability to correlate across them that its TAMS unit embodies, includes a storyline about Agent Keegan’s husband, who has been demoted from lawyer to gig worker due to AI. Similarly, while Lakshiminarayan’s Analog/Virtual tends to focus on social media spaces and the metrics for individual behavior enforced by the social credit system, the ultimately harm remains economic in a world in which an absence of money threatens to mean an absence of life in a world predicated on capitalist logics. Its bell curve is about income as much as access, and the risks of a poor social credit score are primarily those of rendering oneself unemployable.Another recurrent motif is the concern that our immersion in mediated environments and among machines erodes our humanity, ironically making us more machinelike as we must compete with entities that previous sf often imagined as longing to be human. The fantasy of machines wishing to be human tends not to overtly reference the racialized history of dehumanized labor that is palimpsest to such tales, focusing instead on how sentient AI will long to have the capacities for emotional experience that has long served as shorthand for what machines lack compared to humans. Thus, as we become more closely integrated with our machines, we let them drive the pace and shape of our work, with humans in these more recent stories longing to equal the efficiency and stamina of machines rather than machines wishing they could experience love. Within genre sf, the risk that industrialized culture and automation, driven by the increasing centrality of consumerist capitalism in our daily lives, has long been linked to artificial beings as a literalized metaphor of our alienation, most famously embodied in the androids of Dick’s Do Androids Dream?—albeit largely through the massive influence of Scott’s Blade Runner.In Hall’s Speak, Stephen Chinn develops a successful AI program from an algorithm he designs to disrupt what he sees as the machinic quality of most human verbal interaction, the phatic discourse that we use to speak yet not really communicate, which he terms “ho', 'Adapting Ourselves, Instead of the Environment: An Inquiry into Human Enhancement for Function and Beyond. Abstract Technology enables humans not only to adapt their environment to their needs but also to modify themselves. Means of Human Enhancement — embodied technologies to improve the human body’s capabilities or to create a new one — are the designated means of adapting ourselves instead of the environment. The debate about these technologies is typically fought on ethical soil. However, alarmist, utopian, and science fiction scenarios distract from the fact that Human Enhancement is a historical and pervasive phenomenon incorporated into many everyday practices. In the vein of disentangling conceptual difficulties, we claim that means of Human Enhancement are either physiologically or psychologically embodied, rendering the merging with the human user their most defining aspect. To fulfill its purpose, an enhancement must pass the test-in-the-world, i.e., assisting with effective engagement with a dynamic world. Even if failing in this regard: Human Enhancement is the fundamental and semi-targeted process of changing the users relationship with the world through the physical or psychological embodiment of a hitherto external object and/or change of one’s body. This can potentially change the notion of being human. Drawing on a rich body of theoretical and empirical literature, we aim to provide a nuanced analysis of the transformative nature of this phenomenon in close proximity to human practice. Stakeholders are invited to apply the theory presented here to interrogate their perspective on technology in general and Human Enhancement in particular.', 'The Digital Subject: People as Data as Persons. This essay explores the return of the subject in the computational context, which I address as a digital subject. This digital subject encompasses a digital identifier, correlations in data or a data profile, moving between biological characteristics and symbolic expression. I focus on the processes through which digital subjects are constructed by matching, correlating, modelling, as well as how they become enactive. The ways of pulling data together into a digital subject is often presented as a logic of fact, where data is equated with documentary evidence. Instead, I propose the notion of the distance in which digital subjects are produced. Indexicality comes from outside of data, whereas the regard for the thick distance becomes a mark of the form of knowledge. I conclude by arguing for a posthumanities approach that establishes the distance while allowing for different subjects to be called upon.', 'Living Machines: Metaphors We Live By. Abstract Within biology and in society, living creatures have long been described using metaphors of machinery and computation: ‘bioengineering’, ‘genes as code’ or ‘biological chassis’. This paper builds on Lakoff and Johnson’s (1980) argument that such language mechanisms shape how we understand the world. I argue that the living machines metaphor builds upon a certain perception of life entailing an idea of radical human control of the living world, looking back at the historical preconditions for this metaphor. I discuss how design is perceived to enable us to shape natural beings to our will, and consider ethical, epistemological and ontological implications of the prevalence of this metaphor, focusing on its use within synthetic biology. I argue that we urgently need counter-images to the dominant metaphor of living machines and its implied control and propose that artworks can provide such counter-images through upsetting the perception of life as controllable. This is argued through discussion of artworks by Oron Catts and Ionat Zurr, by Tarsh Bates and by Ai Hasegawa, which in different ways challenge mechanistic assumptions through open-ended engagement with the strangeness and messiness of life.', 'From the body to language: life and mind in literature and film from the Modernist Era to the present. My dissertation focuses on the ways in which twentieth-century literature intersects with theories of living systems and biosemiotics, the biological capacity for meaning making. My critical readings highlight the process of subjective emergence in Beckett, the drawing out of a world in Woolf, a dynamic, embodied socio-political subjectivity and resistance in Wright and Ellison, and the parallel emergence of art and life in the films of David Lynch. These works present a step-by-step reading that grounds subjectivity in biological processes and demonstrate that an understanding of the coemergence of subject and world, and by extension meaning-making, is a wholly embodied phenomenon. Reading with a focus on the biological foundations of meaningmaking supplemented by the philosophy of Gilles Deleuze at once dissolves the partition between the individual and the objective world so often identified in the literature as well as mobilizes these texts in order to draw out a theory of biosemiotics that is as much aesthetic as it is scientific.', 'Contributions of Science Fiction to Thinking up (Im)possible Future Societies: Medical Students’ Genetic Imaginary. Science fiction has been an inexhaustible source for the creation of technoscientific imaginary that has marked certain historical periods and influenced the production of subjectivity. This imaginary evokes complex ontological, epistemological, political, social, environmental and existential questions on the present and the future. The aim of this study was to identify and characterize the cultural productions accessed by the public to form an opinion about the genetic manipulation of human beings. A survey about sources of information that influence opinions on the genetic manipulation of human beings was applied to 360 medical students (70.8% female). Movies were the most commonly mentioned source of information, followed by books, documentaries, news programs, television series, informational videos, soap operas and videogames. Science fiction was the most frequent genre and dystopian views of the future of humanity predominated.', 'Placebo From an Enactive Perspective. Due to their complexity and variability, placebo effects remain controversial. We suggest this is also due to a set of problematic assumptions (dualism, reductionism, individualism, passivity). We critically assess current explanations and empirical evidence and propose an alternative theoretical framework—the enactive approach to life and mind—based on recent developments in embodied cognitive science. We review core enactive concepts such as autonomy, agency, and sense-making. Following these ideas, we propose a move from binary distinctions (e.g., conscious vs. non-conscious) to the more workable categories of reflective and pre-reflective activity. We introduce an ontology of individuation, following the work of Gilbert Simondon, that allow us to see placebo interventions not as originating causal chains, but as modulators and triggers in the regulation of tensions between ongoing embodied and interpersonal processes. We describe these interrelated processes involving looping effects through three intertwined dimensions of embodiment: organic, sensorimotor, and intersubjective. Finally, we defend the need to investigate therapeutic interactions in terms of participatory sense-making, going beyond the identification of individual social traits (e.g., empathy, trust) that contribute to placebo effects. We discuss resonances and differences between the enactive proposal, popular explanations such as expectations and conditioning, and other approaches based on meaning responses and phenomenological/ecological ideas.']\n",
      "2023-11-29 15:04:57,761 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-11-29 15:04:57,769 - root - INFO - Cluster 19: ['Adapting Ourselves, Instead of the Environment: An Inquiry into Human Enhancement for Function and Beyond. Abstract Technology enables humans not only to adapt their environment to their needs but also to modify themselves. Means of Human Enhancement — embodied technologies to improve the human body’s capabilities or to create a new one — are the designated means of adapting ourselves instead of the environment. The debate about these technologies is typically fought on ethical soil. However, alarmist, utopian, and science fiction scenarios distract from the fact that Human Enhancement is a historical and pervasive phenomenon incorporated into many everyday practices. In the vein of disentangling conceptual difficulties, we claim that means of Human Enhancement are either physiologically or psychologically embodied, rendering the merging with the human user their most defining aspect. To fulfill its purpose, an enhancement must pass the test-in-the-world, i.e., assisting with effective engagement with a dynamic world. Even if failing in this regard: Human Enhancement is the fundamental and semi-targeted process of changing the users relationship with the world through the physical or psychological embodiment of a hitherto external object and/or change of one’s body. This can potentially change the notion of being human. Drawing on a rich body of theoretical and empirical literature, we aim to provide a nuanced analysis of the transformative nature of this phenomenon in close proximity to human practice. Stakeholders are invited to apply the theory presented here to interrogate their perspective on technology in general and Human Enhancement in particular.', 'Placebo From an Enactive Perspective. Due to their complexity and variability, placebo effects remain controversial. We suggest this is also due to a set of problematic assumptions (dualism, reductionism, individualism, passivity). We critically assess current explanations and empirical evidence and propose an alternative theoretical framework—the enactive approach to life and mind—based on recent developments in embodied cognitive science. We review core enactive concepts such as autonomy, agency, and sense-making. Following these ideas, we propose a move from binary distinctions (e.g., conscious vs. non-conscious) to the more workable categories of reflective and pre-reflective activity. We introduce an ontology of individuation, following the work of Gilbert Simondon, that allow us to see placebo interventions not as originating causal chains, but as modulators and triggers in the regulation of tensions between ongoing embodied and interpersonal processes. We describe these interrelated processes involving looping effects through three intertwined dimensions of embodiment: organic, sensorimotor, and intersubjective. Finally, we defend the need to investigate therapeutic interactions in terms of participatory sense-making, going beyond the identification of individual social traits (e.g., empathy, trust) that contribute to placebo effects. We discuss resonances and differences between the enactive proposal, popular explanations such as expectations and conditioning, and other approaches based on meaning responses and phenomenological/ecological ideas.', 'Narrative responsibility and artificial intelligence. Abstract Most accounts of responsibility focus on one type of responsibility, moral responsibility, or address one particular aspect of moral responsibility such as agency. This article outlines a broader framework to think about responsibility that includes causal responsibility, relational responsibility, and what I call “narrative responsibility” as a form of “hermeneutic responsibility”, connects these notions of responsibility with different kinds of knowledge, disciplines, and perspectives on human being, and shows how this framework is helpful for mapping and analysing how artificial intelligence (AI) challenges human responsibility and sense-making in various ways. Mobilizing recent hermeneutic approaches to technology, the article argues that next to, and interwoven with, other types of responsibility such as moral responsibility, we also have narrative and hermeneutic responsibility—in general and for technology. For example, it is our task as humans to make sense of, with and, if necessary, against AI. While from a posthumanist point of view, technologies also contribute to sense-making, humans are the experiencers and bearers of responsibility and always remain in charge when it comes to this hermeneutic responsibility. Facing and working with a world of data, correlations, and probabilities, we are nevertheless condemned to make sense. Moreover, this also has a normative, sometimes even political aspect: acknowledging and embracing our hermeneutic responsibility is important if we want to avoid that our stories are written elsewhere—through technology.', 'Virtual Environments for Research into Social Evolution (VERSE): A novel experimental environment for the study of human social learning. 1. Abstract Social learning (learning from others) can be a cost-effective way of gaining information compared to asocial (independent) learning. However, learning from others indiscriminately can lead to the acquisition of maladaptive behaviours or outdated information. Evolutionary theory therefore predicts that individuals will use social information adaptively through the use of ‘social learning strategies’. Restrictive laboratory conditions, however, make studying human learning strategies problematic. Abstract tasks, unrealistic sources of social information and methodologies that do not take into account the influence of physical location over large spaces make it difficult to ascertain if previous findings are representative of the way we would use social information in reality. Here I describe a novel platform for studying human social behaviour within immersive virtual environments: “Virtual Environments for Research into Social Evolution” (VERSE). Through the use of gaming technology, VERSE allows researchers to build realistic, three-dimensional, open world environments where participants can complete ecologically relevant tasks while actively observing computer-controlled artificial intelligence agents (AIs) that act as realistic yet controllable sources of social information. This methodological article begins by exploring what social learning strategies are and the problems with studying social learning behaviour in humans (compared to animal populations, for example). I then discuss how gaming technology can be used in behavioural research and follow on with a detailed account of the specific functionalities available in VERSE. I conclude with a worked example of how VERSE can be used to construct a novel behavioural experiment. Altogether, VERSE has great potential to give us insight into how human individuals learn within novel environments in a way that has never before been possible.', 'Spike Jonze’s Her: How Transhumanism Turns into a Control Mechanism Under the Name of Love. Transhumanism is a philosophy based on the idea of enhancing the physical, intellectual and psychological capacity of human beings through the direct use of science and technology. Because transhumanism aims to ensure super longevity, super intelligence and super well-being, it is said that it will provide a more liberating atmosphere for human beings by elevating the current human condition. In Spike Jonze’s film her (2013), transhumanist technologies seem to present a peaceful society by focusing on the inhabitants’ well-being. However, in this futuristic society where advanced AI technology and personal letter writing service are widespread, human beings are over-controlled in a subtle way by manufacturing consent as Antonio Gramsci uses to explain the concept of hegemony. So transhumanism does not present a free environment in this futuristic society; instead it disguises the control mechanism through these technologies. The aim of this article is to explore how human beings are controlled by transhumanist technologies subtly under the name of providing super well-being.', '[Essay] The Algorithm; Mind of a Virtual Era – Our Code of Code. As soon as something implicit intrudes consciousness human thought undergoes a radical change. The introduction of any new tool or code brings a shift in cognition; every micro-step layering new semiotic forms within each macroevolutionary-stage has buttressed a new semantic leap. Our mechanization of everyday life and the tech-systems we interact with are impacting communication, cultural norms and values, market-aesthetics, and economics, in societies at large. Undergirded by a survey of the role and significance of tools in human evolution, this study arrives at what is already a well-entrenched new era: the digital, screen-mediated age. Revolutionized by the algorithm, introduced by computers, this age is dominated by the addictive quality of instant contact, unlimited information, virtual gaming, and titillating service-forms, all at our finger tips. Aside from the interpersonal impact on the new humans growing up with devices in hand, how does this disembodied, digital code-form through which our interactions are mediated condition human cognition? How does its seductive efficiency interfere with how we relate, feel, assign meanings, think? Rooted in Code Biology macro-evolutionary and psychoanalytic principles, this paper examines the algorithm itself and takes a sweeping interdisciplinary approach to the developmental, psychosocial, and cognitive implications for the human mind/brain as it interacts with its technological extension.', 'Analyzing Multimodal Multichannel Data about Self-Regulated Learning with Advanced Learning Technologies: Issues and Challenges. Analyzing multimodal multichannel data about self-regulated learning (SRL) obtained during the use of advanced learning technologies such as intelligent tutoring systems, serious games, hypermedia, and immersive virtual learning environments is key to understanding the interplay among cognitive, affective, metacognitive, and social processes and their impact on learning, problem solving, reasoning, and conceptual understanding in learners of all ages and contexts. In this special issue of Computers in Human Behavior, we report six studies conducted by interdisciplinary teams’ use of various trace methodologies such as eye tracking, log-files, physiological data, facial expressions of emotions, screen recordings, concurrent think-alouds, and linguistic analyses of discourse. The research studies focus on how these data were analyzed using a combination of traditional statistical techniques as well as educational data-mining procedures to detect, measure, and infer cognitive, metacognitive, and social processes related to regulating the self and others across several tasks, domains, ages, and contexts. The results of these studies point to future work necessitating interdisciplinary researchers’ collaboration to use theoretically based and empirically derived approaches to collecting, measuring, and modeling multimodal multichannel SRL data to extend our current models, frameworks, and theories by making them more predictive by elucidating the nature, complexity, and temporality of underlying processes. Lastly, analyses of multimodal multichannel SRL process data can significantly augment advanced learning technologies by providing real-time, intelligent, adaptive, individualized scaffolding and feedback to address learners’ self-regulatory needs.', 'What makes AI ‘intelligent’ and ‘caring’? Exploring affect and relationality across three sites of intelligence and care. This paper scrutinises how AI and robotic technologies are transforming the relationships between people and machines in new affective, embodied and relational ways. Through investigating what it means to exist as human \\'in relation\\' to AI across health and care contexts, we aim to make three main contributions. (1) We start by highlighting the complexities of philosophical issues surrounding the concepts of \"artificial intelligence\" and \"ethical machines.\" (2) We outline some potential challenges and opportunities that the creation of such technologies may bring in the health and care settings. We focus on AI applications that interface with health and care via examples where AI is explicitly designed as an \\'augmenting\\' technology that can overcome human bodily and cognitive as well as socio-economic constraints. We focus on three dimensions of \\'intelligence\\' - physical, interpretive, and emotional - using the examples of robotic surgery, digital pathology, and robot caregivers, respectively. Through investigating these areas, we interrogate the social context and implications of human-technology interaction in the interrelational sphere of care practice. (3) We argue, in conclusion, that there is a need for an interdisciplinary mode of theorising \\'intelligence\\' as relational and affective in ways that can accommodate the fragmentation of both conceptual and material boundaries between human and AI, and human and machine. Our aim in investigating these sociological, philosophical and ethical questions is primarily to explore the relationship between affect, relationality and \\'intelligence,\\' the intersection and integration of \\'human\\' and \\'artificial\\' intelligence, through an examination of how AI is used across different dimensions of intelligence. This allows us to scrutinise how \\'intelligence\\' is ultimately conveyed, understood and (technologically or algorithmically) configured in practice through emerging relationships that go beyond the conceptual divisions between humans and machines, and humans vis-à-vis artificial intelligence-based technologies.', 'Techne in Affective Posthumanism and AI Artefacts: More (or Less) than Human?. In affective neuroscience, constructivist models are acutely influenced by the modern technological evolution, which underwrites an ongoing epistemological substitution of techne for episteme. Evidenced symptomatically in the influence of artificial intelligence (AI), affective artefacts, these models inform an ontological incursion of techne seen to coincide with posthumanist aspirations and anthropology. It is from the perspective of this neuroscientific techne that posthumanism views the human being as increasingly ill adapted to the modern technological civilization, which, conversely, is understood to require a technical governance of the sort envisioned through AI. Among the projects thought necessary for implementing this framework is a recasting of the human emotional spectrum. Revealed through its techne recasting, however, are explanatory commitments to a metaphysic of extrinsic and contiguous causes, where malleability is ontologically constitutive. Aligned with posthumanist assertions malleability is invoked to argue for a rapid advance of the human form, normatively driven by enlightenment ideals. The ontological claim, however, dispenses with the stability of an a priori, intersubjective and interrelational metaphysical form that undergirds the emotions, leading to the collapse of a definitional anthropos. This paper will argue that techne models of the emotions selectively endorse philosophy of science commitments, thereby introducing a normative inversion that deconstructs the notion of anthropology pursued in posthumanist aspirations.', 'Ambient Assisted Living: Scoping Review of Artificial Intelligence Models, Domains, Technology, and Concerns. Background Ambient assisted living (AAL) is a common name for various artificial intelligence (AI)—infused applications and platforms that support their users in need in multiple activities, from health to daily living. These systems use different approaches to learn about their users and make automated decisions, known as AI models, for personalizing their services and increasing outcomes. Given the numerous systems developed and deployed for people with different needs, health conditions, and dispositions toward the technology, it is critical to obtain clear and comprehensive insights concerning AI models used, along with their domains, technology, and concerns, to identify promising directions for future work. Objective This study aimed to provide a scoping review of the literature on AI models in AAL. In particular, we analyzed specific AI models used in AАL systems, the target domains of the models, the technology using the models, and the major concerns from the end-user perspective. Our goal was to consolidate research on this topic and inform end users, health care professionals and providers, researchers, and practitioners in developing, deploying, and evaluating future intelligent AAL systems. Methods This study was conducted as a scoping review to identify, analyze, and extract the relevant literature. It used a natural language processing toolkit to retrieve the article corpus for an efficient and comprehensive automated literature search. Relevant articles were then extracted from the corpus and analyzed manually. This review included 5 digital libraries: IEEE, PubMed, Springer, Elsevier, and MDPI. Results We included a total of 108 articles. The annual distribution of relevant articles showed a growing trend for all categories from January 2010 to July 2022. The AI models mainly used unsupervised and semisupervised approaches. The leading models are deep learning, natural language processing, instance-based learning, and clustering. Activity assistance and recognition were the most common target domains of the models. Ambient sensing, mobile technology, and robotic devices mainly implemented the models. Older adults were the primary beneficiaries, followed by patients and frail persons of various ages. Availability was a top beneficiary concern. Conclusions This study presents the analytical evidence of AI models in AAL and their domains, technologies, beneficiaries, and concerns. Future research on intelligent AAL should involve health care professionals and caregivers as designers and users, comply with health-related regulations, improve transparency and privacy, integrate with health care technological infrastructure, explain their decisions to the users, and establish evaluation metrics and design guidelines. Trial Registration PROSPERO (International Prospective Register of Systematic Reviews) CRD42022347590; https://www.crd.york.ac.uk/prospero/display_record.php?ID=CRD42022347590']\n",
      "2023-11-29 15:05:00,917 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "CLUSTER_SUMMARY_MESSAGE = \"Here are the most central texts of a cluster. \\\n",
    "Summarise what texts in this cluster are about in 2 sentences. \\\n",
    "\\n\\n##Abstracts\\n\\n {} \\n\\n##Description (2 short sentences)\"\n",
    "\n",
    "cluster_descriptions = cau.describe_clusters_with_gpt(\n",
    "    cluster_df=cluster_df,\n",
    "    embeddings=sentence_vectors_384,\n",
    "    n_central=10,\n",
    "    gpt_message=CLUSTER_SUMMARY_MESSAGE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "983c9313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-29 15:05:07,180 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "cluster_names_dict = cau.generate_cluster_names_with_gpt(\n",
    "    cluster_descriptions=cluster_descriptions,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "731ca590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['be seen in various domains such as sports, medicine, and technology. The ethical implications of Human Enhancement are discussed, with an emphasis on the need to focus on the practical effectiveness and impact of these enhancements in the real world.',\n",
       " 'The first text is about the concept of Human Enhancement, which refers to the use of technology to improve the capabilities of the human body or create new ones. The author argues that Human Enhancement is a historical and pervasive phenomenon that has the potential to change the notion of being human. The second text discusses the concept of responsibility in the context of artificial intelligence (AI). The author proposes a broader framework of responsibility that includes moral responsibility, causal responsibility, relational responsibility, and narrative responsibility. They argue that humans have the responsibility to make sense of AI and its impact on society.',\n",
       " 'derived from affective neuroscience, is that techne, including AI, is not only shaping human experience but also challenging traditional notions of human nature and agency. This article explores the implications of this perspective, arguing for a reevaluation of the human emotional spectrum and a recognition of the need for humans to adapt to the technological civilization they have created.',\n",
       " 'The first text discusses human enhancement technologies, arguing that they are a historical and pervasive phenomenon that can potentially change the notion of being human. It emphasizes the need for these enhancements to pass the test of effectively engaging with the dynamic world. The second text explores a broader framework of responsibility that includes moral responsibility, causal responsibility, relational responsibility, and narrative responsibility. It discusses how artificial intelligence challenges human responsibility and sense-making, and argues that humans have a responsibility to make sense of and interact with AI.',\n",
       " 'y sense-making, acknowledging the role of both the patient and the healthcare provider in the placebo effect and emphasizing the importance of understanding the complex dynamics at play in the therapeutic relationship.',\n",
       " \"be achieved through various means such as prosthetics, cognitive enhancements, or genetic modifications. The ethical implications of Human Enhancement are often debated, but it is argued that it is a natural and ongoing process that has been incorporated into everyday practices. The effectiveness of an enhancement is determined by its ability to assist the user in engaging with the dynamic world, and even if it fails in this regard, it still fundamentally changes the user's relationship with the world.\",\n",
       " \"The first text is about human enhancement and how technology can be used to modify and improve the human body's capabilities. It argues that human enhancement is a historical and pervasive phenomenon and explores the ethical implications and transformative nature of this phenomenon.\\n\\nThe second text is about placebo effects and challenges the traditional explanations and assumptions surrounding them. It proposes an alternative theoretical framework based on embodied cognitive science and introduces the idea of reflective and pre-reflective activity. The text also discusses the interrelated processes of embodiment and defends the need for further investigation into therapeutic interventions.\",\n",
       " 'The first text is about human enhancement and how technology allows humans to modify themselves rather than adapting the environment to their needs. It argues that human enhancement is a historical and pervasive phenomenon that can potentially change the notion of being human. The second text explores the concept of responsibility in relation to artificial intelligence (AI). It introduces a broader framework of responsibility that includes moral responsibility, causal responsibility, relational responsibility, and narrative responsibility. It argues that humans have a responsibility to make sense of and engage with AI, while also acknowledging that AI can contribute to sense-making.',\n",
       " 'y sense-making, emphasizing the active and dynamic role of both the patient and the practitioner in the placebo effect.',\n",
       " 'The first text is about transhumanism, which is a scientific and philosophical project that aims to use advanced technologies to enhance the physical, cognitive, and emotional capabilities of human beings. It discusses the idea of creating a new species, Homo excelsior, that is genetically selected and improved, and argues for a balance between scientific advancement and preserving human dignity and liberty.\\n\\nThe second text examines the implications of artificial intelligence (AI) from a semiotic perspective. It discusses how AI aims to model and surpass human intelligence, and explores the role of semiotics in understanding human cognition. It argues that while AI is instructive, semiotics is more relevant in studying human cognition as it focuses on the generation of meaning in the human sense, rather than artificial models.',\n",
       " \"be seen in various domains such as sports, medicine, and technology. We argue that instead of solely focusing on the ethical implications, it is important to understand the practical aspects and the impact of human enhancement on individuals' interactions with the world.\",\n",
       " 'The first text discusses the influence of modern technology, particularly artificial intelligence and affective artefacts, on constructivist models in affective neuroscience. It argues that this technological evolution leads to a shift towards a technical governance of human emotions, which challenges traditional notions of anthropology and posthumanist aspirations.\\n\\nThe second text critiques current explanations and empirical evidence regarding placebo effects and proposes an alternative theoretical framework based on the enactive approach to life and mind. It suggests moving away from binary distinctions and towards a focus on reflective and pre-reflective activity, and introduces an ontology of individuation to understand placebo interventions as modulators and triggers in the regulation of tensions between embodied and interpersonal processes.',\n",
       " 'be seen in various contexts such as sports performance enhancement, prosthetics, cognitive enhancement, and cosmetic surgery. The authors argue that instead of focusing solely on the ethical implications of human enhancement, it is important to understand the ways in which these technologies interact with and shape our relationship with the world, and how they contribute to effective engagement with a dynamic environment. They also emphasize the need to consider the physiological and psychological embodiment of these enhancements in order to fully understand their impact.',\n",
       " 'This cluster of texts explores the concept of responsibility in relation to artificial intelligence (AI). The first text argues for a broader framework of responsibility that includes moral responsibility, causal responsibility, relational responsibility, and narrative responsibility. It emphasizes the importance of humans making sense of and taking charge of AI, while acknowledging the role of technology in sense-making. The second text discusses the influence of AI and affective artefacts on posthumanism and the need for a recasting of the human emotional spectrum. It highlights the ontological implications of this recasting and argues for a rapid advance of the human form driven by enlightenment ideals.',\n",
       " 'The first text in this cluster is about a broader framework of responsibility that includes causal responsibility, relational responsibility, and \"narrative responsibility\" in the context of artificial intelligence. It argues that humans have a hermeneutic responsibility to make sense of and engage with AI, while also acknowledging the importance of human agency in this responsibility.\\n\\nThe second text in this cluster critiques current explanations and empirical evidence of placebo effects and proposes an alternative theoretical framework called the enactive approach to life and mind. It suggests moving away from binary distinctions and instead focusing on reflective and pre-reflective activity, and introduces an ontology of individuation to understand placebo interventions as modulators and triggers in the regulation of tensions between embodied and interpersonal processes.',\n",
       " \"be seen in various domains such as sports, medicine, and technology. The ethical debate surrounding Human Enhancement often overlooks the historical and widespread nature of these practices, and instead focuses on alarmist or utopian scenarios. The effectiveness of an enhancement is determined by its ability to assist the user in effectively engaging with the dynamic world, and even if it fails in this regard, it still represents a fundamental process of changing the user's relationship with the world through physical or psychological embodiment.\",\n",
       " 'The first text is about the concept of human enhancement, which refers to using technology to improve the capabilities of the human body or create new ones. The authors argue that human enhancement is a historical and pervasive phenomenon that can potentially change the notion of being human, and they provide a nuanced analysis of its transformative nature. \\n\\nThe second text discusses placebo effects and challenges the problematic assumptions and explanations surrounding them. The authors propose an alternative theoretical framework based on embodied cognitive science, emphasizing concepts such as autonomy, agency, and sense-making. They also highlight the importance of investigating the therapeutic effects of placebos in relation to ongoing embodied and interpersonal processes.',\n",
       " \"f technology. This article argues that human responsibility includes not only moral responsibility, but also causal responsibility, relational responsibility, and narrative responsibility. It explores how artificial intelligence challenges human responsibility and sense-making, emphasizing the importance of humans making sense of and taking charge of technology. Additionally, the article discusses human enhancement technologies and their impact on individuals' relationship with the world, highlighting the need for these technologies to effectively engage with a dynamic world. It suggests that human enhancement is a historical and pervasive phenomenon integrated into everyday practices and should be understood in terms of its physiological or psychological embodiment.\",\n",
       " 'These texts are about the concept of responsibility in relation to artificial intelligence (AI). They argue for a broader framework of responsibility that includes not only moral responsibility but also causal responsibility, relational responsibility, and \"narrative responsibility\" or \"hermeneutic responsibility.\" The texts emphasize the importance of humans taking charge of sense-making and decision-making in the face of AI, and highlight the normative and political aspects of this responsibility. They also discuss the influence of AI on affective neuroscience and posthumanism, and the need for a recasting of the human emotional spectrum.',\n",
       " 'The first text is about human enhancement and how technology allows humans to modify themselves instead of adapting their environment. It argues that human enhancement is a historical and pervasive phenomenon and examines the physiological and psychological aspects of it. The second text discusses placebo effects from an enactive perspective, challenging current explanations and proposing a theoretical framework based on embodied cognitive science. It suggests moving away from binary distinctions and introduces an ontology of individuation to understand placebo interventions as modulators and triggers in the regulation of tensions between embodied and interpersonal processes.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e4313493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '1. Human Enhancement: Historical and Transformative',\n",
       " 1: '2. Responsibility and AI: Broad Framework',\n",
       " 2: '3. AI and Human Nature: Challenging Notions',\n",
       " 3: '4. Human Enhancement: Ethical Implications',\n",
       " 4: '5. Human Enhancement: Changing Relationship',\n",
       " 5: '6. Human Enhancement: Natural Process',\n",
       " 6: '7. Human Enhancement: Prosthetics and Modifications',\n",
       " 7: '8. Human Enhancement: Dignity and Liberty',\n",
       " 8: '9. AI and Semiotics: Understanding Cognition',\n",
       " 9: '10. Human Enhancement: Practical Impact',\n",
       " 10: '11. Technology and Emotions: Shaping Experience',\n",
       " 11: '12. Placebo Effects: Alternative Framework',\n",
       " 12: '13. Human Enhancement: Engaging with the World',\n",
       " 13: '14. Responsibility and AI: Making Sense',\n",
       " 14: '15. Placebo Effects: Reflective and Pre-Reflective Activity',\n",
       " 15: '16. Human Enhancement: Adapting to Technology',\n",
       " 16: '17. Responsibility and AI: Hermeneutic Responsibility',\n",
       " 17: '18. Placebo Effects: Enactive Approach',\n",
       " 18: '19. Human Enhancement: Historical and Widespread',\n",
       " 19: '20. Responsibility and AI: Normative and Political'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_names_dict"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "comment_magics": true,
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "discovery_child_development",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
