# ---
# jupyter:
#   jupytext:
#     cell_metadata_filter: -all
#     comment_magics: true
#     text_representation:
#       extension: .py
#       format_name: percent
#       format_version: '1.3'
#       jupytext_version: 1.15.2
#   kernelspec:
#     display_name: discovery_child_development
#     language: python
#     name: python3
# ---

# %% [markdown]
# This notebook is for preprocessing the Openlex data we have extracted and turning it into a format that can be the input to some text clustering.
#
# The data was generated by running `discovery_child_development/pipeline/openalex_workflow.py` from the command line via
# ```
# python discovery_child_development/pipeline/openalex_workflow.py run --production=True
# ```
#
# Two outputs are saved:
# * the larger of the two is a table that includes in long format every concept that is associated with each OpenAlex ID and their scores
# * the smaller is a table with one row per work, containing the OpenAlex ID, title, abstract (deinverted) and "text" which is simply title + ' ' + abstract.

# %%
import json
import os
import boto3
from dotenv import load_dotenv, find_dotenv
import pandas as pd
from io import StringIO
import logging

# Utils functions
from discovery_child_development.utils import openalex_utils

CONCEPT_IDS = [
    "C109260823",  # child development
    "C2993937534",  # childhood development
    "C2777082460",  # early childhood
    "C2911196330",  # child rearing
    "C2993037610",  # child care
    "C2779415726",  # child protection
    "C2781192327",  # child behavior checklist
    "C15471489",  # child psychotherapy
    "C178229462",  # early childhood education
    # "C138496976",  # developmental psychology (level 1).
]

CONCEPT_IDS = "|".join(CONCEPT_IDS)

YEARS = [2019, 2020, 2021, 2022, 2023]
years_list = [str(x) for x in YEARS]

load_dotenv()
# Bucket where the file is saved
S3_BUCKET = os.environ["S3_BUCKET"]
# subfolder within the bucket
S3_PATH = "metaflow"

output_filename_concepts = (
    f"concepts_metadata_{CONCEPT_IDS}_year-{'-'.join(years_list)}.csv"
)
output_filepath_concepts = "data/openAlex/concepts/"
output_filename_works = (
    f"openalex_abstracts_{CONCEPT_IDS}_year-{'-'.join(years_list)}.csv"
)
output_filepath_works = "data/openAlex/"

# %%
input_files = [
    f"openalex-works_production-True_concept-{CONCEPT_IDS}_year-{year}.json"
    for year in YEARS
]
input_files

# %%
openalex_df = pd.DataFrame()

for file in input_files:
    # read raw data from s3
    s3_client = boto3.client("s3")
    response = s3_client.get_object(Bucket=S3_BUCKET, Key=f"{S3_PATH}/{file}")
    openalex_raw_data = (
        response["Body"].read().decode("utf-8")
    )  # Convert bytes to string

    # Convert it to a list format
    openalex_data = json.loads(openalex_raw_data)

    print(len(openalex_data))

    year_df = pd.DataFrame(openalex_data)

    openalex_df = pd.concat([openalex_df, year_df])

# %%
openalex_df["publication_year"].value_counts()

# %% [markdown]
# # Filter the data
#
# * Works in English only (for now at least)

# %%
openalex_en = openalex_df[openalex_df["language"] == "en"]

logging.info(
    f"Number of works lost because they were not in English: {len(openalex_df)-len(openalex_en)}"
)

# %%
# Retain only works where abstract and title are not null

logging.info(
    f"Number of NAs in 'abstract_inverted_index' before cleaning: {openalex_en['abstract_inverted_index'].isna().sum()}"
)
logging.info(
    f"Number of NAs in 'title' before cleaning: {openalex_en['title'].isna().sum()}"
)

openalex_en = openalex_en[openalex_en["abstract_inverted_index"].notnull()]
openalex_en = openalex_en[openalex_en["title"].notnull()]

logging.info(
    f"Number of NAs in 'abstract_inverted_index': {openalex_en['abstract_inverted_index'].isna().sum()}"
)
logging.info(f"Number of NAs in 'title': {openalex_en['title'].isna().sum()}")
logging.info(f"Remaining number of works: {len(openalex_en)}")

# %% [markdown]
# # Concepts
#
# Create a dataframe that records all the concepts (concept IDs and names, concept level, wikidata, relevance score) for each work ID.

# %%
data_list = []

for index, row in openalex_en[
    ["id", "title", "publication_year", "concepts"]
].iterrows():
    for concept in row["concepts"]:
        data_list.append(
            {
                "openalex_id": row["id"],
                "title": row["title"],
                "year": row["publication_year"],
                "concept_id": concept["id"],
                "wikidata": concept["wikidata"],
                "display_name": concept["display_name"],
                "level": concept["level"],
                "score": concept["score"],
            }
        )

df = pd.DataFrame(data_list)

df.head()

# %%
# Write the concepts metadata to s3
csv_buffer = StringIO()
df.to_csv(csv_buffer)
s3_resource = boto3.resource("s3")
s3_resource.Object(
    S3_BUCKET,
    f"{output_filepath_concepts}{output_filename_concepts}",
).put(Body=csv_buffer.getvalue())

# %% [markdown]
# # OpenAlex abstracts

# %%
# Reduce the number of columns that we are processing
openalex_en_abstracts = openalex_en[["id", "title", "abstract_inverted_index"]]

# Deinvert the abstract and stick together the title and abstract. This mimics preprocessing done to create [this dataset](https://huggingface.co/datasets/colonelwatch/abstracts-embeddings).

openalex_en_abstracts.loc[:, "abstract"] = openalex_en_abstracts[
    "abstract_inverted_index"
].apply(lambda x: openalex_utils.deinvert_abstract(x))

openalex_en_abstracts.loc[:, "text"] = (
    openalex_en_abstracts["title"] + " " + openalex_en_abstracts["abstract"]
)

# drop the inverted abstract
openalex_en_abstracts = openalex_en_abstracts[["id", "title", "abstract", "text"]]

# Write to s3
csv_buffer = StringIO()
openalex_en_abstracts.to_csv(csv_buffer)
s3_resource = boto3.resource("s3")
s3_resource.Object(
    S3_BUCKET,
    f"{output_filepath_works}{output_filename_works}",
).put(Body=csv_buffer.getvalue())

# %%
