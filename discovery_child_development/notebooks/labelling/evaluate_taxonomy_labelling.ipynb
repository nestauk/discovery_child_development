{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from discovery_child_development import config, PROJECT_DIR\n",
    "from discovery_child_development.utils import jsonl_utils as jsonl\n",
    "from discovery_child_development.utils import taxonomy_labelling_utils as tlu\n",
    "from discovery_child_development.utils.openai_utils import client\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "import tiktoken\n",
    "import wandb\n",
    "\n",
    "MODEL = \"gpt-3.5-turbo-1106\" # \"gpt-4\"\n",
    "\n",
    "def get_model_cost(model):\n",
    "    # based on https://openai.com/pricing\n",
    "    if model == \"gpt-3.5-turbo-1106\":\n",
    "        input = 0.001\n",
    "        output = 0.002\n",
    "    elif model == \"gpt-4\":\n",
    "        input = 0.03\n",
    "        output = 0.06\n",
    "    return input, output\n",
    "\n",
    "MODEL_INPUT_COST, MODEL_OUTPUT_COST = get_model_cost(MODEL)\n",
    "SEED = config[\"seed\"]\n",
    "\n",
    "random.seed(SEED)\n",
    "\n",
    "encoding = tiktoken.encoding_for_model(MODEL)\n",
    "\n",
    "LABELS_PATH = PROJECT_DIR / \"inputs/data/labelling/taxonomy/output/training_validation_data_patents_openalex_LABELLED.jsonl\"\n",
    "PROMPT_OUT_PATH = PROJECT_DIR / \"inputs/data/labelling/taxonomy/output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these functions came from: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding=encoding):\n",
    "  return len(encoding.encode(string))\n",
    "\n",
    "def num_tokens_from_messages(messages, model=MODEL):\n",
    "    \"\"\"Return the number of tokens used by a list of messages.\"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        print(\"Warning: model not found. Using cl100k_base encoding.\")\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    if model in {\n",
    "        \"gpt-3.5-turbo-0613\",\n",
    "        \"gpt-3.5-turbo-16k-0613\",\n",
    "        \"gpt-4-0314\",\n",
    "        \"gpt-4-32k-0314\",\n",
    "        \"gpt-4-0613\",\n",
    "        \"gpt-4-32k-0613\",\n",
    "        }:\n",
    "        tokens_per_message = 3\n",
    "        tokens_per_name = 1\n",
    "    elif model == \"gpt-3.5-turbo-0301\":\n",
    "        tokens_per_message = 4  # every message follows <|start|>{role/name}\\n{content}<|end|>\\n\n",
    "        tokens_per_name = -1  # if there's a name, the role is omitted\n",
    "    elif \"gpt-3.5-turbo\" in model:\n",
    "        print(\"Warning: gpt-3.5-turbo may update over time. Returning num tokens assuming gpt-3.5-turbo-0613.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0613\")\n",
    "    elif \"gpt-4\" in model:\n",
    "        print(\"Warning: gpt-4 may update over time. Returning num tokens assuming gpt-4-0613.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-4-0613\")\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            f\"\"\"num_tokens_from_messages() is not implemented for model {model}. See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\"\n",
    "        )\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data = pd.DataFrame(taxonomy.get_labelling_sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "      <th>accept</th>\n",
       "      <th>model</th>\n",
       "      <th>model_output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>W3094274839</td>\n",
       "      <td>Sleep and Technology in Early Childhood. Resea...</td>\n",
       "      <td>openalex</td>\n",
       "      <td>[Sleep, Technology (general)]</td>\n",
       "      <td>gpt-3.5-turbo-1106</td>\n",
       "      <td>[Sleep]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>W3133624391</td>\n",
       "      <td>Vygotsky’s theory in-play: early childhood edu...</td>\n",
       "      <td>openalex</td>\n",
       "      <td>[Cognitive development, Early childhood develo...</td>\n",
       "      <td>gpt-3.5-turbo-1106</td>\n",
       "      <td>[Cognitive development, Early childhood develo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>W3118424788</td>\n",
       "      <td>Physiological substrates of imagination in ear...</td>\n",
       "      <td>openalex</td>\n",
       "      <td>[Cognitive development, Early childhood develo...</td>\n",
       "      <td>gpt-3.5-turbo-1106</td>\n",
       "      <td>[Cognitive development, Early childhood develo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>W4385669580</td>\n",
       "      <td>A randomized controlled trial on the effect of...</td>\n",
       "      <td>openalex</td>\n",
       "      <td>[Early childhood development (general), Family...</td>\n",
       "      <td>gpt-3.5-turbo-1106</td>\n",
       "      <td>[Early childhood development (general)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>W3007335027</td>\n",
       "      <td>Longitudinal associations between mothers’ and...</td>\n",
       "      <td>openalex</td>\n",
       "      <td>[Cognitive development]</td>\n",
       "      <td>gpt-3.5-turbo-1106</td>\n",
       "      <td>[Cognitive development]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                               text    source  \\\n",
       "0  W3094274839  Sleep and Technology in Early Childhood. Resea...  openalex   \n",
       "1  W3133624391  Vygotsky’s theory in-play: early childhood edu...  openalex   \n",
       "2  W3118424788  Physiological substrates of imagination in ear...  openalex   \n",
       "3  W4385669580  A randomized controlled trial on the effect of...  openalex   \n",
       "4  W3007335027  Longitudinal associations between mothers’ and...  openalex   \n",
       "\n",
       "                                              accept               model  \\\n",
       "0                      [Sleep, Technology (general)]  gpt-3.5-turbo-1106   \n",
       "1  [Cognitive development, Early childhood develo...  gpt-3.5-turbo-1106   \n",
       "2  [Cognitive development, Early childhood develo...  gpt-3.5-turbo-1106   \n",
       "3  [Early childhood development (general), Family...  gpt-3.5-turbo-1106   \n",
       "4                            [Cognitive development]  gpt-3.5-turbo-1106   \n",
       "\n",
       "                                        model_output  \n",
       "0                                            [Sleep]  \n",
       "1  [Cognitive development, Early childhood develo...  \n",
       "2  [Cognitive development, Early childhood develo...  \n",
       "3            [Early childhood development (general)]  \n",
       "4                            [Cognitive development]  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data that has already been labelled using prodigy\n",
    "human_labels = pd.DataFrame(jsonl.load_jsonl(LABELS_PATH))[['id', 'text', 'source', 'accept', 'model', 'model_output']]\n",
    "human_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_ids = human_labels['id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the da\n",
    "# test_data = test_data[test_data['id'].isin(test_ids)]\n",
    "# len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_flat = tlu.load_categories()\n",
    "\n",
    "function = tlu.format_function(categories_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_gpt_output(llm_output, human_output, id, text, model, prompt):\n",
    "    label_diff = len(llm_output) - len(human_output)\n",
    "    if label_diff < 0:\n",
    "        n_missing_labels = abs(label_diff)\n",
    "        n_extra_labels = 0\n",
    "    elif label_diff > 0:\n",
    "        n_extra_labels = label_diff\n",
    "        n_missing_labels = 0\n",
    "    else:\n",
    "        n_extra_labels = 0\n",
    "        n_missing_labels = 0\n",
    "        \n",
    "    return {\n",
    "          \"id\": id,\n",
    "          \"text\": text,\n",
    "          \"model\": model,\n",
    "           \"prompt\": prompt,\n",
    "           \"output\": llm_output,\n",
    "           \"human_output\": human_output,\n",
    "           \"exact_match\": llm_output == human_output,\n",
    "           \"no_overlap\": llm_output.isdisjoint(human_output),\n",
    "           \"label_diff\": label_diff,\n",
    "           \"n_extra_labels\": n_extra_labels,\n",
    "           \"n_missing_labels\": n_missing_labels\n",
    "        }\n",
    "    \n",
    "def summarise_gpt_performance(df):\n",
    "    exact_match = df['exact_match'].sum()\n",
    "    no_overlap = df['no_overlap'].sum()\n",
    "    prop_exact_matches = df['exact_match'].sum() / len(df)\n",
    "    n_missing_labels = df['n_missing_labels'].mean()\n",
    "    prop_no_overlap = df['no_overlap'].sum() / len(df)\n",
    "    return {'exact_matches': exact_match,\n",
    "            'no_overlap': no_overlap,\n",
    "            'prop_exact_matches': prop_exact_matches,\n",
    "            'avg_missing_labels': n_missing_labels,\n",
    "            'prop_no_overlap': prop_no_overlap}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse prodigy labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_labels_dict = human_labels[['id', 'text', 'accept', 'model','model_output']].to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact_matches': 19,\n",
       " 'no_overlap': 0,\n",
       " 'prop_exact_matches': 0.5277777777777778,\n",
       " 'avg_missing_labels': 0.5833333333333334,\n",
       " 'prop_no_overlap': 0.0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prodigy_results = []\n",
    "\n",
    "for row in human_labels_dict:\n",
    "    prodigy_results.append(eval_gpt_output(set(row['model_output']), set(row['accept']), row['id'], row['text'], row['model'], prompt=\"\"))\n",
    "\n",
    "df = pd.DataFrame(prodigy_results)\n",
    "\n",
    "summarise_gpt_performance(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare different models (you can also tweak the prompt and run this part again to see what changes)\n",
    "\n",
    "This code block also logs your prompt and key metrics on weights & biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-15 16:35:54,462 - wandb.jupyter - ERROR - Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrosie-oxbury\u001b[0m (\u001b[33mnesta-uk\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/rosie.oxbury/Documents/git_repos/discovery_child_development/discovery_child_development/notebooks/labelling/wandb/run-20231215_163555-7ath1b6k</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nesta-uk/ISS%20supervised%20ML/runs/7ath1b6k' target=\"_blank\">robust-night-143</a></strong> to <a href='https://wandb.ai/nesta-uk/ISS%20supervised%20ML' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nesta-uk/ISS%20supervised%20ML' target=\"_blank\">https://wandb.ai/nesta-uk/ISS%20supervised%20ML</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nesta-uk/ISS%20supervised%20ML/runs/7ath1b6k' target=\"_blank\">https://wandb.ai/nesta-uk/ISS%20supervised%20ML/runs/7ath1b6k</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-15 16:36:00,779 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-12-15 16:36:01,253 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-12-15 16:36:02,202 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-12-15 16:36:04,921 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-12-15 16:36:05,473 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-12-15 16:36:07,422 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-12-15 16:36:08,136 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-12-15 16:36:11,051 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-12-15 16:36:12,048 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-12-15 16:36:14,797 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-12-15 16:36:15,313 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-12-15 16:36:18,877 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-12-15 16:36:21,967 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-12-15 16:36:22,838 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-12-15 16:36:25,749 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-12-15 16:36:26,197 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-12-15 16:36:28,618 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-12-15 16:36:31,562 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-12-15 16:36:32,816 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-12-15 16:36:34,184 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-12-15 16:36:35,126 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-12-15 16:36:35,580 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-12-15 16:36:37,936 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-12-15 16:36:39,375 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-12-15 16:36:40,501 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-12-15 16:36:41,126 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-12-15 16:36:43,203 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-12-15 16:36:44,973 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-12-15 16:36:46,612 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-12-15 16:36:47,422 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-12-15 16:36:47,925 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-12-15 16:36:51,051 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-12-15 16:36:51,743 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-12-15 16:36:54,936 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-12-15 16:36:56,374 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-12-15 16:36:56,886 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "971048ffb6f64d9798b28aa4f45e745f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.456 MB of 0.733 MB uploaded\\r'), FloatProgress(value=0.6225066015599073, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.33333</td></tr><tr><td>avg_missing_labels</td><td>0.80556</td></tr><tr><td>prop_no_overlap</td><td>0.08333</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">robust-night-143</strong> at: <a href='https://wandb.ai/nesta-uk/ISS%20supervised%20ML/runs/7ath1b6k' target=\"_blank\">https://wandb.ai/nesta-uk/ISS%20supervised%20ML/runs/7ath1b6k</a><br/>Synced 7 W&B file(s), 1 media file(s), 1 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231215_163555-7ath1b6k/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6999cdfabee42e2932ffbcad0eae8b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011167665277349038, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/rosie.oxbury/Documents/git_repos/discovery_child_development/discovery_child_development/notebooks/labelling/wandb/run-20231215_163703-3h09x8kc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nesta-uk/ISS%20supervised%20ML/runs/3h09x8kc' target=\"_blank\">crimson-hill-144</a></strong> to <a href='https://wandb.ai/nesta-uk/ISS%20supervised%20ML' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nesta-uk/ISS%20supervised%20ML' target=\"_blank\">https://wandb.ai/nesta-uk/ISS%20supervised%20ML</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nesta-uk/ISS%20supervised%20ML/runs/3h09x8kc' target=\"_blank\">https://wandb.ai/nesta-uk/ISS%20supervised%20ML/runs/3h09x8kc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-15 16:37:11,116 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-12-15 16:37:12,918 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-12-15 16:37:15,108 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-12-15 16:37:17,009 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-12-15 16:37:18,374 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-12-15 16:37:19,922 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-12-15 16:37:21,769 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-12-15 16:37:22,932 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-12-15 16:37:24,038 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-12-15 16:37:25,363 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-12-15 16:37:26,562 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-12-15 16:37:28,438 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-12-15 16:37:30,840 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-12-15 16:37:32,333 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-12-15 16:37:33,745 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-12-15 16:37:35,076 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-12-15 16:37:37,575 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-12-15 16:37:38,747 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-12-15 16:37:41,323 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-12-15 16:37:42,384 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-12-15 16:37:43,435 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-12-15 16:37:44,804 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-12-15 16:37:46,237 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-12-15 16:37:47,953 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-12-15 16:37:49,656 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-12-15 16:37:50,858 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-12-15 16:37:52,280 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-12-15 16:37:53,269 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-12-15 16:37:54,855 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-12-15 16:37:56,059 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-12-15 16:37:57,330 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-12-15 16:37:58,775 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-12-15 16:37:59,757 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-12-15 16:38:00,880 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-12-15 16:38:02,597 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-12-15 16:38:04,583 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a253ee3c27284e85bad801b048dd3ba8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.455 MB of 0.730 MB uploaded\\r'), FloatProgress(value=0.6229569011122504, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.33333</td></tr><tr><td>avg_missing_labels</td><td>1.36111</td></tr><tr><td>prop_no_overlap</td><td>0.13889</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">crimson-hill-144</strong> at: <a href='https://wandb.ai/nesta-uk/ISS%20supervised%20ML/runs/3h09x8kc' target=\"_blank\">https://wandb.ai/nesta-uk/ISS%20supervised%20ML/runs/3h09x8kc</a><br/>Synced 7 W&B file(s), 1 media file(s), 1 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231215_163703-3h09x8kc/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "models = ['gpt-3.5-turbo-1106', 'gpt-4-0613']\n",
    "\n",
    "results = {}\n",
    "\n",
    "for model in models:\n",
    "    run = wandb.init(\n",
    "                project=\"ISS supervised ML\",\n",
    "                job_type=\"Taxonomy labelling_prompt_engineering\",\n",
    "                save_code=True,\n",
    "                tags=[model],\n",
    "            )\n",
    "    results[model] = {}\n",
    "    results[model]['outputs'] = []\n",
    "    \n",
    "    # Create an artifact for the prompt\n",
    "    prompt_artifact = wandb.Artifact('prompt_artifact', type='text')\n",
    "    temp_prompt = tlu.build_prompt(\"<TEXT>\", categories_flat)\n",
    "    str_prompt = []\n",
    "    for m in temp_prompt:\n",
    "        str_prompt.append(f\"{m['role']}: {m['content']}\\n\")\n",
    "    str_prompt = ''.join(str_prompt)\n",
    "    with open(f\"{PROMPT_OUT_PATH}/prompt.txt\", \"w\") as file:\n",
    "        file.write(str_prompt)\n",
    "    prompt_artifact.add_file(f\"{PROMPT_OUT_PATH}/prompt.txt\")\n",
    "    # Log the artifact\n",
    "    wandb.log_artifact(prompt_artifact)\n",
    "    \n",
    "    for index, row in human_labels.iterrows():\n",
    "        prompt = tlu.build_prompt(row['text'], categories_flat)\n",
    "        r = client.chat.completions.create(\n",
    "            model=model,\n",
    "            temperature=0.0,\n",
    "            messages=prompt,\n",
    "            functions=[function],\n",
    "            function_call={\"name\": \"predict_category\"},\n",
    "            )\n",
    "        llm_output = set(tlu.get_labels_from_gpt_response(r))\n",
    "        human_labels_list = human_labels[human_labels['id'] == row['id']]['accept'].values\n",
    "        human_output = set([label for sublist in human_labels_list for label in sublist])\n",
    "        results[model]['outputs'].append(eval_gpt_output(llm_output, human_output, id=row['id'], text=row['text'], model=model, prompt=prompt))\n",
    "    df = pd.DataFrame(results[model]['outputs'])\n",
    "    wb_table = wandb.Table(\n",
    "                data=df, columns=df.columns\n",
    "            )\n",
    "    run.log({\"Outputs\": wb_table})\n",
    "    # Evaluation metrics\n",
    "    summary_stats = summarise_gpt_performance(df)\n",
    "    results[model]['stats'] = summary_stats\n",
    "    # Log metrics\n",
    "    wandb.run.summary[\"accuracy\"] = summary_stats['prop_exact_matches']\n",
    "    wandb.run.summary['prop_no_overlap'] = summary_stats['prop_no_overlap']\n",
    "    wandb.run.summary['avg_missing_labels'] = summary_stats['avg_missing_labels']\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "discovery_child_development",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
