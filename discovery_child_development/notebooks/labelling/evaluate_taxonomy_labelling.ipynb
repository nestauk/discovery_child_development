{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from discovery_child_development import config, PROJECT_DIR\n",
    "from discovery_child_development.getters import taxonomy\n",
    "from discovery_child_development.utils import jsonl_utils as jsonl\n",
    "from discovery_child_development.utils import taxonomy_labelling_utils as tlu\n",
    "from discovery_child_development.utils.openai_utils import client\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "import tiktoken\n",
    "import wandb\n",
    "\n",
    "MODEL = \"gpt-3.5-turbo-1106\" # \"gpt-4\"\n",
    "\n",
    "def get_model_cost(model):\n",
    "    # based on https://openai.com/pricing\n",
    "    if model == \"gpt-3.5-turbo-1106\":\n",
    "        input = 0.001\n",
    "        output = 0.002\n",
    "    elif model == \"gpt-4\":\n",
    "        input = 0.03\n",
    "        output = 0.06\n",
    "    return input, output\n",
    "\n",
    "MODEL_INPUT_COST, MODEL_OUTPUT_COST = get_model_cost(MODEL)\n",
    "SEED = config[\"seed\"]\n",
    "\n",
    "random.seed(SEED)\n",
    "\n",
    "encoding = tiktoken.encoding_for_model(MODEL)\n",
    "\n",
    "LABELS_PATH = PROJECT_DIR / \"inputs/data/labelling/taxonomy/output/training_validation_data_patents_openalex_LABELLED.jsonl\"\n",
    "PROMPT_OUT_PATH = PROJECT_DIR / \"inputs/data/labelling/taxonomy/output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these functions came from: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding=encoding):\n",
    "  return len(encoding.encode(string))\n",
    "\n",
    "def num_tokens_from_messages(messages, model=MODEL):\n",
    "    \"\"\"Return the number of tokens used by a list of messages.\"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        print(\"Warning: model not found. Using cl100k_base encoding.\")\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    if model in {\n",
    "        \"gpt-3.5-turbo-0613\",\n",
    "        \"gpt-3.5-turbo-16k-0613\",\n",
    "        \"gpt-4-0314\",\n",
    "        \"gpt-4-32k-0314\",\n",
    "        \"gpt-4-0613\",\n",
    "        \"gpt-4-32k-0613\",\n",
    "        }:\n",
    "        tokens_per_message = 3\n",
    "        tokens_per_name = 1\n",
    "    elif model == \"gpt-3.5-turbo-0301\":\n",
    "        tokens_per_message = 4  # every message follows <|start|>{role/name}\\n{content}<|end|>\\n\n",
    "        tokens_per_name = -1  # if there's a name, the role is omitted\n",
    "    elif \"gpt-3.5-turbo\" in model:\n",
    "        print(\"Warning: gpt-3.5-turbo may update over time. Returning num tokens assuming gpt-3.5-turbo-0613.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0613\")\n",
    "    elif \"gpt-4\" in model:\n",
    "        print(\"Warning: gpt-4 may update over time. Returning num tokens assuming gpt-4-0613.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-4-0613\")\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            f\"\"\"num_tokens_from_messages() is not implemented for model {model}. See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\"\n",
    "        )\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data that has already been labelled using prodigy\n",
    "human_labels = pd.DataFrame(taxonomy.get_prodigy_labelled_data())[['id', 'text', 'source', 'accept', 'model', 'model_output']]\n",
    "human_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_labels = human_labels.explode('model_output')\n",
    "\n",
    "gpt_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(gpt_labels['model_output'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_flat = tlu.load_categories()\n",
    "\n",
    "function = tlu.format_function(categories_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_gpt_output(llm_output, human_output, id, text, model, prompt):\n",
    "    label_diff = len(llm_output) - len(human_output)\n",
    "    if label_diff < 0:\n",
    "        n_missing_labels = abs(label_diff)\n",
    "        n_extra_labels = 0\n",
    "    elif label_diff > 0:\n",
    "        n_extra_labels = label_diff\n",
    "        n_missing_labels = 0\n",
    "    else:\n",
    "        n_extra_labels = 0\n",
    "        n_missing_labels = 0\n",
    "        \n",
    "    return {\n",
    "          \"id\": id,\n",
    "          \"text\": text,\n",
    "          \"model\": model,\n",
    "           \"prompt\": prompt,\n",
    "           \"output\": llm_output,\n",
    "           \"human_output\": human_output,\n",
    "           \"exact_match\": llm_output == human_output,\n",
    "           \"no_overlap\": llm_output.isdisjoint(human_output),\n",
    "           \"label_diff\": label_diff,\n",
    "           \"n_extra_labels\": n_extra_labels,\n",
    "           \"n_missing_labels\": n_missing_labels\n",
    "        }\n",
    "    \n",
    "def summarise_gpt_performance(df):\n",
    "    exact_match = df['exact_match'].sum()\n",
    "    no_overlap = df['no_overlap'].sum()\n",
    "    prop_exact_matches = df['exact_match'].sum() / len(df)\n",
    "    n_missing_labels = df['n_missing_labels'].mean()\n",
    "    prop_no_overlap = df['no_overlap'].sum() / len(df)\n",
    "    return {'exact_matches': exact_match,\n",
    "            'no_overlap': no_overlap,\n",
    "            'prop_exact_matches': prop_exact_matches,\n",
    "            'avg_missing_labels': n_missing_labels,\n",
    "            'prop_no_overlap': prop_no_overlap}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse prodigy labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_labels_dict = human_labels[['id', 'text', 'accept', 'model','model_output']].to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prodigy_results = []\n",
    "\n",
    "for row in human_labels_dict:\n",
    "    prodigy_results.append(eval_gpt_output(set(row['model_output']), set(row['accept']), row['id'], row['text'], row['model'], prompt=\"\"))\n",
    "\n",
    "df = pd.DataFrame(prodigy_results)\n",
    "\n",
    "summarise_gpt_performance(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare different models (you can also tweak the prompt and run this part again to see what changes)\n",
    "\n",
    "This code block also logs your prompt and key metrics on weights & biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['gpt-3.5-turbo-1106', 'gpt-4-0613']\n",
    "\n",
    "results = {}\n",
    "\n",
    "for model in models:\n",
    "    run = wandb.init(\n",
    "                project=\"ISS supervised ML\",\n",
    "                job_type=\"Taxonomy labelling_prompt_engineering\",\n",
    "                save_code=True,\n",
    "                tags=[model],\n",
    "            )\n",
    "    results[model] = {}\n",
    "    results[model]['outputs'] = []\n",
    "    \n",
    "    # Create an artifact for the prompt\n",
    "    prompt_artifact = wandb.Artifact('prompt_artifact', type='text')\n",
    "    temp_prompt = tlu.build_prompt(\"<TEXT>\", categories_flat)\n",
    "    str_prompt = []\n",
    "    for m in temp_prompt:\n",
    "        str_prompt.append(f\"{m['role']}: {m['content']}\\n\")\n",
    "    str_prompt = ''.join(str_prompt)\n",
    "    with open(f\"{PROMPT_OUT_PATH}/prompt.txt\", \"w\") as file:\n",
    "        file.write(str_prompt)\n",
    "    prompt_artifact.add_file(f\"{PROMPT_OUT_PATH}/prompt.txt\")\n",
    "    # Log the artifact\n",
    "    wandb.log_artifact(prompt_artifact)\n",
    "    \n",
    "    for index, row in human_labels.iterrows():\n",
    "        prompt = tlu.build_prompt(row['text'], categories_flat)\n",
    "        r = client.chat.completions.create(\n",
    "            model=model,\n",
    "            temperature=0.0,\n",
    "            messages=prompt,\n",
    "            functions=[function],\n",
    "            function_call={\"name\": \"predict_category\"},\n",
    "            )\n",
    "        llm_output = set(tlu.get_labels_from_gpt_response(r))\n",
    "        human_labels_list = human_labels[human_labels['id'] == row['id']]['accept'].values\n",
    "        human_output = set([label for sublist in human_labels_list for label in sublist])\n",
    "        results[model]['outputs'].append(eval_gpt_output(llm_output, human_output, id=row['id'], text=row['text'], model=model, prompt=prompt))\n",
    "    df = pd.DataFrame(results[model]['outputs'])\n",
    "    wb_table = wandb.Table(\n",
    "                data=df, columns=df.columns\n",
    "            )\n",
    "    run.log({\"Outputs\": wb_table})\n",
    "    # Evaluation metrics\n",
    "    summary_stats = summarise_gpt_performance(df)\n",
    "    results[model]['stats'] = summary_stats\n",
    "    # Log metrics\n",
    "    wandb.run.summary[\"accuracy\"] = summary_stats['prop_exact_matches']\n",
    "    wandb.run.summary['prop_no_overlap'] = summary_stats['prop_no_overlap']\n",
    "    wandb.run.summary['avg_missing_labels'] = summary_stats['avg_missing_labels']\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results['gpt-3.5-turbo-1106']['outputs']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "discovery_child_development",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
